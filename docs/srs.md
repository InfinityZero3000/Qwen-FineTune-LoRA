# TÃ€I LIá»†U Äáº¶C Táº¢ YÃŠU Cáº¦U PHáº¦N Má»€M (SRS)

## 1. Giá»›i thiá»‡u

### 1.1. Má»¥c Ä‘Ã­ch

TÃ i liá»‡u nÃ y mÃ´ táº£ chi tiáº¿t cÃ¡c yÃªu cáº§u chá»©c nÄƒng vÃ  phi chá»©c nÄƒng cho há»‡ thá»‘ng **AI há»— trá»£ há»c tiáº¿ng Anh thÃ´ng qua há»™i thoáº¡i vÄƒn báº£n vÃ  giá»ng nÃ³i theo thá»i gian thá»±c**. Há»‡ thá»‘ng táº­p trung vÃ o viá»‡c giÃºp ngÆ°á»i há»c **cáº£i thiá»‡n ngá»¯ phÃ¡p, tá»« vá»±ng, phÃ¡t Ã¢m vÃ  Ä‘á»™ trÃ´i cháº£y khi nÃ³i**, phÃ¹ há»£p vá»›i trÃ¬nh Ä‘á»™ **A2â€“B1**.

TÃ i liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng cho:

* Sinh viÃªn thá»±c hiá»‡n Ä‘á»“ Ã¡n AI / Deep Learning
* Giáº£ng viÃªn hÆ°á»›ng dáº«n vÃ  há»™i Ä‘á»“ng Ä‘Ã¡nh giÃ¡
* Láº­p trÃ¬nh viÃªn phÃ¡t triá»ƒn vÃ  má»Ÿ rá»™ng há»‡ thá»‘ng

---

### 1.2. Pháº¡m vi há»‡ thá»‘ng

Há»‡ thá»‘ng cho phÃ©p ngÆ°á»i dÃ¹ng:

* NÃ³i trá»±c tiáº¿p vá»›i AI (speech-to-speech)
* Chat báº±ng vÄƒn báº£n
* Nháº­n pháº£n há»“i vá»:

  * Lá»—i ngá»¯ phÃ¡p
  * Má»©c Ä‘á»™ trÃ´i cháº£y (fluency)
  * TrÃ¬nh Ä‘á»™ tá»« vá»±ng (CEFR)
  * PhÃ¡t Ã¢m
* Nghe láº¡i cÃ¢u Ä‘Ãºng Ä‘Æ°á»£c AI phÃ¡t Ã¢m chuáº©n

Há»‡ thá»‘ng **khÃ´ng nháº±m thay tháº¿ giÃ¡o viÃªn**, mÃ  Ä‘Ã³ng vai trÃ² **trá»£ lÃ½ há»c táº­p thÃ´ng minh**.

---

### 1.3. Thuáº­t ngá»¯ vÃ  viáº¿t táº¯t

| Thuáº­t ngá»¯ | MÃ´ táº£                        |
| --------- | ---------------------------- |
| DL        | Deep Learning                |
| STT       | Speech-to-Text               |
| TTS       | Text-to-Speech               |
| GEC       | Grammar Error Correction     |
| CEFR      | Khung tham chiáº¿u chÃ¢u Ã‚u     |
| ASR       | Automatic Speech Recognition |

---

## 2. Tá»•ng quan há»‡ thá»‘ng

### 2.1. Kiáº¿n trÃºc tá»•ng thá»ƒ

Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ theo kiáº¿n trÃºc **Unified Multi-Task AI vá»›i Development/Production modes**, sá»­ dá»¥ng **1 base model + multi-task LoRA adapters** cho hiá»‡u quáº£ tá»‘i Æ°u. Táº¥t cáº£ mÃ´ hÃ¬nh Ä‘Æ°á»£c **fine-tune locally**, khÃ´ng dá»±a vÃ o API bÃªn ngoÃ i.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LEXILINGO ARCHITECTURE                      â”‚
â”‚                  (Development vs Production Mode)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[NgÆ°á»i dÃ¹ng] 
   â”‚ (Giá»ng nÃ³i / VÄƒn báº£n)
   â–¼
[Frontend Mobile/Web]
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SPEECH INPUT PIPELINE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STT Service (Speech-to-Text)                                â”‚
â”‚  â€¢ Dev Mode:  Whisper v3 Large (1.5GB, WER 3-5%)             â”‚
â”‚  â€¢ Prod Mode: Whisper v3 Small/Medium (500MB-1.5GB, WER 8%)  â”‚
â”‚  â€¢ Output: Transcription + confidence scores                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â”‚ VÄƒn báº£n: "I like learning English" (+ confidence: 0.95)
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               UNIFIED NLP PROCESSING ENGINE                  â”‚
â”‚                    (Qwen2.5 Base Model)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Base Model (Load once, reuse for all tasks):                â”‚
â”‚  â€¢ Dev:  Qwen2.5-1.5B-Instruct (900MB Q4, 2GB RAM)           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚        4 LoRA Adapters (Task-Specific)             â”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚   
â”‚  â”‚                                                    â”‚      â”‚
â”‚  â”‚  [1] Fluency Scoring Adapter                       â”‚      â”‚
â”‚  â”‚      â€¢ LoRA: r=32 (dev), r=16 (prod)               â”‚      â”‚
â”‚  â”‚      â€¢ Output: Score 0.0-1.0 + reasoning           â”‚      â”‚
â”‚  â”‚      â€¢ Metrics: MAE < 0.12 (dev), < 0.15 (prod)    â”‚      â”‚
â”‚  â”‚                                                    â”‚      â”‚
â”‚  â”‚  [2] Vocabulary Classification Adapter             â”‚      â”‚
â”‚  â”‚      â€¢ LoRA: r=32 (dev), r=16 (prod)               â”‚      â”‚
â”‚  â”‚      â€¢ Output: A2/B1/B2 level + key words          â”‚      â”‚
â”‚  â”‚      â€¢ Accuracy: 90% (dev), 86% (prod)             â”‚      â”‚
â”‚  â”‚                                                    â”‚      â”‚
â”‚  â”‚  [3] Grammar Correction Adapter                    â”‚      â”‚
â”‚  â”‚      â€¢ LoRA: r=32 (dev), r=16 (prod)               â”‚      â”‚
â”‚  â”‚      â€¢ Output: Corrected text + explanations       â”‚      â”‚
â”‚  â”‚      â€¢ F0.5: 68 (dev), 62 (prod)                   â”‚      â”‚
â”‚  â”‚                                                    â”‚      â”‚
â”‚  â”‚  [4] Dialogue Response Adapter                     â”‚      â”‚
â”‚  â”‚      â€¢ LoRA: r=32 (dev), r=16 (prod)               â”‚      â”‚
â”‚  â”‚      â€¢ Output: Encouraging tutor response          â”‚      â”‚
â”‚  â”‚      â€¢ Quality: 96% (dev), 91% (prod)              â”‚      â”‚
â”‚  â”‚                                                    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â”‚  Adapter Switching: < 1ms (no model reload)                  â”‚
â”‚  Memory Efficiency: 72% RAM saving vs separate models        â”‚
â”‚  Total Processing Time: ~510ms (dev), ~300ms (prod)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â”‚ Combined Analysis Output:
   â”‚ â€¢ Fluency: 0.87/1.0 âœ“
   â”‚ â€¢ Vocab: B1 level âœ“
   â”‚ â€¢ Grammar: No errors âœ“
   â”‚ â€¢ Response: "Excellent! Try 'I enjoy learning English'..."
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PARALLEL: PRONUNCIATION ANALYSIS                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HuBERT-large (facebook/hubert-large-ls960)                  â”‚
â”‚  â€¢ Phoneme recognition (CTC decoding)                        â”‚
â”‚  â€¢ Forced alignment with native reference                    â”‚
â”‚  â€¢ Error detection: substitution, deletion, timing           â”‚
â”‚  â€¢ Output: IPA errors, accent analysis, prosody issues       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â”‚ Pronunciation: Minor stress on 'learning' 
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RESPONSE AGGREGATION ENGINE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Combine all analysis results                              â”‚
â”‚  â€¢ Format response for user level (A2/B1/B2)                 â”‚
â”‚  â€¢ Add pronunciation tips if needed                          â”‚
â”‚  â€¢ Prepare text for TTS                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â”‚ Final Response: "Excellent! Your sentence is perfect. 
   â”‚                  To sound more natural, try 'I enjoy 
   â”‚                  learning English'..."
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HYBRID TTS (Text-to-Speech)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       Piper TTS (30-60MB, 100-300ms)                         â”‚
â”‚          â€¢ VITS-based, offline                               â”‚
â”‚          â€¢ Use: Pronunciation demos, lesson audio            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â”‚ Audio output (16kHz/22kHz, WAV/MP3)
   â”‚
   â–¼
[NgÆ°á»i dÃ¹ng]  Nghe pháº£n há»“i


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RESOURCE COMPARISON (Dev vs Prod)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Component      â”‚ Development (Mac)    â”‚ Production (Mobile) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ STT            â”‚ Whisper v3 Large    â”‚ Whisper v3 Small   â”‚
â”‚                â”‚ 1.5GB, WER 3-5%     â”‚ 500MB, WER 8-10%   â”‚
â”‚                â”‚ RAM: 4GB            â”‚ RAM: 1.5GB         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NLP (Unified)  â”‚ Qwen2.5-1.5B        â”‚ Qwen2.5-0.5B       â”‚
â”‚                â”‚ 900MB + 100MB LoRA  â”‚ 300MB + 50MB LoRA  â”‚
â”‚                â”‚ RAM: 2GB            â”‚ RAM: 600MB         â”‚
â”‚                â”‚ Quality: 96%        â”‚ Quality: 91%       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Pronunciation  â”‚ HuBERT-large        â”‚ HuBERT-large       â”‚
â”‚                â”‚ 960M params         â”‚ (server-side)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TTS            â”‚ Native + Piper      â”‚ Native TTS (0MB)   â”‚
â”‚                â”‚ 0-60MB              â”‚ 0MB                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL RAM      â”‚ ~6-7GB              â”‚ ~2.4GB             â”‚
â”‚ Total Storage  â”‚ ~3GB                â”‚ ~1GB               â”‚
â”‚ Latency        â”‚ ~600ms              â”‚ ~400ms             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Æ¯u Ä‘iá»ƒm kiáº¿n trÃºc Unified Multi-Task**:
1. **Memory Efficiency**: 1 base model cho 4 tasks â†’ tiáº¿t kiá»‡m 72% RAM
2. **Speed**: Adapter switching < 1ms, khÃ´ng cáº§n reload model
3. **Consistency**: Shared base representations â†’ quality tá»‘t hÆ¡n
4. **Deployment**: 1 model file + 4 adapters nhá» â†’ dá»… update
5. **Scalability**: ThÃªm task má»›i chá»‰ cáº§n train thÃªm 1 LoRA adapter

---

### 2.2. Luá»“ng xá»­ lÃ½ chi tiáº¿t (System Flow)

#### Luá»“ng 1: NgÆ°á»i dÃ¹ng nÃ³i vá»›i AI

1. NgÆ°á»i dÃ¹ng nÃ³i tiáº¿ng Anh qua á»©ng dá»¥ng
2. Ã‚m thanh Ä‘Æ°á»£c gá»­i Ä‘áº¿n **STT Service**
3. STT chuyá»ƒn giá»ng nÃ³i thÃ nh vÄƒn báº£n (cÃ³ thá»ƒ chá»©a lá»—i)
4. VÄƒn báº£n Ä‘Æ°á»£c gá»­i song song Ä‘áº¿n cÃ¡c module phÃ¢n tÃ­ch:

   * Cháº¥m Ä‘iá»ƒm fluency
   * PhÃ¢n loáº¡i trÃ¬nh Ä‘á»™ tá»« vá»±ng
   * PhÃ¡t hiá»‡n lá»—i ngá»¯ phÃ¡p
   * PhÃ¢n tÃ­ch phÃ¡t Ã¢m
5. CÃ¡c káº¿t quáº£ Ä‘Æ°á»£c tá»•ng há»£p thÃ nh pháº£n há»“i há»c táº­p
6. AI táº¡o cÃ¢u tráº£ lá»i há»™i thoáº¡i phÃ¹ há»£p trÃ¬nh Ä‘á»™ ngÆ°á»i há»c
7. CÃ¢u tráº£ lá»i Ä‘Æ°á»£c chuyá»ƒn sang giá»ng nÃ³i qua TTS
8. NgÆ°á»i dÃ¹ng nghe pháº£n há»“i tá»« AI

---

#### Luá»“ng 2: NgÆ°á»i dÃ¹ng chat báº±ng vÄƒn báº£n

1. NgÆ°á»i dÃ¹ng nháº­p cÃ¢u tiáº¿ng Anh
2. VÄƒn báº£n Ä‘Æ°á»£c gá»­i trá»±c tiáº¿p Ä‘áº¿n NLP Orchestrator
3. CÃ¡c module DL vÃ  rule xá»­ lÃ½ tÆ°Æ¡ng tá»± Luá»“ng 1 (bá» qua STT vÃ  phÃ¡t Ã¢m)
4. Tráº£ vá» pháº£n há»“i dáº¡ng vÄƒn báº£n vÃ /hoáº·c giá»ng nÃ³i

---

## 3. CÃ¡c tÃ¡c nhÃ¢n sá»­ dá»¥ng (Actors)

| TÃ¡c nhÃ¢n       | Vai trÃ²                         |
| -------------- | ------------------------------- |
| NgÆ°á»i há»c      | Thá»±c hÃ nh nÃ³i vÃ  chat tiáº¿ng Anh |
| Há»‡ thá»‘ng AI    | PhÃ¢n tÃ­ch, Ä‘Ã¡nh giÃ¡ vÃ  pháº£n há»“i |
| NhÃ  phÃ¡t triá»ƒn | Huáº¥n luyá»‡n, cáº­p nháº­t model      |

---

## 4. YÃªu cáº§u chá»©c nÄƒng

### 4.1. Module Speech-to-Text (STT)

**FR-STT-01**: Há»‡ thá»‘ng pháº£i chuyá»ƒn Ä‘á»•i giá»ng nÃ³i tiáº¿ng Anh thÃ nh vÄƒn báº£n vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao (WER < 10%).

**FR-STT-02**: Há»— trá»£ xá»­ lÃ½ thá»i gian thá»±c (streaming).

**FR-STT-03**: Cung cáº¥p confidence score cho má»—i tá»«.

**MÃ´ hÃ¬nh Ä‘á» xuáº¥t (sáº¯p xáº¿p theo hiá»‡u nÄƒng)**:

1. **Faster-Whisper** (Recommended)
   - Model base: OpenAI Whisper (Ä‘Ã£ pre-trained trÃªn 680k giá» dá»¯ liá»‡u Ä‘a ngÃ´n ngá»¯)
   - Tá»‘i Æ°u hÃ³a: C++ implementation, 4x nhanh hÆ¡n Whisper gá»‘c
   - KÃ­ch thÆ°á»›c: medium (384M) hoáº·c large (1.5GB) tuá»³ tÃ i nguyÃªn
   - WER tiáº¿ng Anh: ~8% (medium), ~4% (large)
   - Äá»™ trá»…: < 1s cho 10s audio (GPU), < 2s (CPU)
   - YÃªu cáº§u: PyTorch, ffmpeg
   
2. **wav2vec 2.0-large + HuBERT-large** (Alternative)
   - Tá»± training trÃªn SUPERB benchmark
   - Fine-tune trÃªn English dataset (LibriSpeech)
   - WER: ~7-9%
   - Lightweight hÆ¡n Whisper (~340M params)
   - Tá»‘c Ä‘á»™ nhanh hÆ¡n, phÃ¹ há»£p CPU/mobile
   
3. **Vosk** (Fallback for CPU-only)
   - Nháº¹ (~50MB), offline, khÃ´ng cáº§n GPU
   - WER: ~15-20%, kÃ©m chÃ­nh xÃ¡c nhÆ°ng Ä‘á»§ cho prototyping

**Quy trÃ¬nh thá»±c hiá»‡n**:
- Sá»­ dá»¥ng **Faster-Whisper (medium)** lÃ m base
- Input: wav/mp3, mono, 16kHz
- Output: transcription + confidence_score/word
- Post-processing: Remove duplicates, fix capitalization

**Pipeline chi tiáº¿t**:
```
Audio Stream
    â†“
[Resample to 16kHz]
    â†“
[Faster-Whisper inference]
    â†“
Output: {
  "text": "I like learning English",
  "confidence": 0.94,
  "words": [
    {"text": "I", "confidence": 0.99},
    {"text": "like", "confidence": 0.92},
    ...
  ]
}
```

---

### 4.2. Module cháº¥m Ä‘iá»ƒm Ä‘á»™ trÃ´i cháº£y (Fluency Scoring â€“ DL)

**FR-FLU-01**: Há»‡ thá»‘ng pháº£i Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ trÃ´i cháº£y cá»§a cÃ¢u nÃ³i/vÄƒn báº£n.

**FR-FLU-02**: Káº¿t quáº£ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng Ä‘iá»ƒm sá»‘ tá»« 0.0 Ä‘áº¿n 1.0.

**FR-FLU-03**: Fine-tune model Ä‘á»ƒ phÃ¹ há»£p vá»›i trÃ¬nh Ä‘á»™ A2-B1.

**MÃ´ hÃ¬nh Ä‘á» xuáº¥t (theo chiáº¿n lÆ°á»£c Dev/Prod)**:

**DEVELOPMENT MODE:**

**Qwen2.5-1.5B-Instruct fine-tuned** (Best for Development)
- Base model: Qwen/Qwen2.5-1.5B-Instruct
- Parameters: 1.5B (decoder-only Transformer)
- Context: 32K tokens
- Size: ~900MB (Q4), ~3GB (F16)
- RAM: ~2GB inference
- Hiá»‡u nÄƒng: 92.3% accuracy on text classification tasks
- Pre-training: 18T tokens (multilingual, vá»›i focus vÃ o English)
- Advantages:
  - Instruction-tuned: Better zero-shot understanding
  - Long context: Analyze full conversations
  - Fast inference: ~100ms/sentence on CPU
  
**ğŸ“± PRODUCTION MODE:**

**Qwen2.5-0.5B-Instruct fine-tuned** (Mobile Optimized)
- Parameters: 0.5B (3x smaller)
- Size: ~300MB (Q4)
- RAM: ~600MB inference
- Hiá»‡u nÄƒng: 88.5% accuracy (only 4% drop)
- Speed: ~50ms/sentence on mobile CPU
- Quality: â­â­â­â­ (excellent for mobile)

**Quy trÃ¬nh Fine-tune vá»›i LoRA (Parameter-Efficient)**:

```
BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u (1,500-3,000 máº«u)
â”œâ”€ TÃ­nh cháº¥t dá»¯ liá»‡u:
â”‚  â”œâ”€ Source: ESL corpus (EFCAMDAT, TOEFL11), English Learning datasets
â”‚  â”œâ”€ Label: Human-annotated fluency scores (0.0-1.0)
â”‚  â”œâ”€ Format: Instruction-tuning format
â”‚  â”‚   Input: "Rate the fluency of this sentence: {text}"
â”‚  â”‚   Output: "Fluency score: {score}/1.0. Reasoning: {reason}"
â”‚  â”œâ”€ Split: 70% train (1,050), 15% val (225), 15% test (225)
â”‚  â””â”€ Augmentation: Back-translation, paraphrase (TextAugment)

BÆ°á»›c 2: Tokenization & Preprocessing
â”œâ”€ Tokenizer: Qwen2.5Tokenizer (BPE-based, 151,936 vocab)
â”œâ”€ Max length: 512 tokens (cho conversation context)
â”œâ”€ Chat template: <|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant
â””â”€ Padding: Left padding (causal LM requirement)

BÆ°á»›c 3: LoRA Configuration (Development Mode)
â”œâ”€ LoRA rank (r): 32 (higher for better quality)
â”œâ”€ LoRA alpha: 64 (scaling factor)
â”œâ”€ Target modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
â”œâ”€ Trainable params: ~25M (only 1.7% of 1.5B)
â”œâ”€ Dropout: 0.05
â””â”€ Task type: CAUSAL_LM with value head for regression

BÆ°á»›c 4: Training Configuration (Dev Mode - Mac 32GB)
â”œâ”€ Optimizer: AdamW (learning_rate: 3e-4, weight_decay: 0.01)
â”œâ”€ Batch size: 8 (effective 32 with gradient_accumulation=4)
â”œâ”€ Epochs: 5
â”œâ”€ Scheduler: Cosine with warmup (warmup_ratio: 0.03)
â”œâ”€ Precision: bfloat16 (faster on M1/M2 Mac) or float16
â”œâ”€ Gradient clipping: 1.0
â””â”€ Loss: MSE for score + CrossEntropy for reasoning generation

BÆ°á»›c 5: Production Model (Knowledge Distillation)
â”œâ”€ Teacher: Qwen2.5-1.5B (trained above)
â”œâ”€ Student: Qwen2.5-0.5B
â”œâ”€ LoRA config: r=16, alpha=32 (lighter)
â”œâ”€ Distillation loss: MSE(student_output, teacher_output) + KL_div(logits)
â”œâ”€ Training: 3 epochs, batch_size=12
â””â”€ Result: 88-90% teacher performance at 3x speed

BÆ°á»›c 6: Evaluation Metrics
â”œâ”€ MAE (Mean Absolute Error): < 0.12 (dev), < 0.15 (prod)
â”œâ”€ RMSE: < 0.18 (dev), < 0.22 (prod)
â”œâ”€ Pearson correlation: > 0.90 (dev), > 0.86 (prod)
â””â”€ Inference speed: 100ms (dev CPU), 50ms (prod mobile)
```

**Implementation Stack**:
- Framework: HuggingFace Transformers + PEFT (LoRA)
- Training: TRL (Transformer Reinforcement Learning) + PyTorch
- Hardware: Mac 32GB (dev), GPU optional (faster)
- Training time: ~45-60 min (1.5B), ~20-30 min (0.5B)

**Inference pipeline**:
```python
# Development Mode (Qwen2.5-1.5B)
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import re

# Load base model + LoRA adapter
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.bfloat16,  # Better for M1/M2 Mac
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, "path/to/fluency-lora-adapter")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Inference function
def evaluate_fluency(text: str) -> dict:
    prompt = f"""Rate the fluency of this English sentence on a scale of 0.0 to 1.0:
Sentence: {text}

Provide:
1. Fluency score (0.0-1.0)
2. Brief reasoning

Format: Score: X.XX | Reason: ..."""
    
    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=80,
            temperature=0.1,
            do_sample=False
        )
        response = tokenizer.decode(
            outputs[0][len(inputs[0]):],
            skip_special_tokens=True
        )
    
    # Parse: "Score: 0.87 | Reason: Natural grammar, smooth flow"
    score_match = re.search(r"Score: ([0-9.]+)", response)
    reason_match = re.search(r"Reason: (.+)", response)
    
    return {
        "text": text,
        "fluency_score": float(score_match.group(1)) if score_match else 0.5,
        "reasoning": reason_match.group(1).strip() if reason_match else "",
        "response_time_ms": 100  # ~100ms on Mac M1
    }

# Example
result = evaluate_fluency("I like learning English")
print(result)
# Output: {'text': '...', 'fluency_score': 0.87, 'reasoning': '...', ...}

# Production Mode (Qwen2.5-0.5B) - same code, just load 0.5B model
# base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct", ...)
```

**Output**:
```json
{
  "text": "I like learning English",
  "fluency_score": 0.87,
  "fluency_level": "B1 (Good)",
  "issues": ["minor_pausing", "natural_rhythm"]
}
```

---

### 4.3. Module phÃ¢n loáº¡i trÃ¬nh Ä‘á»™ tá»« vá»±ng

**FR-VOC-01**: PhÃ¢n loáº¡i cÃ¢u nÃ³i theo trÃ¬nh Ä‘á»™ CEFR (A2, B1, B2).

**FR-VOC-02**: Hoáº¡t Ä‘á»™ng vá»›i cáº£ vÄƒn báº£n chuáº©n vÃ  vÄƒn báº£n tá»« STT (cÃ³ lá»—i).

**FR-VOC-03**: Cung cáº¥p giáº£i thÃ­ch tá»« vá»±ng khÃ³ (tá»« > B1 level).

**MÃ´ hÃ¬nh Ä‘á» xuáº¥t (theo chiáº¿n lÆ°á»£c Dev/Prod)**:

**DEVELOPMENT MODE:**

**Qwen2.5-1.5B-Instruct fine-tuned** (Unified vá»›i Fluency model)
- Base model: Qwen/Qwen2.5-1.5B-Instruct
- Parameters: 1.5B (same model, different LoRA adapter)
- Advantages:
  - Single model cho nhiá»u tasks (fluency + vocabulary + grammar)
  - Instruction following: Better reasoning vá» vocabulary level
  - Context-aware: PhÃ¢n tÃ­ch trong ngá»¯ cáº£nh cÃ¢u
  - Giáº£i thÃ­ch tá»± nhiÃªn: "This word is B2 level because..."
- Size: ~900MB (Q4), RAM: ~2GB
- Classification: Few-shot prompting + fine-tuning

**ğŸ“± PRODUCTION MODE:**

**Qwen2.5-0.5B-Instruct fine-tuned** (Mobile)
- Parameters: 0.5B
- Size: ~300MB (Q4), RAM: ~600MB
- Quality: 86% accuracy (chá»‰ giáº£m 4% so vá»›i 1.5B)
- Inference: ~50ms/sentence

**Quy trÃ¬nh Fine-tune vá»›i LoRA (Vocabulary Classification)**:

```
BÆ°á»›c 1: Dataset Preparation (2,500 máº«u)
â”œâ”€ Annotation schema:
â”‚  â”œâ”€ Class A2: Common words (basic vocabulary) - 900 máº«u
â”‚  â”‚          e.g., "I like to go", "The weather is nice"
â”‚  â”œâ”€ Class B1: Intermediate vocabulary - 900 máº«u
â”‚  â”‚          e.g., "We should discuss the opportunity"
â”‚  â”œâ”€ Class B2: Advanced vocabulary - 700 máº«u
â”‚  â”‚          e.g., "His argument was quite eloquent"
â”‚  â””â”€ Mixed levels: Sentences vá»›i nhiá»u level - 0 máº«u (Ä‘á»ƒ Ä‘Æ¡n giáº£n)
â”œâ”€ Sources:
â”‚  â”œâ”€ CEFR-graded readers (Oxford, Cambridge)
â”‚  â”œâ”€ TOEFL/IELTS practice materials
â”‚  â”œâ”€ ESL textbooks (level-marked)
â”‚  â””â”€ Custom annotations (teachers)
â”œâ”€ Format: Instruction-tuning
â”‚  Input: "Classify the vocabulary level: {sentence}"
â”‚  Output: "Level: B1 | Key words: discuss (B1), opportunity (B1)"
â””â”€ Distribution: 36% A2, 36% B1, 28% B2

BÆ°á»›c 2: LoRA Configuration (Vocabulary Task)
â”œâ”€ Base: Qwen2.5-1.5B-Instruct (Dev) / 0.5B (Prod)
â”œâ”€ LoRA rank: 32 (Dev), 16 (Prod)
â”œâ”€ LoRA alpha: 64 (Dev), 32 (Prod)
â”œâ”€ Target modules: ["q_proj", "v_proj", "o_proj"]
â”œâ”€ Trainable params: ~18M (Dev), ~6M (Prod)
â””â”€ Max seq length: 512 tokens

BÆ°á»›c 3: Training Configuration (Dev Mode)
â”œâ”€ Optimizer: AdamW (lr: 2e-4, weight_decay: 0.01)
â”œâ”€ Batch size: 12 (gradient_accumulation: 3 â†’ effective 36)
â”œâ”€ Epochs: 4
â”œâ”€ Scheduler: Cosine with warmup (warmup_ratio: 0.05)
â”œâ”€ Precision: bfloat16 (Mac) or float16
â”œâ”€ Loss: CrossEntropy with class weights [0.9, 1.0, 1.1]
â””â”€ Validation: F1-score macro every 200 steps

BÆ°á»›c 4: Production Model (Knowledge Distillation)
â”œâ”€ Teacher: Qwen2.5-1.5B (trained above)
â”œâ”€ Student: Qwen2.5-0.5B
â”œâ”€ LoRA: r=16, alpha=32
â”œâ”€ Distillation: KL divergence on logits + hard labels
â”œâ”€ Training: 3 epochs, batch_size=16
â””â”€ Result: 86% accuracy (vs 90% teacher)

BÆ°á»›c 5: Evaluation Metrics
â”œâ”€ Development Mode:
â”‚  â”œâ”€ Accuracy: > 0.90 (overall)
â”‚  â”œâ”€ Per-class F1: A2 (0.89), B1 (0.91), B2 (0.88)
â”‚  â”œâ”€ Macro F1: > 0.89
â”‚  â””â”€ Inference: ~80ms/sentence (CPU)
â”œâ”€ Production Mode:
â”‚  â”œâ”€ Accuracy: > 0.86 (4% drop)
â”‚  â”œâ”€ Macro F1: > 0.85
â”‚  â””â”€ Inference: ~50ms/sentence (mobile CPU)
â””â”€ Confusion: Low A2â†”B1 misclassification (<8%)
```

**Implementation (Qwen2.5)**:
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import re

# Load model
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, "path/to/vocab-lora-adapter")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Classify vocabulary level
def classify_vocabulary(text: str) -> dict:
    prompt = f"""Classify the vocabulary level of this sentence (A2/B1/B2 CEFR):
Sentence: {text}

Provide:
1. Overall level (A2, B1, or B2)
2. Key words that determine the level
3. Brief explanation

Format: Level: XX | Key words: ... | Reason: ..."""
    
    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=100,
            temperature=0.05,  # More deterministic
            do_sample=False
        )
        response = tokenizer.decode(
            outputs[0][len(inputs[0]):],
            skip_special_tokens=True
        )
    
    # Parse response
    level_match = re.search(r"Level: (A2|B1|B2)", response)
    keywords_match = re.search(r"Key words: (.+?)(?:\||$)", response)
    reason_match = re.search(r"Reason: (.+)", response)
    
    return {
        "text": text,
        "level": level_match.group(1) if level_match else "B1",
        "key_words": keywords_match.group(1).strip() if keywords_match else "",
        "reasoning": reason_match.group(1).strip() if reason_match else "",
        "confidence": 0.90  # Can compute from logits if needed
    }

# Example
result = classify_vocabulary("The government implemented new policies")
print(result)
# Output: {'level': 'B2', 'key_words': 'government (B1), implemented (B2), policies (B2)', ...}
```

**Output**:
```json
{
  "text": "The government implemented new policies",
  "vocabulary_level": "B1",
  "confidence": 0.92,
  "difficult_words": [
    {
      "word": "implemented",
      "level": "B1",
      "definition": "put a decision or plan into effect"
    },
    {
      "word": "policies",
      "level": "B1",
      "definition": "official rules or plans"
    }
  ]
}
```

---

### 4.4. Module phÃ¡t hiá»‡n vÃ  sá»­a lá»—i ngá»¯ phÃ¡p

**FR-GEC-01**: PhÃ¡t hiá»‡n lá»—i ngá»¯ phÃ¡p phá»• biáº¿n á»Ÿ trÃ¬nh Ä‘á»™ A2-B1.

**FR-GEC-02**: Äá» xuáº¥t cÃ¢u sá»­a Ä‘Ãºng vá»›i giáº£i thÃ­ch cáº¥p Ä‘á»™ tá»«ng lá»—i.

**FR-GEC-03**: Cung cáº¥p giáº£i thÃ­ch ngáº¯n gá»n lÃ½ do sá»­a.

**Kiáº¿n trÃºc hai táº§ng (Hybrid Approach)**:

**Táº§ng 1: PhÃ¡t hiá»‡n lá»—i ngá»¯ phÃ¡p (Rule-based + DL)**

1. **ERRANT Rule Engine** (PhÃ¡t hiá»‡n lá»—i cÆ¡ báº£n)
   - Tool: python-errant package
   - Quy táº¯c: Subject-verb agreement, tense consistency, article usage
   - Output: Lá»—i location + error type

2. **GECToR DL Model** (Fine-tuning Ä‘á»ƒ phÃ¡t hiá»‡n lá»—i chi tiáº¿t)
   - Base: Sequence tagging model trÃªn DeBERTa
   - Pre-trained weights: grammarly/coedit-base (Ä‘Ã£ train trÃªn BEA dataset)
   - Architecture: BIO tagging (Begin-Inside-Outside)
   - Output: NhÃ£n lá»—i cho má»—i token

```
Input: "She go to school yesterday"
          B-VERB O O O O

Lá»—i phÃ¡t hiá»‡n: 
- Token "go" (index 1): VERB_TENSE error
```

**Táº§ng 2: Sá»­a lá»—i ngá»¯ phÃ¡p (Sequence-to-Sequence)**

**DEVELOPMENT MODE:**

**Qwen2.5-1.5B-Instruct fine-tuned** (Best cho GEC)
- Base: Qwen/Qwen2.5-1.5B-Instruct (1.5B params)
- Pre-training: 18T tokens vá»›i extensive English text
- Fine-tune: BEA-2019 (4.5K), CoNLL-2014 (1.3K), W&I+LOCNESS (3.4K)
- Advantages:
  - Instruction-tuned: Understand "correct this grammar error"
  - Reasoning: Explain why correction is needed
  - Multi-turn: Handle follow-up questions
  - Contextual: Better than pure seq2seq
- Size: ~900MB (Q4), RAM: ~2GB
- Precision: 78% (vs 70% for T5-large)
- F0.5 score: 68 (SOTA among open models <2B)

**ğŸ“± PRODUCTION MODE:**

**Qwen2.5-0.5B-Instruct fine-tuned** (Mobile)
- Parameters: 0.5B
- Size: ~300MB (Q4), RAM: ~600MB
- Precision: 72% (only 6% drop)
- F0.5 score: 62 (excellent for mobile)
- Inference: ~100ms/sentence

**Alternative (if needed):**

**T5-efficient-large fine-tuned** (Specialized GEC)
- Base: T5 v1.1 efficient variant (220M params)
- Architecture: Encoder-decoder (better for seq2seq)
- Pre-trained: C4 corpus + GEC datasets
- Size: ~880MB (F16), ~450MB (Q8)
- Speed: Faster than Qwen2.5 on CPU-only
- Use case: Fallback náº¿u Qwen2.5 quÃ¡ cháº­m trÃªn low-end devices

**Quy trÃ¬nh Fine-tune Qwen2.5 cho GEC**:

```
BÆ°á»›c 1: Dataset chuáº©n bá»‹ (9,200 máº«u tá»•ng cá»™ng)
â”œâ”€ Public datasets:
â”‚  â”œâ”€ BEA-2019 (Write & Improve + LOCNESS): 4,477 máº«u
â”‚  â”œâ”€ CoNLL-2014: 1,312 máº«u
â”‚  â”œâ”€ FCE (Cambridge): 2,805 máº«u
â”‚  â””â”€ Custom ESL corpus (A2-B1 focus): 606 máº«u
â”œâ”€ Lá»—i loáº¡i A2-B1 (prioritized):
â”‚  â”œâ”€ Subject-verb agreement (She go â†’ She goes)
â”‚  â”œâ”€ Tense errors (I go yesterday â†’ I went yesterday)
â”‚  â”œâ”€ Article errors (I like apple â†’ I like an apple)
â”‚  â”œâ”€ Preposition errors (arrive in 8am â†’ arrive at 8am)
â”‚  â”œâ”€ Word order (go I â†’ I go)
â”‚  â””â”€ Spelling (recieve â†’ receive)
â”œâ”€ Instruction format:
â”‚  Input: "Correct the grammar errors: {incorrect_sentence}"
â”‚  Output: "Corrected: {correct_sentence}\nExplanation: {reasoning}"
â”œâ”€ Split: 70% train (6,440), 15% val (1,380), 15% test (1,380)
â””â”€ Augmentation: Error injection (30% extra synthetic errors)

BÆ°á»›c 2: LoRA Fine-tuning Configuration
â”œâ”€ Development Mode (Qwen2.5-1.5B):
â”‚  â”œâ”€ LoRA rank (r): 32
â”‚  â”œâ”€ LoRA alpha: 64
â”‚  â”œâ”€ Target modules: All attention + MLP layers
â”‚  â”œâ”€ Trainable params: ~25M (1.7% of base)
â”‚  â””â”€ Dropout: 0.05
â”œâ”€ Production Mode (Qwen2.5-0.5B):
â”‚  â”œâ”€ LoRA rank: 16 (lighter)
â”‚  â”œâ”€ LoRA alpha: 32
â”‚  â”œâ”€ Trainable params: ~8M (1.6% of base)
â”‚  â””â”€ Knowledge distillation from 1.5B teacher
â””â”€ Multi-task: GEC + explanation generation (shared LoRA)

BÆ°á»›c 3: Training Configuration (Dev Mode - Mac 32GB)
â”œâ”€ Optimizer: AdamW (lr: 2e-4, weight_decay: 0.01)
â”œâ”€ Batch size: 8 (gradient_accumulation: 4 â†’ effective 32)
â”œâ”€ Epochs: 7
â”œâ”€ Scheduler: Cosine with warmup (warmup_steps: 200)
â”œâ”€ Precision: bfloat16 (M1/M2) or float16 (NVIDIA)
â”œâ”€ Gradient clipping: 1.0
â”œâ”€ Loss: CrossEntropy (correction) + MSE (confidence score)
â””â”€ Validation: Every 500 steps, early stopping patience=3

BÆ°á»›c 4: Post-processing & Inference
â”œâ”€ Decoding strategy:
â”‚  â”œâ”€ Sampling: temperature=0.1 (deterministic-like)
â”‚  â”œâ”€ Top-k: 5 (avoid very unlikely corrections)
â”‚  â”œâ”€ Max new tokens: 128
â”‚  â””â”€ Stop tokens: ["<|im_end|>", "\n\n"]
â”œâ”€ Confidence scoring:
â”‚  â”œâ”€ Logit-based: avg(log_prob) over generated tokens
â”‚  â”œâ”€ Threshold: Keep correction if confidence > 0.65
â”‚  â””â”€ Multiple corrections: Rank by confidence
â”œâ”€ Rule-based post-check:
â”‚  â”œâ”€ ERRANT validation: Ensure edit is valid
â”‚  â”œâ”€ Minimal edit: Prefer fewer changes
â”‚  â””â”€ Preserve meaning: Check semantic similarity (>0.85)
â””â”€ Explanation parsing: Extract reasoning from output

BÆ°á»›c 5: Evaluation Metrics
â”œâ”€ BLEU score: > 76 (dev), > 72 (prod)
â”œâ”€ M2 Scorer F0.5: > 68 (dev), > 62 (prod)
â”œâ”€ Precision: > 78% (dev), > 72% (prod)
â”œâ”€ Recall: > 68% (dev), > 62% (prod)
â”œâ”€ GLEU: > 0.72 (generalized BLEU for edits)
â”œâ”€ Inference speed: ~150ms/sentence (dev), ~80ms (prod)
â””â”€ Manual evaluation: Native speakers (fluency, accuracy)
```

**Giáº£i thÃ­ch lá»—i (Explanation Module)**:

```
CÆ¡ cháº¿:
â”œâ”€ Lá»—i loáº¡i A: Quy táº¯c Ä‘Æ¡n giáº£n (verb agreement, article)
â”‚  â””â”€ Giáº£i thÃ­ch: Rule-based tá»« rule database
â”œâ”€ Lá»—i loáº¡i B: Phá»©c táº¡p (paraphrase, context-dependent)
â”‚  â””â”€ Giáº£i thÃ­ch: LLM (Flan-T5) táº¡o giáº£i thÃ­ch tá»± nhiÃªn
â””â”€ Output: Vietnamese giáº£i thÃ­ch cho ngÆ°á»i há»c

VÃ­ dá»¥:
Input: "He go to school"
Error: VERB_AGREEMENT (he = 3rd person singular, need 's')
Explanation_EN: "Subject 'he' is singular (3rd person), so verb 'go' 
                 must be 'goes'"
Explanation_VI: "Chá»§ ngá»¯ 'he' lÃ  sá»‘ Ã­t (ngÆ°á»i thá»© 3), nÃªn Ä‘á»™ng tá»« 
                 pháº£i lÃ  'goes'"
```

**Implementation Stack**:
```python
# Pipeline with Qwen2.5
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from errant import Annotator
import torch

# 1. Load Qwen2.5 GEC model (Dev mode)
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
gec_model = PeftModel.from_pretrained(base_model, "path/to/gec-lora-adapter")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# 2. Rule-based pre-check (ERRANT)
errant_annotator = Annotator("en")
potential_errors = errant_annotator.parse(source_sent)

# 3. DL correction with explanation
prompt = f"""Correct the grammar errors in this sentence and explain why:
Sentence: {source_sent}

Provide:
1. Corrected sentence
2. List of errors found
3. Brief explanation for each correction"""

messages = [{"role": "user", "content": prompt}]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt"
)

with torch.no_grad():
    outputs = gec_model.generate(
        inputs,
        max_new_tokens=200,
        temperature=0.1,
        top_k=5,
        do_sample=False
    )
    response = tokenizer.decode(
        outputs[0][len(inputs[0]):],
        skip_special_tokens=True
    )

# 4. Parse response
import re
corrected_match = re.search(r"Corrected: (.+)", response)
errors_match = re.findall(r"Error \d+: (.+)", response)

result = {
    "original": source_sent,
    "corrected": corrected_match.group(1) if corrected_match else source_sent,
    "errors": errors_match,
    "explanation": response,
    "confidence": calculate_confidence(outputs)
}

# Production mode: Same code, load Qwen2.5-0.5B instead
```

**Output**:
```json
{
  "original": "She go to school yesterday",
  "errors": [
    {
      "position": 1,
      "word": "go",
      "error_type": "VERB_TENSE/AGREEMENT",
      "correction": "goes",
      "confidence": 0.96,
      "explanation": "3rd person singular present needs 's' suffix"
    }
  ],
  "corrected": "She goes to school yesterday",
  "correction_confidence": 0.94
}
```

---

### 4.5. Module Ä‘Ã¡nh giÃ¡ trÃ¬nh Ä‘á»™ ngÆ°á»i há»c (Learner Proficiency Assessment â€“ NEW)

**FR-LPA-01**: ÄÃ¡nh giÃ¡ tá»•ng thá»ƒ trÃ¬nh Ä‘á»™ ngÆ°á»i há»c (A1/A2/B1/B2/C1) dá»±a trÃªn nhiá»u chiá»u Ä‘o.

**FR-LPA-02**: Theo dÃµi progress theo thá»i gian (tracking improvement).

**FR-LPA-03**: Äá» xuáº¥t Ä‘iá»ƒm máº¡nh/yáº¿u vÃ  lá»™ trÃ¬nh há»c táº­p cÃ¡ nhÃ¢n hÃ³a.

**MÃ´ hÃ¬nh Ä‘á» xuáº¥t (Multi-dimensional Assessment)**:

**ğŸ¯ CORE MODEL: Qwen2.5-1.5B fine-tuned vá»›i Multi-Task LoRA (Holistic Assessment)**

**Kiáº¿n trÃºc**:
```
Input Features (Aggregated tá»« conversation history):
â”œâ”€ Grammar: Error rate, error types distribution
â”œâ”€ Vocabulary: CEFR level distribution (% A2/B1/B2 words used)
â”œâ”€ Fluency: Average fluency scores over time
â”œâ”€ Pronunciation: Phoneme accuracy, prosody scores
â””â”€ Interaction: Response coherence, conversation depth

     â†“ (Feature Engineering)

[Qwen2.5-1.5B + Proficiency LoRA Adapter]
- Input: Conversation transcript + metrics history
- Task: Multi-class classification (A1/A2/B1/B2/C1)
- Output: 
  * Overall CEFR level (confidence scores)
  * Subscores: Grammar (X/10), Vocabulary (X/10), etc.
  * Weaknesses identification
  * Personalized learning recommendations

     â†“

Result: {
  "current_level": "A2",
  "confidence": 0.87,
  "subscores": {
    "grammar": 6.5,
    "vocabulary": 7.2,
    "fluency": 6.8,
    "pronunciation": 7.0
  },
  "weaknesses": ["past_tense_verbs", "article_usage"],
  "recommendations": [
    "Practice past simple tense with regular verbs",
    "Review article rules (a/an/the)"
  ],
  "progress": "+0.3 (compared to last week)"
}
```

**Fine-tuning Strategy**:

```
BÆ°á»›c 1: Dataset Construction (Longitudinal Learner Data)
â”œâ”€ Source datasets:
â”‚  â”œâ”€ EFCAMDAT: Cambridge Learner Corpus (83K texts, CEFR-labeled)
â”‚  â”œâ”€ EF-Cambridge Open Language Database (CEFR A1-C2)
â”‚  â”œâ”€ TOEFL11: 12K essays (scored + proficiency levels)
â”‚  â””â”€ Custom: Simulated conversation histories (2K users)
â”œâ”€ Feature extraction per learner:
â”‚  â”œâ”€ Grammar errors: Extract tá»« GEC model outputs (over 10+ sessions)
â”‚  â”œâ”€ Vocabulary profile: CEFR distribution tá»« 50+ sentences
â”‚  â”œâ”€ Fluency trend: Average cá»§a fluency scores (20+ samples)
â”‚  â””â”€ Interaction quality: Conversation depth, coherence
â”œâ”€ Labels: Expert-annotated CEFR levels (A1-C1)
â”œâ”€ Format: Instruction-tuning vá»›i context aggregation
â”‚  Input: """Assess the English proficiency level based on:
â”‚           - Recent conversations: {transcript_summary}
â”‚           - Grammar errors: {error_stats}
â”‚           - Vocabulary usage: {vocab_stats}
â”‚           - Fluency scores: {fluency_history}"""
â”‚  Output: """Level: A2 (confidence: 0.87)
â”‚            Subscores: Grammar 6.5/10, Vocabulary 7.2/10...
â”‚            Weaknesses: Past tense, articles
â”‚            Recommendations: ..."""
â””â”€ Split: 70% train (14K), 15% val (3K), 15% test (3K)

BÆ°á»›c 2: LoRA Fine-tuning (Proficiency Assessment Task)
â”œâ”€ Base: Qwen2.5-1.5B-Instruct (Dev) / 0.5B (Prod)
â”œâ”€ LoRA config:
â”‚  â”œâ”€ Rank (r): 32 (Dev), 16 (Prod)
â”‚  â”œâ”€ Alpha: 64 (Dev), 32 (Prod)
â”‚  â”œâ”€ Target modules: All attention + MLP (comprehensive understanding)
â”‚  â””â”€ Trainable params: ~28M (Dev), ~9M (Prod)
â”œâ”€ Training:
â”‚  â”œâ”€ Optimizer: AdamW (lr: 2e-4)
â”‚  â”œâ”€ Batch: 6 (gradient_accumulation: 5 â†’ effective 30)
â”‚  â”œâ”€ Epochs: 6
â”‚  â”œâ”€ Loss: CrossEntropy (classification) + MSE (subscores)
â”‚  â””â”€ Validation: Every 400 steps
â””â”€ Multi-task: Level classification + subscore prediction + recommendations

BÆ°á»›c 3: Integration vá»›i Pipeline
â”œâ”€ Trigger: Every 5-10 conversations hoáº·c user request
â”œâ”€ Input collection:
â”‚  â”œâ”€ Aggregate last 10 conversations
â”‚  â”œâ”€ Compute stats: grammar error rate, vocab distribution, etc.
â”‚  â””â”€ Format features into prompt
â”œâ”€ Inference:
â”‚  â”œâ”€ Model: Qwen2.5 + Proficiency LoRA adapter
â”‚  â”œâ”€ Decoding: temperature=0.2 (balanced)
â”‚  â”œâ”€ Time: ~200ms (processing aggregated data)
â”‚  â””â”€ Cache: Store result for 24h (avoid re-computation)
â””â”€ Output: JSON response vá»›i level + recommendations

BÆ°á»›c 4: Progress Tracking (Temporal Analysis)
â”œâ”€ Storage: SQLite database
â”‚  â”œâ”€ Table: user_assessments (user_id, date, level, subscores)
â”‚  â”œâ”€ History: LÆ°u 50 assessments gáº§n nháº¥t
â”‚  â””â”€ Trend: Calculate improvement rate per month
â”œâ”€ Visualization:
â”‚  â”œâ”€ Line chart: CEFR level over time
â”‚  â”œâ”€ Radar chart: Subscores (grammar, vocab, fluency, pronunciation)
â”‚  â””â”€ Milestone badges: "Reached B1!", "Grammar Master"
â””â”€ Adaptive difficulty: Adjust exercise difficulty based on current level

BÆ°á»›c 5: Evaluation Metrics
â”œâ”€ Classification accuracy: > 0.85 (Â±1 level tolerance: 0.94)
â”œâ”€ Subscore MAE: < 0.8 (on 10-point scale)
â”œâ”€ Cohen's Kappa: > 0.78 (agreement vá»›i human raters)
â”œâ”€ Prediction stability: Low variance across 3 consecutive assessments
â””â”€ Inference time: ~200ms (aggregated data processing included)
```

**Advantages cá»§a approach nÃ y**:
1. **Holistic**: ÄÃ¡nh giÃ¡ nhiá»u chiá»u (grammar, vocab, fluency, pronunciation)
2. **Personalized**: Recommendations dá»±a trÃªn weaknesses cá»¥ thá»ƒ
3. **Temporal**: Track progress over time â†’ motivate learners
4. **Unified**: Reuse Qwen2.5 base model â†’ chá»‰ thÃªm LoRA adapter (50-100MB)
5. **Explainable**: Cung cáº¥p subscores vÃ  reasoning rÃµ rÃ ng

**Implementation Example**:
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json

# Load proficiency assessment model
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)
proficiency_model = PeftModel.from_pretrained(
    base_model, 
    "path/to/proficiency-lora-adapter"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Collect user data (from last 10 conversations)
user_data = {
    "conversations": [...],  # Last 10 conversations
    "grammar_errors": {"verb_tense": 8, "articles": 5, "prepositions": 3},
    "vocab_distribution": {"A2": 65, "B1": 30, "B2": 5},
    "avg_fluency": 0.73,
    "pronunciation_score": 7.2
}

# Assessment prompt
prompt = f"""Assess the English proficiency level based on the following data:

Grammar Errors (last 10 conversations):
- Verb tense errors: 8
- Article errors: 5
- Preposition errors: 3

Vocabulary Usage:
- A2 level words: 65%
- B1 level words: 30%
- B2 level words: 5%

Fluency: Average score 0.73/1.0
Pronunciation: Average score 7.2/10

Provide:
1. Overall CEFR level (A1/A2/B1/B2/C1) with confidence
2. Subscores for Grammar, Vocabulary, Fluency, Pronunciation (out of 10)
3. Top 3 weaknesses
4. 3 specific learning recommendations"""

messages = [{"role": "user", "content": prompt}]
inputs = tokenizer.apply_chat_template(
    messages, 
    add_generation_prompt=True,
    return_tensors="pt"
).to("cuda")

with torch.no_grad():
    outputs = proficiency_model.generate(
        inputs,
        max_new_tokens=300,
        temperature=0.2,
        top_p=0.9,
        do_sample=True
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Parse response (extract JSON-like structure)
result = parse_assessment_response(response)
print(json.dumps(result, indent=2))
```

**Output Example**:
```json
{
  "timestamp": "2026-01-14T10:30:00Z",
  "current_level": "A2",
  "confidence": 0.87,
  "subscores": {
    "grammar": 6.5,
    "vocabulary": 7.2,
    "fluency": 6.8,
    "pronunciation": 7.0
  },
  "overall_score": 6.9,
  "weaknesses": [
    "Past tense verb conjugation (8 errors)",
    "Article usage (a/an/the) (5 errors)",
    "Preposition selection (3 errors)"
  ],
  "recommendations": [
    "Practice irregular past tense verbs with flashcards",
    "Review article rules: Use 'a/an' for countable singular nouns",
    "Study common preposition pairs (arrive at, interested in)"
  ],
  "progress": {
    "last_assessment": "2026-01-07",
    "level_change": "Stable (A2 â†’ A2)",
    "subscore_change": {
      "grammar": "+0.3",
      "vocabulary": "+0.5",
      "fluency": "+0.1",
      "pronunciation": "0.0"
    },
    "improvement_rate": "+0.15/week"
  }
}
```

**Integration vÃ o kiáº¿n trÃºc**:
```
[User Conversation History]
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Feature Aggregator         â”‚
â”‚  (Collect last 10 sessions)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qwen2.5 + Proficiency LoRA   â”‚
â”‚  (Holistic Assessment)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Progress Tracker           â”‚
â”‚  (Store + Visualize Trends)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
[Dashboard: Level + Recommendations + Progress Chart]
```

---

### 4.6. Module Ä‘Ã¡nh giÃ¡ phÃ¡t Ã¢m

**FR-PRO-01**: ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c phÃ¡t Ã¢m á»Ÿ cáº¥p Ä‘á»™ phoneme.

**FR-PRO-02**: PhÃ¡t hiá»‡n lá»—i phÃ¡t Ã¢m phá»• biáº¿n.

**FR-PRO-03**: So sÃ¡nh Ã¢m thanh ngÆ°á»i dÃ¹ng vá»›i native speaker.

**MÃ´ hÃ¬nh Ä‘á» xuáº¥t (Recommended)**:

**HuBERT-large + Phoneme Alignment (Preferred)**
- Base: facebook/hubert-large-ls960 (960M params, pre-trained trÃªn LibriSpeech)
- Task: Acoustic phoneme recognition
- Advantage: State-of-the-art speech representations (SUPERB Benchmark)
- Output: Phoneme sequence + confidence scores

**Alternative**: wav2vec 2.0-large (Lighter, ~360M params)

**Quy trÃ¬nh thá»±c hiá»‡n**:

```
BÆ°á»›c 1: Audio Preprocessing
â”œâ”€ Input: User audio (.wav/.mp3)
â”œâ”€ Resample: 16kHz (HuBERT requirement)
â”œâ”€ Mono conversion: If stereo, mix down
â”œâ”€ Normalize: Peak normalization to -3dB
â””â”€ Split: If > 30s, chunk into 10s segments

BÆ°á»›c 2: Phoneme Recognition (HuBERT-large)
â”œâ”€ Model: HuBERT fine-tuned on TIMIT dataset
â”œâ”€ Feature extraction: MFCC + log-Mel spectrogram
â”œâ”€ CTC decoding: Connectionist Temporal Classification
â”œâ”€ Output: Phoneme sequence + frame-level confidence
â””â”€ Inventory: 44 phonemes (ARPAbet: AH, EH, IY, etc.)

VÃ­ dá»¥:
Audio Input: "She goes to school"
Phoneme output: [SH, IY, G, OW, Z, T, OW, S, K, UW, L]
Confidence: [0.98, 0.96, 0.99, 0.94, 0.91, 0.97, 0.95, 0.98, 0.96, 0.92, 0.99]

BÆ°á»›c 3: Forced Alignment (Align with Reference)
â”œâ”€ Reference (Native speaker):
â”‚  â””â”€ Text: "She goes to school"
â”‚  â””â”€ Phoneme: [SH, IY, G, OW, Z, ...] (from TTS hoáº·c pre-recorded)
â”‚  â””â”€ Timing: [0.0-0.2s, 0.2-0.4s, ...] (frame duration)
â”œâ”€ Alignment algorithm: Dynamic Time Warping (DTW) hoáº·c HMM
â”œâ”€ Output: Matched phoneme pairs (user vs reference)
â””â”€ Distance metric: Edit distance, Euclidean distance (embeddings)

BÆ°á»›c 4: Error Detection
â”œâ”€ Phoneme-level comparison:
â”‚  â”œâ”€ Substitution: /Å‹/ â†’ /n/ (sing/sin)
â”‚  â”œâ”€ Deletion: Missing phoneme
â”‚  â”œâ”€ Insertion: Extra phoneme
â”‚  â””â”€ Timing issues: Slow/fast pronunciation
â”œâ”€ Prosody analysis:
â”‚  â”œâ”€ Pitch contour: F0 trajectory comparison
â”‚  â”œâ”€ Stress pattern: Phoneme duration distribution
â”‚  â”œâ”€ Rhythm: Speech rate (syllables/sec)
â”‚  â””â”€ Intonation: Rising/falling patterns
â””â”€ Output: [Error_type, phoneme, confidence, severity]

BÆ°á»›c 5: Feedback Generation
â”œâ”€ Severity levels:
â”‚  â”œâ”€ Critical: Phoneme change meaning (live/leave)
â”‚  â”œâ”€ Medium: Accent-like, understandable (w/v confusion)
â”‚  â””â”€ Minor: Native variation, acceptable
â”œâ”€ Correction samples:
â”‚  â””â”€ Play native pronunciation for problematic phoneme
â””â”€ Cultural/accent awareness:
     â””â”€ Accept common English variants (rhotic vs non-rhotic)
```

**Implementation Architecture**:

```python
import librosa
import torch
from transformers import HubertForCTC, Wav2Vec2Processor
from scipy.spatial.distance import euclidean
from dtaidistance import dtw

# 1. Load HuBERT model
processor = Wav2Vec2Processor.from_pretrained("facebook/hubert-large-ls960-phoneme")
model = HubertForCTC.from_pretrained("facebook/hubert-large-ls960-phoneme")

# 2. Process user audio
audio, sr = librosa.load(audio_path, sr=16000)
inputs = processor(audio, return_tensors="pt", sampling_rate=16000)

# 3. Get phoneme predictions
with torch.no_grad():
    logits = model(**inputs).logits

predictions = torch.argmax(logits, dim=-1)
phoneme_sequence = processor.decode(predictions[0])

# 4. Load reference (native pronunciation)
reference_phonemes = get_reference_phonemes(transcription)  # from DB or TTS

# 5. Alignment & Error detection
errors = detect_phoneme_errors(
    user_phonemes=phoneme_sequence,
    reference_phonemes=reference_phonemes,
    confidence_scores=get_confidence(logits)
)

# 6. Generate feedback
feedback = generate_pronunciation_feedback(errors)
```

**Output**:
```json
{
  "text": "She goes to school",
  "phonemes": "SH-IY-G-OW-Z-T-UW-S-K-UW-L",
  "pronunciation_score": 0.85,
  "phoneme_errors": [
    {
      "position": 7,
      "phoneme_user": "Z",
      "phoneme_reference": "Z",
      "error_type": "Duration",
      "duration_user": 0.15,
      "duration_reference": 0.12,
      "severity": "minor",
      "feedback": "Slightly longer /z/ sound"
    },
    {
      "position": 9,
      "phoneme_user": "K",
      "phoneme_reference": "K",
      "error_type": "Pronunciation",
      "ipa_user": "kÊ°",
      "ipa_reference": "k",
      "severity": "medium",
      "feedback": "Reduce aspiration on /k/ before vowel"
    }
  ],
  "prosody": {
    "stress_pattern": "Correct",
    "intonation": "Falling (appropriate for statement)",
    "speech_rate": "1.2 syllables/sec (slightly fast)"
  },
  "overall_assessment": "Good pronunciation with minor rhythm issues"
}
```

**Fine-tuning cho Custom Accent/Dialect** (Optional):
- Collect: 500-1000 labeled audio samples tá»« target learners
- Loss: CTC loss + contrastive loss (pull correct phonemes closer)
- Training: 5-10 epochs, learning_rate=1e-4
- Validation: Phoneme error rate (PER) < 15%

---

### 4.6. Module Text-to-Speech (TTS)

**FR-TTS-01**: Chuyá»ƒn vÄƒn báº£n pháº£n há»“i thÃ nh giá»ng nÃ³i tá»± nhiÃªn, chuáº©n má»±c.

**FR-TTS-02**: Hoáº¡t Ä‘á»™ng tá»‘t trÃªn **mobile CPU**; Ä‘á»™ trá»… < 500ms cho 10s output.

**FR-TTS-03**: Há»— trá»£ Ä‘iá»u khiá»ƒn prosody (pitch, speed).

**FR-TTS-04**: Há»— trá»£ **offline mode** hoÃ n toÃ n trÃªn mobile.

---

#### LÆ°u Ã½ vá» FastPitch + HiFi-GAN

FastPitch + HiFi-GAN **KHÃ”NG phÃ¹ há»£p cho mobile deployment**:

| Yáº¿u tá»‘ | FastPitch + HiFi-GAN | Mobile Requirement |
|--------|----------------------|-------------------|
| **RAM Runtime** | ~500MB - 1GB | QuÃ¡ náº·ng |
| **Inference (CPU)** | 2-5 giÃ¢y | QuÃ¡ cháº­m |
| **Real-time Factor** | 0.5-2x trÃªn CPU | KhÃ´ng real-time |

â†’ Chá»‰ phÃ¹ há»£p cho **server-side deployment** hoáº·c **desktop vá»›i GPU**.

---

#### MÃ´ hÃ¬nh Ä‘á» xuáº¥t cho Mobile (Recommended Stack)

**Kiáº¿n trÃºc Hybrid: Native TTS + Piper TTS + Cloud TTS**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LEXILINGO MOBILE TTS ARCHITECTURE              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           TIER 1: NATIVE OS TTS (Default)           â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚
â”‚  â”‚  â€¢ iOS: AVSpeechSynthesizer                         â”‚   â”‚
â”‚  â”‚  â€¢ Android: TextToSpeech API                        â”‚   â”‚
â”‚  â”‚  â€¢ Size: 0 MB (built-in)                            â”‚   â”‚
â”‚  â”‚  â€¢ Latency: < 100ms                                 â”‚   â”‚
â”‚  â”‚  â€¢ Quality: â­â­â­ (MOS ~3.5)                        â”‚   â”‚
â”‚  â”‚  â€¢ Use case: Regular AI responses, quick feedback   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         TIER 2: PIPER TTS (Enhanced Quality)        â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚
â”‚  â”‚  â€¢ Model: rhasspy/piper (VITS-based)                â”‚   â”‚
â”‚  â”‚  â€¢ Size: 30-60 MB per voice                         â”‚   â”‚
â”‚  â”‚  â€¢ Latency: 100-300ms (real-time on mobile CPU)     â”‚   â”‚
â”‚  â”‚  â€¢ Quality: â­â­â­â­ (MOS ~3.8-4.0)                  â”‚   â”‚
â”‚  â”‚  â€¢ Offline: 100%                                 â”‚   â”‚
â”‚  â”‚  â€¢ Use case: Pronunciation demos, lesson audio      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼ (Online + Premium)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          TIER 3: CLOUD TTS (Best Quality)           â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚
â”‚  â”‚  â€¢ Google Cloud TTS / Azure Neural TTS              â”‚   â”‚
â”‚  â”‚  â€¢ Latency: 300-800ms (network dependent)           â”‚   â”‚
â”‚  â”‚  â€¢ Quality: â­â­â­â­â­ (MOS ~4.3-4.5)               â”‚   â”‚
â”‚  â”‚  â€¢ Use case: Critical pronunciation, premium users  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 1. Native OS TTS (Primary - Zero Cost)

**iOS - AVSpeechSynthesizer**:
```swift
import AVFoundation

let synthesizer = AVSpeechSynthesizer()
let utterance = AVSpeechUtterance(string: "Hello, how are you?")
utterance.voice = AVSpeechSynthesisVoice(language: "en-US")
utterance.rate = 0.5  // Speed control (0.0 - 1.0)
utterance.pitchMultiplier = 1.0  // Pitch control
synthesizer.speak(utterance)
```

**Android - TextToSpeech**:
```kotlin
val tts = TextToSpeech(context) { status ->
    if (status == TextToSpeech.SUCCESS) {
        tts.language = Locale.US
        tts.setSpeechRate(1.0f)
        tts.speak("Hello, how are you?", TextToSpeech.QUEUE_FLUSH, null, null)
    }
}
```

**Flutter Implementation**:
```dart
import 'package:flutter_tts/flutter_tts.dart';

class NativeTTSService {
  final FlutterTts _tts = FlutterTts();
  
  Future<void> init() async {
    await _tts.setLanguage("en-US");
    await _tts.setSpeechRate(0.5);  // 0.0 - 1.0
    await _tts.setPitch(1.0);       // 0.5 - 2.0
    await _tts.setVolume(1.0);      // 0.0 - 1.0
    
    // Get available voices
    final voices = await _tts.getVoices;
    // Select high-quality voice if available
    final enhancedVoice = voices.firstWhere(
      (v) => v['name'].contains('Enhanced') || v['name'].contains('Neural'),
      orElse: () => voices.first,
    );
    await _tts.setVoice(enhancedVoice);
  }
  
  Future<void> speak(String text) async {
    await _tts.speak(text);
  }
  
  Future<void> stop() async {
    await _tts.stop();
  }
}
```

---

#### 2. Piper TTS (Enhanced Quality - Offline)

**Äáº·c Ä‘iá»ƒm**:
- Architecture: VITS (Variational Inference TTS) - end-to-end
- Model size: 30-60 MB per voice
- Inference: Real-time trÃªn mobile ARM CPU
- Quality: MOS ~3.8-4.0 (near-human)
- Voices: 20+ English voices available

**Available English Voices**:
| Voice | Gender | Accent | Size | Quality |
|-------|--------|--------|------|---------|
| `en_US-amy-medium` | Female | American | 45 MB | â­â­â­â­ |
| `en_US-ryan-medium` | Male | American | 42 MB | â­â­â­â­ |
| `en_GB-alba-medium` | Female | British | 48 MB | â­â­â­â­ |
| `en_US-lessac-medium` | Female | American | 60 MB | â­â­â­â­â­ |

**Flutter Integration**:
```dart
// Using piper_tts package (community)
import 'package:piper_tts/piper_tts.dart';

class PiperTTSService {
  late PiperTts _piper;
  
  Future<void> init() async {
    _piper = PiperTts();
    
    // Download voice model on first use
    await _piper.downloadVoice('en_US-amy-medium');
    await _piper.loadVoice('en_US-amy-medium');
  }
  
  Future<Uint8List> synthesize(String text) async {
    // Returns raw audio bytes (WAV format)
    final audioData = await _piper.synthesize(text);
    return audioData;
  }
  
  Future<void> speak(String text) async {
    final audio = await synthesize(text);
    await _playAudio(audio);
  }
}
```

**Native Integration (Performance)**:
```
Piper TTS cÃ³ thá»ƒ tÃ­ch há»£p native qua:
â”œâ”€ iOS: Compile vá»›i Swift/Objective-C wrapper
â”œâ”€ Android: JNI vá»›i C++ library
â””â”€ Flutter: Platform channel + native code

Performance trÃªn mobile:
â”œâ”€ iPhone 12+: ~50-100ms cho 10 tá»«
â”œâ”€ Android (Snapdragon 8 Gen 1): ~80-150ms
â””â”€ Android (mid-range): ~150-300ms
```

---

#### 3. Cloud TTS (Premium Quality)

**Google Cloud TTS**:
```dart
import 'package:googleapis/texttospeech/v1.dart';

class GoogleTTSService {
  final TexttospeechApi _api;
  
  Future<Uint8List> synthesize(String text) async {
    final request = SynthesizeSpeechRequest(
      input: SynthesisInput(text: text),
      voice: VoiceSelectionParams(
        languageCode: 'en-US',
        name: 'en-US-Neural2-J',  // Neural voice
        ssmlGender: 'MALE',
      ),
      audioConfig: AudioConfig(
        audioEncoding: 'MP3',
        speakingRate: 1.0,
        pitch: 0.0,
      ),
    );
    
    final response = await _api.text.synthesize(request);
    return base64Decode(response.audioContent!);
  }
}
```

**Azure Neural TTS** (Best quality):
```dart
// Azure offers the most natural-sounding voices
// Recommended for pronunciation demonstrations
final azureVoices = [
  'en-US-JennyNeural',    // Female, conversational
  'en-US-GuyNeural',      // Male, professional
  'en-GB-SoniaNeural',    // British female
  'en-AU-NatashaNeural',  // Australian female
];
```

---

#### Hybrid TTS Router

```dart
enum TTSQuality { standard, enhanced, premium }
enum TTSUseCase { response, pronunciation, lesson }

class HybridTTSService {
  final NativeTTSService _native;
  final PiperTTSService _piper;
  final CloudTTSService _cloud;
  
  Future<void> speak(
    String text, {
    TTSUseCase useCase = TTSUseCase.response,
    bool forceOffline = false,
  }) async {
    final hasInternet = await _checkConnectivity();
    final isPremiumUser = await _checkPremiumStatus();
    
    // Routing logic
    if (forceOffline || !hasInternet) {
      // Offline: Use Piper if available, fallback to Native
      if (await _piper.isReady()) {
        await _piper.speak(text);
      } else {
        await _native.speak(text);
      }
    } else if (useCase == TTSUseCase.pronunciation && isPremiumUser) {
      // Pronunciation demo: Use Cloud for best quality
      await _cloud.speak(text);
    } else if (useCase == TTSUseCase.lesson) {
      // Lesson content: Use Piper for good quality offline
      await _piper.speak(text);
    } else {
      // Regular response: Use Native for speed
      await _native.speak(text);
    }
  }
}
```

---

#### So sÃ¡nh cÃ¡c giáº£i phÃ¡p TTS

| Feature | Native TTS | Piper TTS | Cloud TTS |
|---------|------------|-----------|-----------|
| **Size** | 0 MB | 30-60 MB | 0 MB |
| **Latency** | <100ms | 100-300ms | 300-800ms |
| **Quality (MOS)** | 3.5 | 3.8-4.0 | 4.3-4.5 |
| **Offline** | | | |
| **Cost** | Free | Free | $4-16/1M chars |
| **Prosody Control** | Basic | Good | Excellent |
| **Voice Variety** | OS dependent | 20+ voices | 100+ voices |
| **Mobile Optimized** | | | N/A |

---

#### Pipeline chi tiáº¿t

```
BÆ°á»›c 1: Text Preprocessing
â”œâ”€ Input: Text tá»« response generator
â”œâ”€ Normalization:
â”‚  â”œâ”€ Expand abbreviations (Dr. â†’ Doctor)
â”‚  â”œâ”€ Number-to-word (123 â†’ one hundred twenty three)
â”‚  â””â”€ Emoji removal hoáº·c conversion
â””â”€ Output: Cleaned text string

BÆ°á»›c 2: TTS Selection
â”œâ”€ Check use case (response/pronunciation/lesson)
â”œâ”€ Check network status
â”œâ”€ Check user tier (free/premium)
â””â”€ Select appropriate TTS engine

BÆ°á»›c 3: Synthesis
â”œâ”€ Native TTS: Direct API call
â”œâ”€ Piper TTS: Model inference â†’ WAV bytes
â””â”€ Cloud TTS: API request â†’ MP3/WAV bytes

BÆ°á»›c 4: Audio Playback
â”œâ”€ Native: System audio player
â”œâ”€ Piper/Cloud: audioplayers package
â””â”€ Queue management for sequential playback

BÆ°á»›c 5: Caching (Optional)
â”œâ”€ Cache frequently used phrases
â”œâ”€ Pre-generate lesson audio
â””â”€ Store in local storage
```

**Output Example**:
```json
{
  "input_text": "You speak English well!",
  "tts_engine": "piper",
  "voice": "en_US-amy-medium",
  "audio_format": "wav",
  "sample_rate": 22050,
  "duration_seconds": 1.8,
  "inference_time_ms": 120,
  "cached": false
}
```

---

#### Server-side TTS (For Pre-generated Content)

Äá»‘i vá»›i **lesson audio pre-generation** trÃªn server, cÃ³ thá»ƒ sá»­ dá»¥ng FastPitch + HiFi-GAN:

```python
# Server-side only - Pre-generate lesson audio
from fastpitch import FastPitch
from hifigan import Generator

# Generate high-quality audio for lessons (offline processing)
def generate_lesson_audio(lesson_texts: list[str]) -> list[bytes]:
    fastpitch = FastPitch.load_from_checkpoint("fastpitch.ckpt")
    hifigan = Generator.load_from_checkpoint("hifigan.ckpt")
    
    audio_files = []
    for text in lesson_texts:
        mel = fastpitch(text)
        waveform = hifigan(mel)
        audio_files.append(waveform_to_bytes(waveform))
    
    return audio_files

# Upload to CDN, download to mobile for offline playback
```

---

### 4.7. Module Dialogue Response Generation (AI Orchestrator)

**FR-ORCH-01**: Táº¡o pháº£n há»“i há»™i thoáº¡i phÃ¹ há»£p vá»›i trÃ¬nh Ä‘á»™ ngÆ°á»i dÃ¹ng.

**FR-ORCH-02**: TÃ­ch há»£p feedback tá»« cÃ¡c module phÃ¢n tÃ­ch thÃ nh cÃ¢u tráº£ lá»i liá»n máº¡ch.

**FR-ORCH-03**: Äáº£m báº£o Ä‘á»™ trá»… tháº¥p (tá»•ng < 2 giÃ¢y).

**MÃ´ hÃ¬nh Ä‘á» xuáº¥t (theo chiáº¿n lÆ°á»£c Dev/Prod)**:

**DEVELOPMENT MODE (Mac 32GB RAM):**

**Option 1: Qwen2.5-1.5B-Instruct fine-tuned** (Recommended - Unified)
- Base: Qwen/Qwen2.5-1.5B-Instruct (1.5B params)
- **Æ¯u Ä‘iá»ƒm chÃ­nh**: Sá»­ dá»¥ng cÃ¹ng 1 model cho Táº¤T Cáº¢ tasks
  - Multi-task LoRA: 4 adapters (fluency, vocab, grammar, dialogue)
  - Shared base model: Tiáº¿t kiá»‡m RAM (chá»‰ load 1 láº§n)
  - Consistent quality across tasks
- Advantages:
  - Instruction-tuned: Excellent dialogue understanding
  - Long context (32K): Remember full conversation history
  - Reasoning: Natural explanations ("because...", "you should...")
  - Multilingual: Can explain in Vietnamese if needed
- Size: ~900MB (Q4), RAM: ~2GB
- Quality: â­â­â­â­â­ (96% human-like responses)
- Response time: ~200ms (CPU), ~50ms (GPU M1)

**Option 2: Llama-3.2-1B-Instruct fine-tuned** (Alternative)
- Base: meta-llama/Llama-3.2-1B-Instruct (1.2B params)
- Release: September 2024 (Meta's latest small model)
- Advantages:
  - State-of-the-art for <2B models
  - Excellent instruction following
  - Strong multilingual (128K vocab)
  - Better at creative responses
- Size: ~600MB (Q4), RAM: ~1.5GB
- Quality: â­â­â­â­â­ (95% human-like)
- Response time: ~180ms (CPU)

**ğŸ“± PRODUCTION MODE (Mobile Devices):**

**Option 1: Qwen2.5-0.5B-Instruct fine-tuned** (Best Mobile)
- Parameters: 0.5B (3x smaller than 1.5B)
- Size: ~300MB (Q4), RAM: ~600MB
- Quality: â­â­â­â­ (91% quality, only 5% drop from 1.5B)
- Response time: ~100ms (mobile CPU)
- Knowledge distillation: Trained from 1.5B teacher
- Battery: ~0.3% per minute of conversation
- Works offline: No internet required

**Option 2: SmolLM2-360M-Instruct fine-tuned** (Ultra-light)
- Base: HuggingFaceTB/SmolLM2-360M-Instruct (360M params)
- Release: November 2024 (HuggingFace's SmolLM2 series)
- Size: ~200MB (Q4), RAM: ~400MB
- Quality: â­â­â­â­ (88% quality)
- Response time: ~80ms (mobile CPU)
- Best for: Low-end devices (<4GB RAM)
- Training: 11T tokens (SmolLM2 is SOTA for <500M)
- Battery: ~0.2% per minute

**Quy trÃ¬nh thá»±c hiá»‡n (Multi-Task Unified Model)**:

```
BÆ°á»›c 1: Context Assembly (Dynamic Routing)
â”œâ”€ Unified model approach: 1 base model + 4 LoRA adapters
â”œâ”€ Input sources:
â”‚  â”œâ”€ User transcription: "I like learning English"
â”‚  â”œâ”€ Analysis results (from same model, different adapters):
â”‚  â”‚  â”œâ”€ fluency_score: 0.87 (from fluency adapter)
â”‚  â”‚  â”œâ”€ vocabulary_level: "B1" (from vocab adapter)
â”‚  â”‚  â”œâ”€ grammar_errors: [] (from grammar adapter)
â”‚  â”‚  â”œâ”€ pronunciation_issues: [minor_stress] (from external ASR)
â”‚  â”‚  â””â”€ user_proficiency: "B1" (user profile)
â”‚  â””â”€ Conversation history: Last 5 turns (stored in context)
â”œâ”€ Construct comprehensive prompt:
â”‚  â”œâ”€ System: "You are an encouraging English tutor"
â”‚  â”œâ”€ Context: Full analysis + history
â”‚  â”œâ”€ Task: Generate appropriate response
â”‚  â””â”€ Constraints: Match user level, be encouraging
â””â”€ Feed to dialogue adapter of Qwen2.5

VÃ­ dá»¥ prompt (Instruction format):
"
<|im_start|>system
You are an encouraging English learning tutor. The user is at B1 level.<|im_end|>
<|im_start|>user
Context:
- User said: 'I like learning English'
- Fluency score: 0.87/1.0 (good)
- Vocabulary: B1 level (appropriate)
- Grammar: No errors detected
- Pronunciation: Minor stress on 'learning'
- Previous turns: [User asked about present perfect 2 turns ago]

Generate a response that:
1) Acknowledges their statement positively
2) Provides one helpful tip (related to pronunciation)
3) Asks a follow-up question to continue dialogue
4) Uses B1-level language (simple but not patronizing)<|im_end|>
<|im_start|>assistant
"

BÆ°á»›c 2: Multi-Task Dataset Preparation (Total ~14,000 examples)

â”œâ”€ Task distribution:
â”‚  â”œâ”€ Fluency scoring: 1,500 examples (10.7%)
â”‚  â”œâ”€ Vocabulary classification: 2,500 examples (17.9%)
â”‚  â”œâ”€ Grammar correction: 9,200 examples (65.7%)
â”‚  â””â”€ Dialogue generation: 800 examples (5.7%)
â”œâ”€ Dialogue dataset sources (800 examples):
â”‚  â”œâ”€ ESL tutoring transcripts: 300 examples
â”‚  â”‚  â””â”€ Real teacher-student interactions
â”‚  â”œâ”€ Language exchange forums: 200 examples
â”‚  â”‚  â””â”€ HelloTalk, Tandem logs (anonymized)
â”‚  â”œâ”€ English learning chatbots: 200 examples
â”‚  â”‚  â””â”€ Duolingo, Busuu conversations
â”‚  â””â”€ Synthetic generation: 100 examples
â”‚      â””â”€ GPT-4 generated conversations (quality-checked)
â”œâ”€ Format: (input_context, target_response)
â”œâ”€ Label method: Human annotation (teachers/native speakers)
â”œâ”€ Diversity:
â”‚  â”œâ”€ Different proficiency levels (A2, B1, B2)
â”‚  â”œâ”€ Different error types (grammar, pronunciation, vocabulary)
â”‚  â”œâ”€ Different response styles (encouragement, correction, question)
â”‚  â””â”€ Dialogue continuity (context-aware responses)
â”œâ”€ Data split:
â”‚  â”œâ”€ Train: 70% (1,050 examples)
â”‚  â”œâ”€ Val: 15% (300 examples)
â”‚  â””â”€ Test: 15% (300 examples)
â””â”€ Augmentation (optional):
    â”œâ”€ Paraphrase user input (keep meaning)
    â”œâ”€ Vary response style (formal/informal)
    â””â”€ Synthetic error injection (back-translation)

BÆ°á»›c 3: Multi-Task LoRA Architecture

Architecture (Qwen2.5-1.5B base):
â”œâ”€ Decoder-only Transformer: 28 layers, 1.5B params
â”œâ”€ Hidden size: 1,536
â”œâ”€ Attention heads: 12
â”œâ”€ Vocabulary: 151,936 tokens (multilingual)
â”œâ”€ Context window: 32K tokens
â””â”€ LoRA adapters (4 task-specific):
    â”œâ”€ Fluency LoRA: r=32, alpha=64, modules=[q_proj, v_proj]
    â”œâ”€ Vocabulary LoRA: r=32, alpha=64, modules=[q_proj, v_proj]
    â”œâ”€ Grammar LoRA: r=32, alpha=64, modules=[all attention + MLP]
    â””â”€ Dialogue LoRA: r=32, alpha=64, modules=[all attention + MLP]

Training Configuration (Development Mode - Mac 32GB):
â”œâ”€ Multi-task strategy: Sequential task training with shared base
â”œâ”€ Phase 1: Train all tasks together (epoch 1-3)
â”‚  â”œâ”€ Task sampling: Proportional to dataset size
â”‚  â”œâ”€ Batch composition: Mixed tasks in each batch
â”‚  â””â”€ Loss: Weighted sum (grammar: 0.4, dialogue: 0.3, others: 0.15 each)
â”œâ”€ Phase 2: Fine-tune each task separately (epoch 4-5)
â”‚  â”œâ”€ Load best multi-task checkpoint
â”‚  â”œâ”€ Train each LoRA adapter independently
â”‚  â””â”€ Prevent catastrophic forgetting with regularization
â”œâ”€ Optimizer: AdamW (lr: 3e-4, weight_decay: 0.01)
â”œâ”€ Batch size: 8 (gradient_accumulation: 4 â†’ effective 32)
â”œâ”€ Epochs: 5 total (3 multi-task + 2 per-task)
â”œâ”€ Scheduler: Cosine with warmup (warmup_ratio: 0.03)
â”œâ”€ Precision: bfloat16 (M1/M2) or float16 (NVIDIA)
â”œâ”€ Gradient clipping: 1.0
â”œâ”€ Dropout: 0.05 (in LoRA layers)
â””â”€ Early stopping: patience=2 per task

BÆ°á»›c 4: Production Model (Knowledge Distillation)
â”œâ”€ Teacher: Qwen2.5-1.5B with all 4 LoRA adapters
â”œâ”€ Student: Qwen2.5-0.5B (3x smaller)
â”œâ”€ Distillation process:
â”‚  â”œâ”€ Generate soft labels from teacher on training set
â”‚  â”œâ”€ Train student with: Î±*KL_div(teacher, student) + (1-Î±)*task_loss
â”‚  â”œâ”€ Î± = 0.7 (70% distillation, 30% hard labels)
â”‚  â””â”€ Temperature: 2.0 (soften distributions)
â”œâ”€ LoRA config (student): r=16, alpha=32 (lighter)
â”œâ”€ Training: 4 epochs, batch_size=12
â”œâ”€ Validation: Compare student vs teacher on all tasks
â””â”€ Result: 91% teacher performance (5% quality drop)

BÆ°á»›c 5: Inference Pipeline (Runtime)

Development Mode (Qwen2.5-1.5B):
â”œâ”€ Load base model once (~900MB Q4)
â”œâ”€ Load 4 LoRA adapters (~100MB total)
â”œâ”€ Runtime memory: ~2GB
â”œâ”€ Adapter switching: <1ms (just change weights)
â”œâ”€ Sequential execution:
â”‚  1. Fluency adapter â†’ score (80ms)
â”‚  2. Vocab adapter â†’ level (80ms)
â”‚  3. Grammar adapter â†’ corrections (150ms)
â”‚  4. Dialogue adapter â†’ response (200ms)
â”‚  â””â”€ Total: ~510ms (parallel optimization possible)
â””â”€ Output: Complete feedback package

Production Mode (Qwen2.5-0.5B):
â”œâ”€ Load base model (~300MB Q4)
â”œâ”€ Load adapters (~50MB total)
â”œâ”€ Runtime memory: ~600MB
â”œâ”€ Sequential execution:
â”‚  1. Fluency: 50ms
â”‚  2. Vocab: 50ms
â”‚  3. Grammar: 100ms
â”‚  4. Dialogue: 100ms
â”‚  â””â”€ Total: ~300ms
â””â”€ Mobile-optimized: ONNX or CoreML export

BÆ°á»›c 6: Validation Metrics (Per Task)

Dialogue Response Quality:
â”œâ”€ BLEU score: > 38 (vs > 35 for Flan-T5)
â”œâ”€ ROUGE-L: > 0.45 (vs 0.40)
â”œâ”€ METEOR: > 0.40 (vs 0.35)
â”œâ”€ BERTScore F1: > 0.88
â”œâ”€ Perplexity: < 35 (vs 50 for Flan-T5)
â”œâ”€ Response relevance: > 4.3/5.0 (human eval)
â”œâ”€ Encouragement tone: > 4.5/5.0
â”œâ”€ Grammar appropriateness: > 4.2/5.0
â””â”€ Level matching: > 90% (uses B1 when should)

Overall System Performance:
â”œâ”€ Fluency: MAE < 0.12, Pearson > 0.90
â”œâ”€ Vocabulary: F1 > 0.89, Accuracy > 0.90
â”œâ”€ Grammar: F0.5 > 68, Precision > 78%
â”œâ”€ Dialogue: BLEU > 38, ROUGE-L > 0.45
â”œâ”€ End-to-end latency: < 600ms (dev), < 400ms (prod)
â””â”€ Multi-task advantage: Consistent quality across all tasks
```

**Implementation Stack (Unified Multi-Task Model)**:

```python
# Complete implementation vá»›i multi-task Qwen2.5
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
from typing import Dict, List

class UnifiedLexiLingoModel:
    """
    Unified model cho Táº¤T Cáº¢ tasks:
    - Fluency scoring
    - Vocabulary classification
    - Grammar correction
    - Dialogue response generation
    """
    
    def __init__(self, base_model_path: str, adapter_paths: Dict[str, str]):
        """
        Args:
            base_model_path: Qwen/Qwen2.5-1.5B-Instruct (dev) hoáº·c 0.5B (prod)
            adapter_paths: Dict mapping task_name â†’ LoRA adapter path
                Example: {
                    "fluency": "path/to/fluency-adapter",
                    "vocabulary": "path/to/vocab-adapter",
                    "grammar": "path/to/grammar-adapter",
                    "dialogue": "path/to/dialogue-adapter"
                }
        """
        # Load base model (chá»‰ 1 láº§n!)
        self.base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,  # M1/M2 Mac optimize
            device_map="auto",
            low_cpu_mem_usage=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        
        # Load all LoRA adapters
        self.adapters = {}
        for task_name, adapter_path in adapter_paths.items():
            self.adapters[task_name] = PeftModel.from_pretrained(
                self.base_model,
                adapter_path,
                adapter_name=task_name
            )
        
        self.current_adapter = None
    
    def switch_adapter(self, task_name: str):
        """Switch to specific task adapter (< 1ms)"""
        if task_name not in self.adapters:
            raise ValueError(f"Unknown task: {task_name}")
        self.current_adapter = task_name
        self.adapters[task_name].set_adapter(task_name)
    
    def _generate(self, prompt: str, max_new_tokens: int = 100, 
                  temperature: float = 0.1) -> str:
        """Internal generation method"""
        messages = [{"role": "user", "content": prompt}]
        inputs = self.tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.base_model.device)
        
        with torch.no_grad():
            outputs = self.base_model.generate(
                inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=(temperature > 0),
                top_k=5 if temperature > 0 else None
            )
            response = self.tokenizer.decode(
                outputs[0][len(inputs[0]):],
                skip_special_tokens=True
            )
        return response.strip()
    
    def evaluate_fluency(self, text: str) -> Dict:
        """Task 1: Fluency scoring"""
        self.switch_adapter("fluency")
        prompt = f"""Rate the fluency of this sentence (0.0-1.0):
Sentence: {text}
Format: Score: X.XX | Reason: ..."""
        
        response = self._generate(prompt, max_new_tokens=80)
        
        import re
        score_match = re.search(r"Score: ([0-9.]+)", response)
        reason_match = re.search(r"Reason: (.+)", response)
        
        return {
            "fluency_score": float(score_match.group(1)) if score_match else 0.5,
            "reasoning": reason_match.group(1).strip() if reason_match else ""
        }
    
    def classify_vocabulary(self, text: str) -> Dict:
        """Task 2: Vocabulary level classification"""
        self.switch_adapter("vocabulary")
        prompt = f"""Classify vocabulary level (A2/B1/B2):
Sentence: {text}
Format: Level: XX | Key words: ... | Reason: ..."""
        
        response = self._generate(prompt, max_new_tokens=100)
        
        import re
        level_match = re.search(r"Level: (A2|B1|B2)", response)
        keywords_match = re.search(r"Key words: (.+?)(?:\\||$)", response)
        
        return {
            "level": level_match.group(1) if level_match else "B1",
            "key_words": keywords_match.group(1).strip() if keywords_match else "",
            "reasoning": response
        }
    
    def correct_grammar(self, text: str) -> Dict:
        """Task 3: Grammar error correction"""
        self.switch_adapter("grammar")
        prompt = f"""Correct grammar errors:
Sentence: {text}
Provide: 1) Corrected sentence 2) Errors list 3) Explanations"""
        
        response = self._generate(prompt, max_new_tokens=200, temperature=0.1)
        
        import re
        corrected_match = re.search(r"Corrected: (.+?)(?:\\n|$)", response)
        
        return {
            "original": text,
            "corrected": corrected_match.group(1).strip() if corrected_match else text,
            "explanation": response
        }
    
    def generate_dialogue_response(self, user_input: str, 
                                   analysis: Dict,
                                   history: List[Dict] = None) -> str:
        """Task 4: Generate encouraging tutor response"""
        self.switch_adapter("dialogue")
        
        # Build context with analysis
        context_lines = [
            f"User said: '{user_input}'",
            f"Fluency: {analysis.get('fluency_score', 0.85):.2f}/1.0",
            f"Vocabulary level: {analysis.get('vocabulary_level', 'B1')}",
            f"Grammar: {analysis.get('grammar_status', 'Correct')}",
        ]
        
        if history:
            context_lines.append(f"Previous turns: {len(history)} turns")
        
        context = "\\n".join(context_lines)
        
        prompt = f"""You are an encouraging English tutor (B1 level).
Context:
{context}

Generate a response that:
1) Acknowledges positively
2) Provides helpful tip if needed
3) Asks follow-up question
Keep it simple and encouraging."""
        
        response = self._generate(prompt, max_new_tokens=150, temperature=0.7)
        return response
    
    def analyze_complete(self, text: str, history: List = None) -> Dict:
        """
        Complete analysis pipeline - all tasks in sequence
        Returns: {fluency, vocabulary, grammar, dialogue_response}
        """
        # Run all tasks
        fluency_result = self.evaluate_fluency(text)
        vocab_result = self.classify_vocabulary(text)
        grammar_result = self.correct_grammar(text)
        
        # Combine for dialogue
        analysis = {
            "fluency_score": fluency_result["fluency_score"],
            "vocabulary_level": vocab_result["level"],
            "grammar_status": "Correct" if grammar_result["corrected"] == text else "Has errors"
        }
        
        dialogue_response = self.generate_dialogue_response(text, analysis, history)
        
        return {
            "input": text,
            "fluency": fluency_result,
            "vocabulary": vocab_result,
            "grammar": grammar_result,
            "dialogue_response": dialogue_response,
            "timestamp": "2026-01-14T10:30:00Z"
        }

# Usage Example
if __name__ == "__main__":
    # Initialize model (once at startup)
    model = UnifiedLexiLingoModel(
        base_model_path="Qwen/Qwen2.5-1.5B-Instruct",  # or 0.5B for mobile
        adapter_paths={
            "fluency": "adapters/fluency-lora",
            "vocabulary": "adapters/vocabulary-lora",
            "grammar": "adapters/grammar-lora",
            "dialogue": "adapters/dialogue-lora"
        }
    )
    
    # Complete analysis
    user_input = "I like learning English every day"
    result = model.analyze_complete(user_input)
    
    print(f"Fluency: {result['fluency']['fluency_score']:.2f}")
    print(f"Vocab Level: {result['vocabulary']['level']}")
    print(f"Grammar: {result['grammar']['corrected']}")
    print(f"Response: {result['dialogue_response']}")
    
    # Output:
    # Fluency: 0.91
    # Vocab Level: B1
    # Grammar: I like learning English every day
    # Response: Great job! Your sentence shows consistency with "every day". 
    #           Try varying it: "I enjoy learning English daily" or 
    #           "Learning English is my daily habit". 
    #           What topics interest you most in English?
```

**Æ¯u Ä‘iá»ƒm cá»§a Unified Multi-Task Approach**:

1. **Memory Efficiency**: Load 1 base model (~900MB) + 4 adapters (~100MB) = **1GB total**
   - So vá»›i loading 4 separate models: ~3.6GB
   - **Tiáº¿t kiá»‡m 72% RAM**

2. **Speed**: Adapter switching < 1ms
   - No model reloading
   - Can run all 4 tasks in < 600ms (dev mode)

3. **Consistency**: Same base representations across tasks
   - Fluency and grammar use same understanding
   - Dialogue aware of vocabulary level naturally

4. **Training**: Multi-task learning improves all tasks
   - Grammar correction helps fluency understanding
   - Vocabulary knowledge improves dialogue quality

5. **Deployment**: Single model file + 4 small adapters
   - Easy to update (just swap adapter)
   - A/B testing per task

---

## 5. Báº£ng Tá»•ng Há»£p: Development vs Production Models

| Component | Development Mode (Mac 32GB) | Production Mode (Mobile) |
|-----------|----------------------------|--------------------------|
| **STT** | Whisper Large v3 (1.5GB, WER 3-5%) | Whisper Small/Medium (500MB-1.5GB, WER 8-10%) |
| **Fluency** | Qwen2.5-1.5B (900MB Q4, MAE < 0.12) | Qwen2.5-0.5B (300MB Q4, MAE < 0.15) |
| **Vocabulary** | Qwen2.5-1.5B (same model, 90% acc) | Qwen2.5-0.5B (same model, 86% acc) |
| **Grammar** | Qwen2.5-1.5B (F0.5: 68, Prec: 78%) | Qwen2.5-0.5B (F0.5: 62, Prec: 72%) |
| **Dialogue** | Qwen2.5-1.5B (BLEU: 38, 96% quality) | Qwen2.5-0.5B (BLEU: 35, 91% quality) |
| **TTS** | Native + Piper (offline) | Native TTS (0MB)
â”‚  â””â”€ Learning goals
â”œâ”€ Conversation flow:
â”‚  â”œâ”€ Topics covered
â”‚  â”œâ”€ Grammar points taught
â”‚  â””â”€ Questions asked
â””â”€ Personalization:
    â”œâ”€ Reference previous errors
    â”œâ”€ Build on achieved goals
    â””â”€ Adapt difficulty progressively
```

**Implementation Stack**:

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch

# 1. Load fine-tuned model
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained(
    "path/to/fine-tuned-flan-t5"
)
model.to("cuda" if torch.cuda.is_available() else "cpu")

# 2. Prepare input
context = f"""
Task: Generate English learning response
User level: B1
User said: {user_transcription}
Analysis:
- Fluency: {fluency_score}/1.0
- Vocabulary: {vocab_level}
- Grammar errors: {grammar_errors_str}
- Pronunciation: {pronunciation_issues_str}

Respond with encouragement, tips, and a question to continue.
Keep language at B1 level.
"""

inputs = tokenizer(context, return_tensors="pt", max_length=512, truncation=True)
inputs = inputs.to(model.device)

# 3. Generate response
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=150,
        min_length=30,
        num_beams=5,
        temperature=0.7,
        top_p=0.95,
        early_stopping=True,
        do_sample=False,  # beam search
        no_repeat_ngram_size=2
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# 4. Post-process
response = response.strip()
if not response.endswith((".", "!", "?")):
    response += "."
```

**Output**:
```json
{
  "user_input": "I like learning English",
  "analysis": {
    "fluency": 0.87,
    "vocabulary_level": "B1",
    "grammar_status": "Correct",
    "pronunciation_issues": ["minor_stress"]
  },
  "ai_response": "That's wonderful! I can see you're making great progress. Your sentence is grammatically perfect, which shows good understanding. Just a tip: stress the second syllable when saying 'LEARning' - it really improves naturalness. What topics do you enjoy discussing in English?",
  "response_type": "encouragement_with_tip",
  "generation_time_ms": 320,
  "confidence": 0.89
}
```

**Advanced Features (Optional)**:

1. **Multi-turn dialogue management**:
   - Maintain conversation context across turns
   - Track corrections/improvements
   - Detect topic shifts
   
2. **Personalized learning path**:
   - Identify grammar weak points
   - Suggest focused exercises
   - Adjust difficulty progressively
   
3. **Emotion/engagement detection**:
   - Monitor user confidence level
   - Adjust response tone (encouraging vs challenging)
   - Suggest breaks if stress detected
   
4. **Native speaker variation**:
   - Introduce regional pronunciations
   - British vs American English
   - Casual vs formal registers

---

## 5. Huáº¥n luyá»‡n vÃ  fine-tune mÃ´ hÃ¬nh DL (Chi tiáº¿t thá»±c hÃ nh)

### 5.1. Dá»¯ liá»‡u huáº¥n luyá»‡n - Chi tiáº¿t cáº¥u trÃºc

| Module          | Sá»‘ lÆ°á»£ng máº«u | Nguá»“n dá»¯ liá»‡u | CÃ´ng viá»‡c ghi chÃº |
| --------------- | ------------ | ------------- | --------------- |
| Fluency (Regression) | 2,500 | ESL corpus (TOEFL essays), LANG-8, English learner corpora | Annotation: 0.0-1.0 scale, triple-annotated |
| Vocabulary (Classification) | 1,500 | CEFR vocabulary lists + learner essays | Label: A2/B1/B2 (sentence-level) |
| Grammar (Seq2seq) | 2,000 | BEA-2019, CoNLL-2014, NUCLE + synthetic errors | Pairs: (incorrect, correct), error type tagging |
| Pronunciation (Phoneme) | 1,000-2,000 | LibriSpeech (English subset), Common Voice | Audio + transcript + IPA phonemes |
| Dialogue Response | 1,500-2,000 | ESL forums, tutoring logs, human-generated | Context â†’ Response (teacher annotated) |
| **Total** | **~9,500** | **Multi-source** | **All human-quality or rule-validated** |

**Quy trÃ¬nh chuáº©n bá»‹ dá»¯ liá»‡u**:

1. **Data Collection**:
   ```
   Fluency:
   â”œâ”€ Crawl LANG-8 (learner exchange platform)
   â”œâ”€ TOEFL essay dataset (publicly available)
   â”œâ”€ English-Only Wikipedia edits (show progression)
   â””â”€ Annotate with native speaker teams (3 raters per sample)
   
   Grammar:
   â”œâ”€ BEA-2019 shared task dataset (publicly available)
   â”œâ”€ Generate synthetic errors using rule templates
   â”‚  â””â”€ Tools: ERRANT, artificial corruption
   â””â”€ Create correction pairs via rule application
   
   Pronunciation:
   â”œâ”€ Download LibriSpeech train-other-500 (500 hours)
   â”œâ”€ Subset: English speakers only (~300 hours)
   â””â”€ Extract phoneme sequences via forced alignment (Montreal Forced Aligner)
   
   Dialogue:
   â”œâ”€ Collect real teacher-student interactions
   â”œâ”€ Paraphrase + generate synthetic variations
   â””â”€ Validate with native English teachers
   ```

2. **Data Validation & Cleaning**:
   - Remove duplicates (fuzzy matching)
   - Filter out poor-quality samples (automated quality checks)
   - Ensure balance across classes/labels
   - Handle outliers (extreme fluency scores, very long texts)

3. **Augmentation** (Ä‘á»ƒ tÄƒng dataset size):
   - Back-translation: English â†’ FR/DE â†’ English
   - Paraphrase with keep-meaning constraint
   - Synonym replacement (controlled)
   - Noise injection (typos, phonetic variations)
   - Output: 2-3x dataset size

---

### 5.2. Cáº¥u hÃ¬nh huáº¥n luyá»‡n chi tiáº¿t (Unified Training Framework)

**Environment Setup**:
```bash
# GPU Requirements
- GPU: NVIDIA RTX 3080 (10GB VRAM) hoáº·c A100 (40GB)
- Framework: PyTorch 2.0+
- Libraries:
  â”œâ”€ transformers (HuggingFace)
  â”œâ”€ lightning (PyTorch Lightning)
  â”œâ”€ wandb (experiment tracking)
  â”œâ”€ optuna (hyperparameter tuning)
  â””â”€ accelerate (distributed training)

# Installation
pip install torch transformers pytorch-lightning wandb optuna accelerate
```

**Unified Training Pipeline** (Ã¡p dá»¥ng cho táº¥t cáº£ mÃ´ hÃ¬nh):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Loop Template (All DL Modules)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚ 1. Load base pre-trained model                  â”‚
â”‚    â””â”€ Freeze early layers (first 6-8 layers)    â”‚
â”‚       â””â”€ Unfreeze last 4-6 layers for fine-tune â”‚
â”‚                                                  â”‚
â”‚ 2. Data loading (with optimization)              â”‚
â”‚    â”œâ”€ Batch size: 16-32                         â”‚
â”‚    â”œâ”€ Pin memory: True                          â”‚
â”‚    â”œâ”€ Num workers: 4                            â”‚
â”‚    â””â”€ Prefetch factor: 2                        â”‚
â”‚                                                  â”‚
â”‚ 3. Optimizer setup                              â”‚
â”‚    â”œâ”€ AdamW (weight_decay=0.01)                 â”‚
â”‚    â”œâ”€ Learning rate: 1e-5 to 5e-5               â”‚
â”‚    â”œâ”€ Warmup: 10% of total steps                â”‚
â”‚    â””â”€ Scheduler: Linear / Cosine annealing      â”‚
â”‚                                                  â”‚
â”‚ 4. Training loop                                â”‚
â”‚    â”œâ”€ Forward pass                              â”‚
â”‚    â”œâ”€ Calculate loss                            â”‚
â”‚    â”œâ”€ Backward pass                             â”‚
â”‚    â”œâ”€ Gradient clipping (max_norm=1.0)          â”‚
â”‚    â”œâ”€ Optimizer step                            â”‚
â”‚    â””â”€ Validation every N batches                â”‚
â”‚                                                  â”‚
â”‚ 5. Early stopping & checkpointing                â”‚
â”‚    â”œâ”€ Monitor: val_loss / val_metric            â”‚
â”‚    â”œâ”€ Patience: 2-3 epochs                      â”‚
â”‚    â”œâ”€ Save best checkpoint                      â”‚
â”‚    â””â”€ Save final model                          â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Module-specific Training Configs**:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ FLUENCY SCORING (Regression)                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Model: DeBERTa-v3-large                                   â•‘
â•‘ Learning rate: 2e-5                                       â•‘
â•‘ Batch size: 16                                            â•‘
â•‘ Epochs: 6                                                 â•‘
â•‘ Loss: MSE + L1 regularization                             â•‘
â•‘ Metrics: MAE, RMSE, Pearson correlation                  â•‘
â•‘ Validation frequency: Every 50 batches                    â•‘
â•‘ GPU memory: ~7GB                                          â•‘
â•‘ Training time: ~30 min (3000 samples)                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ VOCABULARY CLASSIFICATION (3-class)                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Model: XLM-RoBERTa-large                                  â•‘
â•‘ Learning rate: 3e-5                                       â•‘
â•‘ Batch size: 32                                            â•‘
â•‘ Epochs: 5                                                 â•‘
â•‘ Loss: Cross-entropy + class weighting [0.8, 1.0, 1.2]    â•‘
â•‘ Metrics: F1 (macro), Precision, Recall per class         â•‘
â•‘ Validation frequency: Every 30 batches                    â•‘
â•‘ GPU memory: ~10GB                                         â•‘
â•‘ Training time: ~20 min (1500 samples)                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ GRAMMAR CORRECTION (Seq2Seq)                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Model: BART-base (or DeBERTa encoder + Transformer dec)   â•‘
â•‘ Learning rate: 3e-5                                       â•‘
â•‘ Batch size: 32                                            â•‘
â•‘ Epochs: 10                                                â•‘
â•‘ Loss: Cross-entropy + beam search RL (optional)           â•‘
â•‘ Metrics: BLEU, ROUGE-L, M2 score, Token-level accuracy   â•‘
â•‘ Validation frequency: Every 25 batches                    â•‘
â•‘ GPU memory: ~12GB                                         â•‘
â•‘ Training time: ~60 min (2000 samples)                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ DIALOGUE RESPONSE (Seq2Seq)                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Model: Flan-T5-large                                      â•‘
â•‘ Learning rate: 1e-4                                       â•‘
â•‘ Batch size: 16 (grad accum Ã—2 = 32 effective)            â•‘
â•‘ Epochs: 7                                                 â•‘
â•‘ Loss: Cross-entropy + label smoothing (0.1)               â•‘
â•‘ Metrics: BLEU, ROUGE-L, METEOR, human evaluation         â•‘
â•‘ Validation frequency: Every 40 batches                    â•‘
â•‘ GPU memory: ~8GB                                          â•‘
â•‘ Training time: ~45 min (1500 samples)                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Distributed Training** (náº¿u multi-GPU):
```python
# Using PyTorch Lightning (recommended)
trainer = pl.Trainer(
    gpus=[0, 1, 2],  # Use GPUs 0, 1, 2
    strategy="ddp",  # Distributed Data Parallel
    max_epochs=7,
    precision=16,  # FP16 mixed precision
    gradient_clip_val=1.0,
    val_check_interval=0.5,  # Validate every 0.5 epoch
    early_stopping_callback=EarlyStopping(monitor='val_loss', patience=2)
)
trainer.fit(model, train_dataloader, val_dataloader)
```

---

### 5.3. Monitoring & Experiment Tracking

**Setup WandB** (Weights & Biases):
```python
import wandb
from pytorch_lightning.loggers import WandbLogger

wandb.init(project="lexilingo-dl", name="fluency-scoring-v1")

wandb_logger = WandbLogger(project="lexilingo-dl")
trainer = pl.Trainer(logger=wandb_logger, ...)

# Log metrics
wandb.log({
    "train_loss": loss,
    "val_mae": mae,
    "pearson_corr": correlation,
    "learning_rate": optimizer.param_groups[0]['lr']
})
```

**Key metrics to monitor**:
- Training loss & validation loss (convergence check)
- Task-specific metrics (F1, BLEU, MAE, etc.)
- Learning rate changes
- GPU memory usage
- Training speed (samples/sec)
- Gradient norms (detect vanishing/exploding gradients)

---

### 5.4. Hyperparameter Tuning (Optuna)

```python
import optuna

def objective(trial):
    lr = trial.suggest_loguniform(1e-6, 1e-3)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    warmup_ratio = trial.suggest_uniform(0.05, 0.2)
    
    # Train model with these hyperparameters
    model = train_model(lr=lr, batch_size=batch_size, warmup_ratio=warmup_ratio)
    
    # Return validation metric
    return model.evaluate(val_dataset)['f1_score']

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)
best_params = study.best_params
```

---

## 6. YÃªu cáº§u phi chá»©c nÄƒng

### 6.1. Hiá»‡u nÄƒng (Latency SLAs)

| Module | Latency Target | Hardware | Notes |
|--------|---|---|---|
| STT (Faster-Whisper) | < 1.5s | GPU / CPU | Depends on audio length |
| Fluency Scoring | 100-150ms | GPU | Batch inference |
| Vocabulary Classification | 80-120ms | GPU | Batch inference |
| Grammar Correction | 200-300ms | GPU | Beam search decoding |
| Pronunciation Analysis | 1-2s | GPU | Phoneme alignment |
| Dialogue Generation | 300-500ms | GPU | T5 decoding |
| TTS (FastPitch+HiFi-GAN) | 800-1200ms | GPU/CPU | Includes vocoding |
| **End-to-end (STTâ†’Analysisâ†’Responseâ†’TTS)** | **< 5s** | GPU cluster | Parallel execution |

**Optimization techniques**:
- Model quantization (INT8, FP16)
- Knowledge distillation (smaller models)
- Batch inference (collect N requests â†’ process)
- Caching & memoization
- GPU memory pooling
- Request pipelining

### 6.2. Kháº£ nÄƒng má»Ÿ rá»™ng (Scalability)

**Horizontal scaling**:
- Each module runs as independent microservice
- Load balancer distributes requests
- Auto-scaling based on queue depth
- Containerization: Docker + Kubernetes

**Infrastructure architecture**:
```
Load Balancer
    â”œâ”€ STT Service (3-5 replicas)
    â”œâ”€ Analysis Pipeline (5-10 replicas)
    â”‚  â”œâ”€ Fluency (2 replicas)
    â”‚  â”œâ”€ Vocabulary (2 replicas)
    â”‚  â”œâ”€ Grammar (3 replicas)
    â”‚  â””â”€ Pronunciation (2 replicas)
    â”œâ”€ Response Generation (3-5 replicas)
    â””â”€ TTS Service (2-3 replicas)

Cache Layer:
â”œâ”€ User preferences/history (Redis)
â”œâ”€ Pronunciation samples (local disk)
â””â”€ Grammar patterns (in-memory DB)
```

### 6.3. Kháº£ nÄƒng báº£o trÃ¬ & Governance

**Model versioning**:
```
models/
â”œâ”€ v1.0/
â”‚  â”œâ”€ fluency-deberta.pt
â”‚  â”œâ”€ vocabulary-xlm-roberta.pt
â”‚  â”œâ”€ grammar-bart.pt
â”‚  â””â”€ metadata.json (training date, metrics, data version)
â”œâ”€ v1.1/
â”‚  â””â”€ [improved models]
â””â”€ latest/ â†’ (symlink to best performing version)
```

**Model monitoring & updates**:
- A/B testing: Deploy new model to 10% traffic
- Performance metrics: Track live accuracy/latency
- Feedback loop: Collect user corrections â†’ retrain
- Automated retraining pipeline: Weekly with new data
- Rollback mechanism: Switch to previous version if degradation

**Logging & observability**:
```
Log structure:
{
  "timestamp": "2024-01-13T10:30:45Z",
  "user_id": "user_123",
  "session_id": "sess_456",
  "module": "grammar_correction",
  "input": "She go to school",
  "output": "She goes to school",
  "confidence": 0.94,
  "processing_time_ms": 245,
  "model_version": "v1.0",
  "feedback": "correct" / "incorrect" (user feedback after)
}
```

**Deployment pipeline**:
```
Git Push â†’ CI Pipeline â†’ Unit Tests â†’ Model Tests â†’ 
  Staging Deployment â†’ Performance Validation â†’ 
  Production Canary (10%) â†’ Full Rollout (100%)
```

---

## 6. YÃªu cáº§u phi chá»©c nÄƒng

### 6.1. Hiá»‡u nÄƒng

* Äá»™ trá»… STT < 1.5s
* PhÃ¢n tÃ­ch NLP < 500ms
* TTS < 1s

---

### 6.2. Kháº£ nÄƒng má»Ÿ rá»™ng

* Má»—i module triá»ƒn khai Ä‘á»™c láº­p
* CÃ³ thá»ƒ thay tháº¿ model mÃ  khÃ´ng áº£nh hÆ°á»Ÿng há»‡ thá»‘ng

---

### 6.3. Kháº£ nÄƒng báº£o trÃ¬

* Versioning model
* Log káº¿t quáº£ phÃ¢n tÃ­ch

---

## 7. Kiáº¿n trÃºc Deployment & Backend Implementation

### 7.1. Microservices Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLIENT LAYER (Mobile/Web)                                   â”‚
â”‚ â”œâ”€ React Native / Flutter App                               â”‚
â”‚ â””â”€ WebSocket / gRPC client                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API GATEWAY (Load Balancer)                                 â”‚
â”‚ â”œâ”€ Request routing                                          â”‚
â”‚ â”œâ”€ Rate limiting (100 req/min per user)                    â”‚
â”‚ â”œâ”€ Authentication & JWT tokens                              â”‚
â”‚ â””â”€ Request/Response logging                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  â”‚                  â”‚                  â”‚
â–¼                  â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STT Service  â”‚  â”‚ NLP Pipeline â”‚  â”‚ TTS Service  â”‚  â”‚ DB/Cache     â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ Faster-      â”‚  â”œâ”€ Fluency     â”‚  â”‚ FastPitch    â”‚  â”œâ”€ User data   â”‚
â”‚ Whisper      â”‚  â”œâ”€ Vocabulary  â”‚  â”‚ + HiFi-GAN   â”‚  â”œâ”€ Histories   â”‚
â”‚              â”‚  â”œâ”€ Grammar     â”‚  â”‚              â”‚  â”œâ”€ Models      â”‚
â”‚ Replicas: 3  â”‚  â”œâ”€ Pronunciation
â”‚              â”‚  â”œâ”€ Response Genâ”‚  â”‚ Replicas: 2  â”‚  â”‚ (PostgreSQL/ â”‚
â”‚ GPU: 1 T4    â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  Redis)      â”‚
â”‚              â”‚  â”‚ Replicas: 8  â”‚  â”‚ GPU: 1 A100  â”‚  â”‚              â”‚
â”‚ Latency: 1s  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚              â”‚  â”‚ GPU: 2 A100  â”‚  â”‚ Latency: 1s  â”‚  â”‚              â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚              â”‚  â”‚ Latency:     â”‚  â”‚              â”‚  â”‚              â”‚
â”‚              â”‚  â”‚  500ms       â”‚  â”‚              â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                   â”‚                  â”‚                 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Orchestrator Service     â”‚
              â”‚ (Request Coordinator)    â”‚
              â”‚                          â”‚
              â”‚ Aggregates results       â”‚
              â”‚ Manages parallel calls   â”‚
              â”‚ Caches intermediate data â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2. Data Flow & Request Lifecycle

```
1. USER SUBMISSION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Text or Audio Input â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ API Gateway receives    â”‚
   â”‚ request, authenticates  â”‚
   â”‚ & validates input       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
2. PROCESSING (Parallel)
   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                 â”‚                 â”‚                â”‚
   â–¼                 â–¼                 â–¼                â–¼
   STT Service       (if audio)        NLP Pipeline     Store request
   â”‚                                   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
             â”‚                       â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Faster-Whisper inference   â”‚
             â”‚ Output: transcription      â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Parallel Analysis (5 models run concurrently)          â”‚
             â”‚                                                         â”‚
             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
             â”‚ â”‚ Fluency      â”‚ â”‚ Vocabulary   â”‚ â”‚ Grammar + Pron. â”‚ â”‚
             â”‚ â”‚ DeBERTa      â”‚ â”‚ XLM-RoBERTa  â”‚ â”‚ ERRANT+GECToR   â”‚ â”‚
             â”‚ â”‚ Score: 0-1   â”‚ â”‚ Level: A2/B1 â”‚ â”‚ Correction: str â”‚ â”‚
             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
             â”‚                                                         â”‚
             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
             â”‚ â”‚ Pronunciation Analysis (if speech input)        â”‚   â”‚
             â”‚ â”‚ HuBERT + Phoneme alignment                      â”‚   â”‚
             â”‚ â”‚ Errors: [phoneme, type, confidence]            â”‚   â”‚
             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
             â”‚                                                         â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
3. AGGREGATION & RESPONSE GENERATION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Orchestrator Service                                             â”‚
   â”‚                                                                  â”‚
   â”‚ Collect results from all modules                                â”‚
   â”‚ â”œâ”€ Fluency: 0.87                                               â”‚
   â”‚ â”œâ”€ Vocabulary: B1                                              â”‚
   â”‚ â”œâ”€ Grammar errors: [correction data]                           â”‚
   â”‚ â”œâ”€ Pronunciation issues: [phoneme errors]                      â”‚
   â”‚ â””â”€ Aggregate into context                                      â”‚
   â”‚                                                                  â”‚
   â”‚ Generate feedback & dialogue response                           â”‚
   â”‚ â”œâ”€ Flan-T5 generates: encouragement + tips + question         â”‚
   â”‚ â””â”€ Output: Natural English response                            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
4. TEXT-TO-SPEECH
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ TTS Service (if voice feedback needed)           â”‚
   â”‚                                                  â”‚
   â”‚ FastPitch â†’ Mel-spectrogram                      â”‚
   â”‚ HiFi-GAN â†’ Waveform (22kHz)                      â”‚
   â”‚ Output: Audio file (.wav)                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
5. RESPONSE DELIVERY
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ JSON Response to Client                             â”‚
   â”‚ {                                                   â”‚
   â”‚   "transcription": "I like learning English",       â”‚
   â”‚   "analysis": {                                     â”‚
   â”‚     "fluency": 0.87,                               â”‚
   â”‚     "vocabulary": "B1",                            â”‚
   â”‚     "grammar_corrections": [...],                 â”‚
   â”‚     "pronunciation": [...]                         â”‚
   â”‚   },                                               â”‚
   â”‚   "ai_response": "That's wonderful!...",          â”‚
   â”‚   "audio_url": "/api/audio/response_123.wav"      â”‚
   â”‚ }                                                   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Latency: 3-4 seconds (parallel execution)
```

### 7.3. Backend Technology Stack

```
Framework & Runtime:
â”œâ”€ API Server: FastAPI (Python) hoáº·c Go (Gin)
â”œâ”€ async/await: asyncio + aiohttp
â””â”€ Container: Docker

Model Serving:
â”œâ”€ TorchServe (PyTorch models)
â”œâ”€ Triton Inference Server (multi-model optimization)
â””â”€ BentoML (model packaging)

Database:
â”œâ”€ Primary: PostgreSQL (user data, history, feedback)
â”œâ”€ Cache: Redis (session state, model cache)
â”œâ”€ Message Queue: RabbitMQ hoáº·c Kafka (async tasks)
â””â”€ Document Store: MongoDB (optional, unstructured logs)

Monitoring & Observability:
â”œâ”€ Metrics: Prometheus
â”œâ”€ Visualization: Grafana
â”œâ”€ Tracing: Jaeger / Zipkin
â”œâ”€ Logging: ELK Stack (Elasticsearch, Logstash, Kibana)
â””â”€ Alerting: PagerDuty

DevOps:
â”œâ”€ Container orchestration: Kubernetes (K8s)
â”œâ”€ CI/CD: GitHub Actions / GitLab CI
â”œâ”€ Infrastructure: AWS / GCP / Azure
â””â”€ Model versioning: DVC (Data Version Control)
```

### 7.4. Sample API Endpoints

```
POST /api/v1/analyze
â”œâ”€ Input: audio (WAV) hoáº·c text
â”œâ”€ Process: STT (if audio) â†’ Parallel analysis â†’ Response gen
â””â”€ Output: JSON with all analysis + AI response

GET /api/v1/user/{user_id}/history
â”œâ”€ Return: Last 20 interactions
â””â”€ Include: Original input, feedback, corrections learned

POST /api/v1/feedback
â”œâ”€ Log: User feedback on AI response quality
â”œâ”€ Trigger: Incremental model retraining (daily)
â””â”€ Update: User proficiency model

GET /api/v1/audio/{response_id}
â”œâ”€ Return: Pre-generated or cached audio
â””â”€ Cache: 24 hours (reduce TTS load)

POST /api/v1/exercise-recommendation
â”œâ”€ Input: User proficiency, weak areas
â”œâ”€ Return: Personalized exercise suggestions
â””â”€ Link: External content (YouTube, lessons)
```

---

## 8. Roadmap PhÃ¡t triá»ƒn & Má»Ÿ rá»™ng tÆ°Æ¡ng lai

### 8.1 Phase 1: MVP (0-3 thÃ¡ng)
âœ“ **HoÃ n thÃ nh cÃ¡c module core**:
- [x] Faster-Whisper STT (base model, no fine-tune needed)
- [x] DeBERTa fluency scoring (fine-tune + deploy)
- [x] XLM-RoBERTa vocabulary classification (fine-tune)
- [x] ERRANT + GECToR grammar correction (fine-tune GECToR)
- [x] Flan-T5 dialogue response (fine-tune)
- [x] FastPitch + HiFi-GAN TTS (deploy, no fine-tune)
- [ ] Basic pronunciation analysis (HuBERT base model)
- [ ] Simple web interface (React)
- [ ] PostgreSQL database + Redis cache

**Deliverable**: Working prototype with text + basic audio input

---

### 8.2 Phase 2: Enhanced Experience (3-6 thÃ¡ng)
- [ ] Fine-tune HuBERT on custom pronunciation corpus
- [ ] Add phoneme-level feedback
- [ ] User proficiency tracking (ML-based assessment)
- [ ] Personalized learning path recommendation
- [ ] Mobile app (React Native / Flutter)
- [ ] Real-time streaming audio processing
- [ ] A/B testing framework for model improvements
- [ ] User feedback loop â†’ retraining pipeline

**Metric targets**:
- STT accuracy: WER < 8%
- Grammar correction: F0.5 score > 70
- User satisfaction: > 4.0/5.0
- Concurrent users: 100+

---

### 8.3 Phase 3: Advanced Features (6-12 thÃ¡ng)
- [ ] Multi-language support (French, Spanish, German)
- [ ] Speaker diarization (identify different speakers)
- [ ] Emotion recognition (detect frustration, confidence)
- [ ] Conversation flow analysis
- [ ] Native speaker accent variation (regional English)
- [ ] Integration with TOEFL/IELTS preparation
- [ ] Teacher dashboard (class management)
- [ ] Gamification (rewards, achievements, leaderboard)
- [ ] LLM-based dialogue (GPT-4 fine-tuned for teaching)

**Infrastructure scaling**:
- Multi-region deployment (reduce latency)
- Kubernetes cluster auto-scaling
- Edge deployment (on-device models for privacy)

---

### 8.4 Phase 4: Production Enterprise (12+ thÃ¡ng)
- [ ] Corporate licensing model
- [ ] Advanced analytics (detailed progress reports)
- [ ] Teacher-AI collaboration features
- [ ] Integration with LMS (Canvas, Blackboard)
- [ ] Offline mode with model quantization
- [ ] SOC 2 compliance & data privacy
- [ ] White-label solution for schools
- [ ] Research publication (paper on learner profiling)

---

## 9. RÃ ng buá»™c & Giáº£ Ä‘á»‹nh

### 9.1 Giáº£ Ä‘á»‹nh dá»¯ liá»‡u & CÃ´ng nghá»‡
- Dataset training Ä‘Æ°á»£c chuáº©n bá»‹ báº±ng tay (high quality)
- GPU access cho training (RTX 3080 hoáº·c A100)
- Infrastructure cloud (AWS/GCP/Azure)
- Pre-trained base models tá»« HuggingFace hub

### 9.2 RÃ ng buá»™c kinh doanh
- Budget dev: Giá»›i háº¡n (startup phase)
- Timeline: Aggressive (MVP trong 3 thÃ¡ng)
- User base: Báº¯t Ä‘áº§u tá»« 50-100 beta users
- Support language: English â†’ Vietnamese explanation

### 9.3 Rá»§i ro & Mitigation
| Rá»§i ro | TÃ¡c Ä‘á»™ng | Mitigation |
|--------|----------|-----------|
| Low-quality training data | Model accuracy giáº£m | Implement strict QA; use crowdsourcing |
| Model inference latency | User churn | Model quantization; caching; CDN |
| GPU cost scaling | Profitability áº£nh hÆ°á»Ÿng | Use LoRA fine-tuning; knowledge distillation |
| User privacy concerns | Regulatory issues | End-to-end encryption; on-device models |
| Competitor copying | Market share máº¥t | Focus on unique pedagogy + community |

---

## 10. Káº¿t luáº­n

TÃ i liá»‡u SRS chi tiáº¿t nÃ y mÃ´ táº£ má»™t há»‡ thá»‘ng **AI há»c tiáº¿ng Anh máº¡nh máº½, kháº£ thi, xÃ¢y dá»±ng trÃªn cÃ´ng nghá»‡ Deep Learning state-of-the-art**:

### Æ¯u Ä‘iá»ƒm:
1. **Kiáº¿n trÃºc modular**: Má»—i module Ä‘á»™c láº­p, dá»… maintain & upgrade
2. **Fine-tuned DL models**: KhÃ´ng rely trÃªn API bÃªn ngoÃ i, full control
3. **Production-ready**: Scalable, monitored, deployment-optimized
4. **Pedagogically sound**: Dá»±a trÃªn CEFR framework, phÃ¹ há»£p A2-B1 learners
5. **Real-time feedback**: Speech + text analysis, multi-dimensional learning
6. **Clear roadmap**: Phased development, manageable scope

### ğŸ¯ Success Metrics:
- **User retention**: > 60% after 30 days
- **Learning outcomes**: Average score improvement 15% after 2 months
- **System reliability**: 99.5% uptime
- **Performance**: End-to-end latency < 5 seconds
- **Model accuracy**: Fluency MAE < 0.15, Grammar F0.5 > 70

### Deliverables:
1. Fine-tuned DL models (5 models)
2. Microservices backend (FastAPI)
3. Mobile/Web frontend (React/React Native)
4. Kubernetes deployment manifests
5. Training pipeline & documentation
6. User feedback loop system

**Æ¯á»›c tÃ­nh effort**: 
- Development: 8-12 ngÆ°á»i-thÃ¡ng (3 months, team of 4)
- Training data: 1 ngÆ°á»i-thÃ¡ng (3000-5000 samples annotation)
- Infrastructure: 0.5 ngÆ°á»i-thÃ¡ng (DevOps)

**Æ¯á»›c tÃ­nh cost**:
- Development: $80K - $120K
- GPU training: $3K - $5K
- Infrastructure (first year): $10K - $20K
- Data annotation: $5K - $8K
- **Total MVP cost**: ~$100K - $150K

---

**TÃ i liá»‡u cáº­p nháº­t láº§n cuá»‘i**: 13/01/2025
**PhiÃªn báº£n**: 2.0 (Detailed Technical Specification)
**TÃ¡c giáº£**: AI Engineering Team
**Status**: Ready for Development Sprint Planning
