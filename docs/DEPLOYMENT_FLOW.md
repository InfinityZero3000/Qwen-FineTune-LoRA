# üöÄ LexiLingo Deployment Flow: Training ‚Üí GGUF ‚Üí llama.cpp

## üìã Overview

Quy tr√¨nh deploy model LexiLingo t·ª´ training tr√™n Kaggle/Colab ƒë·∫øn ch·∫°y local tr√™n Mac Intel.

**Flow:**
```
Kaggle/Colab (GPU)         Local Mac (CPU)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Train with Unsloth    ‚Üí    Convert to GGUF
Merge LoRA adapter    ‚Üí    Quantize Q4_K_M
Export 16-bit         ‚Üí    Deploy with llama.cpp
```

**Benefits:**
- ‚úÖ 2x faster training v·ªõi Unsloth
- ‚úÖ 4x smaller model v·ªõi GGUF quantization
- ‚úÖ Ch·∫°y nhanh tr√™n CPU (llama.cpp optimized)
- ‚úÖ Low memory usage (~2-4GB RAM)
- ‚úÖ Cross-platform (Mac/Linux/Windows)

---

## üéØ Complete Deployment Pipeline

### Phase 1: Training on Kaggle/Colab (GPU Required)

#### Step 1.1: Setup Environment

```python
# In Kaggle/Colab notebook
!pip install -q unsloth
!pip install -q transformers trl datasets
```

#### Step 1.2: Train with Unsloth

```python
from unsloth import FastLanguageModel
import torch

# Load model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "Qwen/Qwen2.5-1.5B-Instruct",
    max_seq_length = 2048,
    load_in_4bit = True,
    dtype = None,
)

# Apply LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r = 32,
    lora_alpha = 64,
    lora_dropout = 0.05,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    use_gradient_checkpointing = "unsloth",
)

# Train (your training code here)
# trainer.train()
```

#### Step 1.3: Merge LoRA Adapter

**CRITICAL:** Merge adapter v·ªõi base model ƒë·ªÉ c√≥ full model

```python
from unsloth import FastLanguageModel

# Merge LoRA adapter v·ªõi base model
# save_method = "merged_16bit" ‚Üí Gi·ªØ full precision, kh√¥ng m·∫•t th√¥ng tin
model.save_pretrained_merged(
    "lexilingo_qwen25_1.5b_merged",
    tokenizer,
    save_method = "merged_16bit",  # ‚Üê QUAN TR·ªåNG: 16-bit ƒë·ªÉ kh√¥ng m·∫•t d·ªØ li·ªáu
)

print("‚úÖ Model merged successfully!")
print("Output: lexilingo_qwen25_1.5b_merged/")
print("  - model.safetensors (ho·∫∑c model-*.safetensors)")
print("  - tokenizer.json")
print("  - tokenizer_config.json")
print("  - config.json")
```

**Why merged_16bit?**
- ‚úÖ Gi·ªØ nguy√™n precision c·ªßa model (float16)
- ‚úÖ Kh√¥ng m·∫•t th√¥ng tin khi convert sang GGUF
- ‚úÖ GGUF quantization s·∫Ω x·ª≠ l√Ω n√©n ·ªü b∆∞·ªõc sau
- ‚ùå Kh√¥ng d√πng `merged_4bit` ‚Üí M·∫•t quality khi convert

#### Step 1.4: Download Model ho·∫∑c Push to HuggingFace

**Option A: Download v·ªÅ m√°y (Kaggle/Colab)**

```python
# Zip folder ƒë·ªÉ download nhanh h∆°n
!zip -r lexilingo_qwen25_merged.zip lexilingo_qwen25_1.5b_merged/

# Download qua Kaggle Output ho·∫∑c Colab Files
# Kaggle: T·ª± ƒë·ªông save v√†o Output
# Colab: Right-click ‚Üí Download
```

**Option B: Push to HuggingFace (RECOMMENDED)**

```python
from huggingface_hub import HfApi, create_repo

# Login (c·∫ßn HF token)
from huggingface_hub import login
login(token="hf_...")  # Your HuggingFace token

# Create repo
repo_id = "your-username/lexilingo-qwen25-1.5b"
create_repo(repo_id, exist_ok=True, private=True)

# Push model
model.push_to_hub(
    repo_id,
    token="hf_...",
    private=True,
)

tokenizer.push_to_hub(
    repo_id,
    token="hf_...",
    private=True,
)

print(f"‚úÖ Model pushed to: https://huggingface.co/{repo_id}")
```

---

### Phase 2: Convert to GGUF (Mac or Linux)

#### Step 2.1: Install llama.cpp

```bash
# Clone llama.cpp
cd ~/Projects
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Build
make clean
make

# Verify
./llama-cli --version
```

#### Step 2.2: Install Python Dependencies

```bash
# Create virtual environment
python3 -m venv llama-env
source llama-env/bin/activate

# Install requirements
pip install -r requirements.txt

# Or install manually
pip install torch numpy sentencepiece transformers
```

#### Step 2.3: Download Model from HuggingFace

```bash
# Install HuggingFace CLI
pip install -U huggingface_hub

# Login (if private repo)
huggingface-cli login

# Download model
huggingface-cli download your-username/lexilingo-qwen25-1.5b \
    --local-dir ./models/lexilingo_qwen25_merged \
    --local-dir-use-symlinks False
```

**Or copy from local if downloaded from Kaggle:**

```bash
# Unzip downloaded file
unzip lexilingo_qwen25_merged.zip -d ./models/

# Verify structure
ls -lh models/lexilingo_qwen25_1.5b_merged/
# Should see:
#   - config.json
#   - model.safetensors (or model-*.safetensors)
#   - tokenizer.json
#   - tokenizer_config.json
```

#### Step 2.4: Convert to GGUF (F16)

```bash
cd ~/Projects/llama.cpp

# Convert HuggingFace model to GGUF F16
python3 convert_hf_to_gguf.py \
    ./models/lexilingo_qwen25_1.5b_merged/ \
    --outfile ./models/lexilingo_qwen25_f16.gguf \
    --outtype f16

# Verify conversion
ls -lh ./models/lexilingo_qwen25_f16.gguf
# Expected size: ~3.0 GB (1.5B model in F16)
```

**What is F16?**
- F16 = Float16 (half precision)
- Full quality, lossless conversion t·ª´ merged_16bit
- Baseline ƒë·ªÉ quantize sang c√°c format nh·ªè h∆°n

---

### Phase 3: Quantize with llama.cpp

#### Step 3.1: Choose Quantization Level

| Format | Size | Quality | Speed | Use Case |
|--------|------|---------|-------|----------|
| **Q4_K_M** | ~1.0 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | **RECOMMENDED** - Best balance |
| Q4_K_S | ~0.9 GB | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | Faster, slightly lower quality |
| Q5_K_M | ~1.2 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | Better quality, slower |
| Q6_K | ~1.5 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | Near F16 quality |
| Q8_0 | ~2.0 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | Highest quality |

**Q4_K_M is RECOMMENDED because:**
- ‚úÖ 3x smaller than F16 (3GB ‚Üí 1GB)
- ‚úÖ Minimal quality loss (<2%)
- ‚úÖ Fast inference on CPU
- ‚úÖ Fits in 4GB RAM easily

#### Step 3.2: Quantize to Q4_K_M

```bash
cd ~/Projects/llama.cpp

# Quantize F16 to Q4_K_M
./llama-quantize \
    ./models/lexilingo_qwen25_f16.gguf \
    ./models/lexilingo_qwen25_q4_km.gguf \
    Q4_K_M

# Output:
# main: quantizing './models/lexilingo_qwen25_f16.gguf' to './models/lexilingo_qwen25_q4_km.gguf' as Q4_K_M
# ...
# main: model size  = 3000.00 MB
# main: quant size  =  980.00 MB
# main: total time  = 45.23 s
```

**Expected results:**
- Original F16: ~3.0 GB
- Q4_K_M: ~1.0 GB
- Compression: **3x smaller**
- Time: ~30-60 seconds

#### Step 3.3: Verify Quantized Model

```bash
# Get model info
./llama-cli \
    -m ./models/lexilingo_qwen25_q4_km.gguf \
    --version

# Test simple prompt
./llama-cli \
    -m ./models/lexilingo_qwen25_q4_km.gguf \
    -p "What is the capital of France?" \
    -n 50
```

---

### Phase 4: Deploy with llama.cpp on Mac

#### Step 4.1: Test Inference

**Basic usage:**

```bash
cd ~/Projects/llama.cpp

# Interactive chat mode
./llama-cli \
    -m ./models/lexilingo_qwen25_q4_km.gguf \
    -n 256 \
    --temp 0.7 \
    --top-p 0.9 \
    --repeat-penalty 1.1 \
    -i

# Example prompts for LexiLingo tasks:
# - "Analyze the fluency of this sentence: I goes to school."
# - "Classify the vocabulary level: The phenomenon is fascinating."
# - "Correct this sentence: She don't like apples."
```

**Batch processing:**

```bash
# Process multiple prompts from file
./llama-cli \
    -m ./models/lexilingo_qwen25_q4_km.gguf \
    -f prompts.txt \
    -n 256 \
    --log-disable
```

#### Step 4.2: Server Mode (API)

**Start llama.cpp server:**

```bash
# Start server on port 8080
./llama-server \
    -m ./models/lexilingo_qwen25_q4_km.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    -n 256 \
    --ctx-size 2048

# Server available at: http://localhost:8080
```

**API Usage:**

```bash
# Test API with curl
curl http://localhost:8080/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "messages": [
            {"role": "user", "content": "Analyze fluency: The cat sat on the mat."}
        ],
        "temperature": 0.7,
        "max_tokens": 256
    }'
```

**Python client:**

```python
import requests

def query_lexilingo(prompt: str) -> str:
    """Query LexiLingo model via llama.cpp server"""
    response = requests.post(
        "http://localhost:8080/v1/chat/completions",
        json={
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7,
            "max_tokens": 256,
        }
    )
    return response.json()["choices"][0]["message"]["content"]

# Test
result = query_lexilingo("Classify vocabulary level: The cat sat on the mat.")
print(result)  # Expected: "A1"
```

#### Step 4.3: Performance Optimization

**Mac-specific optimizations:**

```bash
# Use Metal acceleration (Mac M1/M2/M3)
# Note: Intel Mac kh√¥ng c√≥ Metal, ch·ªâ d√πng CPU
./llama-cli \
    -m ./models/lexilingo_qwen25_q4_km.gguf \
    -ngl 0  # 0 = CPU only (Intel Mac)

# Adjust threads for best performance
./llama-cli \
    -m ./models/lexilingo_qwen25_q4_km.gguf \
    -t 8  # Use 8 threads (adjust based on CPU cores)
```

**Expected performance on Mac Intel i9:**

| Task | Tokens | Time (Q4_K_M) | Speed |
|------|--------|---------------|-------|
| Simple query | 20-50 | 2-5s | ~10 tok/s |
| Medium response | 50-100 | 5-10s | ~10 tok/s |
| Long response | 100-200 | 10-20s | ~10 tok/s |

**Compare to Python transformers:**
- llama.cpp Q4_K_M: **10 tok/s** ‚ö°
- transformers FP16: **3-5 tok/s** üêå
- **2-3x faster!**

---

## üìä Complete Size & Performance Comparison

### Model Sizes:

| Format | Size | Quality Loss | Use Case |
|--------|------|--------------|----------|
| **Training (4-bit LoRA)** | ~3.5 GB VRAM | N/A | Kaggle/Colab training |
| **Merged (16-bit)** | ~3.0 GB | 0% | Conversion baseline |
| **GGUF F16** | ~3.0 GB | 0% | Lossless conversion |
| **GGUF Q4_K_M** | ~1.0 GB | <2% | **Deployment (BEST)** |
| **GGUF Q4_K_S** | ~0.9 GB | ~3-5% | Faster, lower quality |
| **GGUF Q8_0** | ~2.0 GB | <0.5% | Highest quality |

### Inference Speed (Mac Intel i9):

| Method | Format | Speed | RAM |
|--------|--------|-------|-----|
| **transformers (Python)** | FP16 | 3-5 tok/s | ~8 GB |
| **transformers (Python)** | 4-bit | 5-8 tok/s | ~4 GB |
| **llama.cpp** | Q4_K_M | **10-15 tok/s** | ~2 GB |
| **llama.cpp** | Q8_0 | 8-12 tok/s | ~4 GB |

**Winner: llama.cpp Q4_K_M** üèÜ
- 2-3x faster than transformers
- 50% less RAM usage
- 3x smaller model size

---

## üéØ Recommended Workflow

### For Development (Training):

```
1. Train on Kaggle/Colab with Unsloth
   ‚îú‚îÄ Use finetune_qwen_lora_kaggle.v1.0.ipynb
   ‚îú‚îÄ Train with 4-bit + Unsloth (2x faster)
   ‚îî‚îÄ Time: 4-5 hours on P100

2. Merge LoRA adapter
   ‚îú‚îÄ model.save_pretrained_merged(..., save_method="merged_16bit")
   ‚îî‚îÄ Output: 3GB merged model

3. Push to HuggingFace
   ‚îî‚îÄ Private repo for versioning
```

### For Deployment (Production):

```
1. Download merged model from HuggingFace
   ‚îî‚îÄ huggingface-cli download ...

2. Convert to GGUF F16
   ‚îú‚îÄ python3 convert_hf_to_gguf.py ...
   ‚îî‚îÄ Output: 3GB F16 model

3. Quantize to Q4_K_M
   ‚îú‚îÄ ./llama-quantize ... Q4_K_M
   ‚îî‚îÄ Output: 1GB Q4_K_M model

4. Deploy with llama.cpp
   ‚îú‚îÄ Server mode: ./llama-server ...
   ‚îî‚îÄ Python API client
```

---

## üß™ Testing & Validation

### Step 1: Test Original Model (Before GGUF)

```python
# In Kaggle/Colab after training
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = "lexilingo_qwen25_1.5b_merged"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Test all 5 tasks
test_prompts = [
    "Analyze the fluency of this sentence: The cat sat on the mat.",
    "Classify the vocabulary level: The cat sat on the mat.",
    "Correct this sentence: She don't like apples.",
    "User: What's the weather like today?",
    "Error: I goes ‚Üí Correct: I go. Explain in Vietnamese.",
]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=128)
    print(f"Prompt: {prompt}")
    print(f"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\n")
```

### Step 2: Test GGUF Model (After Conversion)

```bash
# Test all 5 tasks with llama.cpp
cd ~/Projects/llama.cpp

# Test 1: Fluency
./llama-cli -m ./models/lexilingo_qwen25_q4_km.gguf \
    -p "Analyze the fluency of this sentence: The cat sat on the mat." \
    -n 64

# Test 2: Vocabulary
./llama-cli -m ./models/lexilingo_qwen25_q4_km.gguf \
    -p "Classify the vocabulary level: The cat sat on the mat." \
    -n 32

# Test 3: Grammar
./llama-cli -m ./models/lexilingo_qwen25_q4_km.gguf \
    -p "Correct this sentence: She don't like apples." \
    -n 64

# Test 4: Dialogue
./llama-cli -m ./models/lexilingo_qwen25_q4_km.gguf \
    -p "User: What's the weather like today?" \
    -n 128

# Test 5: Explanation
./llama-cli -m ./models/lexilingo_qwen25_q4_km.gguf \
    -p "Error: I goes ‚Üí Correct: I go. Explain the grammar error in Vietnamese." \
    -n 256
```

### Step 3: Compare Outputs

Create comparison script:

```python
import subprocess
import json

def test_gguf_model(prompt: str) -> str:
    """Test GGUF model with llama.cpp"""
    cmd = [
        "./llama-cli",
        "-m", "./models/lexilingo_qwen25_q4_km.gguf",
        "-p", prompt,
        "-n", "128",
        "--log-disable",
    ]
    result = subprocess.run(cmd, capture_output=True, text=True)
    return result.stdout

# Compare outputs
test_cases = [
    "Analyze the fluency of this sentence: I goes to school.",
    "Classify the vocabulary level: The phenomenon is fascinating.",
    "Correct this sentence: He don't know nothing.",
]

print("Testing GGUF model quality...\n")
for prompt in test_cases:
    output = test_gguf_model(prompt)
    print(f"Prompt: {prompt}")
    print(f"Output: {output}\n")
    print("-" * 70)
```

---

## üêõ Troubleshooting

### Issue 1: "Model file not found"

```bash
# Check file exists
ls -lh ./models/lexilingo_qwen25_f16.gguf

# Check file is not corrupted
file ./models/lexilingo_qwen25_f16.gguf
# Should show: data or GGUF model file
```

### Issue 2: "Unsupported model architecture"

```bash
# Qwen models are supported since llama.cpp v1.5+
# Update llama.cpp to latest version
cd ~/Projects/llama.cpp
git pull
make clean
make
```

### Issue 3: Conversion fails with "KeyError"

```python
# Use correct conversion script for Qwen
python3 convert_hf_to_gguf.py \
    ./models/lexilingo_qwen25_1.5b_merged/ \
    --outfile ./models/lexilingo_qwen25_f16.gguf \
    --outtype f16 \
    --model-name qwen  # ‚Üê Add model type hint
```

### Issue 4: Slow inference on Mac

```bash
# Check CPU usage
top -o cpu

# Adjust thread count (8-12 threads optimal for i9)
./llama-cli -m ./models/lexilingo_qwen25_q4_km.gguf -t 10

# Disable logging for faster inference
./llama-cli -m ./models/lexilingo_qwen25_q4_km.gguf --log-disable
```

### Issue 5: "Out of memory"

```bash
# Reduce context size
./llama-cli \
    -m ./models/lexilingo_qwen25_q4_km.gguf \
    --ctx-size 1024  # Reduce from 2048

# Or use smaller quantization
./llama-quantize \
    ./models/lexilingo_qwen25_f16.gguf \
    ./models/lexilingo_qwen25_q4_ks.gguf \
    Q4_K_S  # Smaller than Q4_K_M
```

---

## üìù Complete Script Examples

### Automation Script: train_and_convert.sh

```bash
#!/bin/bash
# Complete pipeline: Train ‚Üí Merge ‚Üí Convert ‚Üí Quantize

set -e  # Exit on error

echo "==================================================================="
echo "LexiLingo Deployment Pipeline"
echo "==================================================================="

# Configuration
MODEL_NAME="lexilingo_qwen25_1.5b"
HF_REPO="your-username/lexilingo-qwen25-1.5b"
LLAMA_DIR="$HOME/Projects/llama.cpp"

# Step 1: Download merged model from HuggingFace
echo -e "\n[1/4] Downloading merged model from HuggingFace..."
huggingface-cli download $HF_REPO \
    --local-dir ./models/${MODEL_NAME}_merged \
    --local-dir-use-symlinks False

# Step 2: Convert to GGUF F16
echo -e "\n[2/4] Converting to GGUF F16..."
cd $LLAMA_DIR
python3 convert_hf_to_gguf.py \
    ./models/${MODEL_NAME}_merged/ \
    --outfile ./models/${MODEL_NAME}_f16.gguf \
    --outtype f16

# Step 3: Quantize to Q4_K_M
echo -e "\n[3/4] Quantizing to Q4_K_M..."
./llama-quantize \
    ./models/${MODEL_NAME}_f16.gguf \
    ./models/${MODEL_NAME}_q4_km.gguf \
    Q4_K_M

# Step 4: Test model
echo -e "\n[4/4] Testing quantized model..."
./llama-cli \
    -m ./models/${MODEL_NAME}_q4_km.gguf \
    -p "Analyze the fluency of this sentence: The cat sat on the mat." \
    -n 64

echo -e "\n==================================================================="
echo "‚úÖ Deployment complete!"
echo "Model location: $LLAMA_DIR/models/${MODEL_NAME}_q4_km.gguf"
echo "Size: $(du -h $LLAMA_DIR/models/${MODEL_NAME}_q4_km.gguf | cut -f1)"
echo "==================================================================="
```

### Python API Wrapper: lexilingo_client.py

```python
#!/usr/bin/env python3
"""
LexiLingo API Client
Wraps llama.cpp server for easy Python usage
"""

import requests
from typing import Optional, Dict, Any
import subprocess
import time
import os

class LexiLingoClient:
    """Client for LexiLingo model via llama.cpp server"""
    
    def __init__(self, 
                 model_path: str,
                 host: str = "localhost",
                 port: int = 8080):
        self.model_path = model_path
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.server_process = None
        
    def start_server(self):
        """Start llama.cpp server"""
        print(f"Starting llama.cpp server on {self.host}:{self.port}...")
        
        llama_dir = os.path.expanduser("~/Projects/llama.cpp")
        cmd = [
            f"{llama_dir}/llama-server",
            "-m", self.model_path,
            "--host", self.host,
            "--port", str(self.port),
            "-n", "512",
            "--ctx-size", "2048",
        ]
        
        self.server_process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        
        # Wait for server to start
        time.sleep(3)
        print("‚úÖ Server started")
        
    def stop_server(self):
        """Stop llama.cpp server"""
        if self.server_process:
            self.server_process.terminate()
            self.server_process.wait()
            print("‚úÖ Server stopped")
    
    def query(self, 
              prompt: str,
              max_tokens: int = 256,
              temperature: float = 0.7,
              top_p: float = 0.9) -> str:
        """
        Query LexiLingo model
        
        Args:
            prompt: Input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (0.0-1.0)
            top_p: Nucleus sampling parameter
            
        Returns:
            Generated text
        """
        response = requests.post(
            f"{self.base_url}/v1/chat/completions",
            json={
                "messages": [{"role": "user", "content": prompt}],
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_tokens,
            },
            timeout=60,
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def analyze_fluency(self, sentence: str) -> float:
        """Analyze fluency score"""
        prompt = f"Analyze the fluency of this sentence: {sentence}"
        result = self.query(prompt, max_tokens=32)
        try:
            return float(result.strip())
        except ValueError:
            return 0.0
    
    def classify_vocabulary(self, sentence: str) -> str:
        """Classify vocabulary level"""
        prompt = f"Classify the vocabulary level: {sentence}"
        result = self.query(prompt, max_tokens=16)
        return result.strip()[:2].upper()
    
    def correct_grammar(self, sentence: str) -> str:
        """Correct grammar errors"""
        prompt = f"Correct this sentence: {sentence}"
        result = self.query(prompt, max_tokens=128)
        return result.strip()
    
    def generate_dialogue(self, user_message: str) -> str:
        """Generate dialogue response"""
        prompt = f"User: {user_message}"
        result = self.query(prompt, max_tokens=256)
        return result.strip()
    
    def explain_error(self, error: str, correct: str) -> str:
        """Explain grammar error in Vietnamese"""
        prompt = f"Error: {error} ‚Üí Correct: {correct}\nExplain the grammar error in Vietnamese."
        result = self.query(prompt, max_tokens=512)
        return result.strip()

# Example usage
if __name__ == "__main__":
    # Initialize client
    model_path = os.path.expanduser("~/Projects/llama.cpp/models/lexilingo_qwen25_q4_km.gguf")
    client = LexiLingoClient(model_path)
    
    try:
        # Start server
        client.start_server()
        
        # Test all 5 tasks
        print("\n" + "="*70)
        print("Testing LexiLingo Model")
        print("="*70)
        
        # Task 1: Fluency
        score = client.analyze_fluency("The cat sat on the mat.")
        print(f"\n1. Fluency: {score}")
        
        # Task 2: Vocabulary
        level = client.classify_vocabulary("The cat sat on the mat.")
        print(f"2. Vocabulary Level: {level}")
        
        # Task 3: Grammar
        correction = client.correct_grammar("She don't like apples.")
        print(f"3. Grammar Correction: {correction}")
        
        # Task 4: Dialogue
        response = client.generate_dialogue("What's the weather like today?")
        print(f"4. Dialogue: {response}")
        
        # Task 5: Explanation
        explanation = client.explain_error("I goes", "I go")
        print(f"5. Explanation: {explanation}")
        
        print("\n" + "="*70)
        
    finally:
        # Stop server
        client.stop_server()
```

---

## ‚úÖ Summary & Recommendations

### This Workflow is EXCELLENT Because:

1. **‚úÖ Optimal for Mac Intel:**
   - llama.cpp is 2-3x faster than transformers on CPU
   - Q4_K_M uses 50% less RAM
   - Cross-platform (Mac/Linux/Windows)

2. **‚úÖ Production-Ready:**
   - Server mode v·ªõi REST API
   - Easy integration v·ªõi Python/Node.js/etc
   - Supports batch processing

3. **‚úÖ Quality Preservation:**
   - merged_16bit ‚Üí No precision loss during merge
   - GGUF F16 ‚Üí Lossless conversion
   - Q4_K_M ‚Üí Minimal quality loss (<2%)

4. **‚úÖ Efficient Storage:**
   - 3x smaller than original (3GB ‚Üí 1GB)
   - Easy to distribute
   - Fast loading times

### Recommended Next Steps:

1. **Train on Kaggle/Colab** v·ªõi Unsloth (4-5h)
2. **Merge v·ªõi save_method="merged_16bit"**
3. **Push to HuggingFace** (private repo)
4. **Download v√† convert to GGUF Q4_K_M**
5. **Deploy with llama.cpp** tr√™n Mac
6. **Build Python API** cho production

### Final Verdict:

**‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5) - HIGHLY RECOMMENDED**

ƒê√¢y l√† quy tr√¨nh deployment t·ªët nh·∫•t cho LLM tr√™n Mac Intel!

---

**Version:** 1.0  
**Last Updated:** 2026-01-27  
**Status:** ‚úÖ Production Ready
