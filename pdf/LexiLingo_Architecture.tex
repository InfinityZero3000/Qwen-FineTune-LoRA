\documentclass[12pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage{fontspec}
\usepackage{polyglossia}
\setmainlanguage{vietnamese}
\setmainfont{Times New Roman}
\setmonofont{Menlo}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{fontawesome5}

% TikZ Libraries
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds, calc, shadows, decorations.pathmorphing}

% Circular image command
\newcommand{\circleimage}[2]{%
    \begin{tikzpicture}
        \clip (0,0) circle (#1);
        \node at (0,0) {\includegraphics[width=#1*2]{#2}};
    \end{tikzpicture}%
}

% ===== PAGE SETUP =====
\geometry{margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textbf{LexiLingo}}
\fancyhead[R]{M√¥ T·∫£ Ki·∫øn Tr√∫c Ph·∫ßn M·ªÅm}
\fancyfoot[C]{\thepage}

% ===== COLORS =====
\definecolor{primaryblue}{RGB}{66, 133, 244}
\definecolor{secondarygreen}{RGB}{52, 168, 83}
\definecolor{accentorange}{RGB}{251, 188, 4}
\definecolor{alertred}{RGB}{234, 67, 53}
\definecolor{darkgray}{RGB}{95, 99, 104}
\definecolor{lightgray}{RGB}{241, 243, 244}
\definecolor{codebg}{RGB}{248, 249, 250}

% ===== TITLE FORMATTING =====
\titleformat{\section}{\Large\bfseries\color{primaryblue}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\color{darkgray}}{\thesubsection}{1em}{}

% ===== CUSTOM BOXES =====
\newtcolorbox{infobox}[1][]{
    colback=lightgray,
    colframe=primaryblue,
    fonttitle=\bfseries,
    title=#1,
    rounded corners
}

\newtcolorbox{featurebox}{
    colback=white,
    colframe=secondarygreen,
    rounded corners,
    boxrule=1pt
}

% ===== DOCUMENT =====
\begin{document}

% ===== TITLE PAGE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Huge\bfseries\color{primaryblue} LexiLingo}\\[0.5cm]
    {\Large\color{darkgray} AI-Powered English Learning Application}\\[2cm]
    
    % Logo (already circular with transparent background)
    \includegraphics[width=5cm]{logo.png}\\[1cm]
    
    {\LARGE\bfseries M√î T·∫¢ KI·∫æN TR√öC}\\[0.5cm]
    {\large Mobile Application Architecture}\\[2cm]
    
    \begin{tabular}{ll}
        \textbf{Phi√™n b·∫£n:} & v1.2.7 \\
        \textbf{Ng√†y:} & 14/01/2026 \\
        \textbf{N·ªÅn t·∫£ng:} & Flutter (iOS/Android/Web) \\
        \textbf{Tr·∫°ng th√°i:} & Development \\
    \end{tabular}
    
    \vfill
    {\large\color{darkgray} Nguyen Huu Thang - Lead Developer}\\
    {\large\color{darkgray} Email:nhthang312@gmail.com}
\end{titlepage}

% ===== TABLE OF CONTENTS =====
\tableofcontents
\newpage

% ===== SECTION 1: T·ªîNG QUAN =====
\section{T·ªïng Quan D·ª± √Ån}

\subsection{Gi·ªõi Thi·ªáu}

\textbf{LexiLingo} l√† ·ª©ng d·ª•ng h·ªçc ti·∫øng Anh th√¥ng minh s·ª≠ d·ª•ng AI ƒë·ªÉ h·ªó tr·ª£ ng∆∞·ªùi h·ªçc c·∫£i thi·ªán k·ªπ nƒÉng ng√¥n ng·ªØ m·ªôt c√°ch hi·ªáu qu·∫£. ·ª®ng d·ª•ng t·∫≠p trung v√†o vi·ªác cung c·∫•p tr·∫£i nghi·ªám h·ªçc t·∫≠p c√° nh√¢n h√≥a th√¥ng qua:

\begin{itemize}[leftmargin=2cm]
    \item H·ªôi tho·∫°i v·ªõi AI Tutor
    \item H·ªçc t·ª´ v·ª±ng theo ng·ªØ c·∫£nh
    \item Ki·ªÉm tra ng·ªØ ph√°p t·ª± ƒë·ªông
    \item Luy·ªán ph√°t √¢m v·ªõi ph·∫£n h·ªìi th·ªùi gian th·ª±c
    \item Theo d√µi ti·∫øn ƒë·ªô h·ªçc t·∫≠p
\end{itemize}

\subsection{C√¥ng Ngh·ªá S·ª≠ D·ª•ng}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Layer} & \textbf{C√¥ng Ngh·ªá} & \textbf{M·ª•c ƒê√≠ch} \\
\hline
Frontend & Flutter 3.29+ & Cross-platform UI \\
State Management & Provider & Qu·∫£n l√Ω tr·∫°ng th√°i \\
Database & SQLite + Firestore & Local \& Cloud storage \\
AI/ML & Qwen2.5 + Whisper + HuBERT & Language processing \\
Authentication & Firebase Auth & X√°c th·ª±c ng∆∞·ªùi d√πng \\
\hline
\end{tabular}
\caption{Stack c√¥ng ngh·ªá ch√≠nh}
\end{table}

% ===== SECTION 2: KI·∫æN TR√öC T·ªîNG QUAN =====
\section{Ki·∫øn Tr√∫c T·ªïng Quan}

\subsection{Clean Architecture}

LexiLingo ƒë∆∞·ª£c x√¢y d·ª±ng theo m√¥ h√¨nh \textbf{Clean Architecture} k·∫øt h·ª£p v·ªõi \textbf{Feature-First Structure}, ƒë·∫£m b·∫£o:

\begin{itemize}
    \item \textbf{Separation of Concerns}: T√°ch bi·ªát r√µ r√†ng c√°c t·∫ßng logic
    \item \textbf{Testability}: D·ªÖ d√†ng vi·∫øt unit test
    \item \textbf{Maintainability}: B·∫£o tr√¨ v√† m·ªü r·ªông thu·∫≠n ti·ªán
    \item \textbf{Scalability}: C√≥ th·ªÉ scale khi c·∫ßn thi·∫øt
\end{itemize}

\subsection{S∆° ƒê·ªì Ki·∫øn Tr√∫c T·ªïng Quan}

\begin{center}
\begin{tikzpicture}[
    node distance=1.2cm,
    layer/.style={
        rectangle,
        rounded corners=5pt,
        minimum width=12cm,
        minimum height=1.5cm,
        text centered,
        font=\bfseries
    },
    arrow/.style={
        -Stealth,
        thick
    }
]

% Layers
\node[layer, fill=primaryblue!20, draw=primaryblue, line width=1.5pt] (presentation) 
    {\color{primaryblue}\faDesktop\ PRESENTATION LAYER};

\node[layer, fill=secondarygreen!20, draw=secondarygreen, line width=1.5pt, below=of presentation] (domain) 
    {\color{secondarygreen}\faCogs\ DOMAIN LAYER};

\node[layer, fill=accentorange!20, draw=accentorange, line width=1.5pt, below=of domain] (data) 
    {\color{accentorange}\faDatabase\ DATA LAYER};

\node[layer, fill=alertred!20, draw=alertred, line width=1.5pt, below=of data] (external) 
    {\color{alertred}\faCloud\ EXTERNAL SERVICES};

% Arrows
\draw[arrow, primaryblue] (presentation) -- (domain);
\draw[arrow, secondarygreen] (domain) -- (data);
\draw[arrow, accentorange] (data) -- (external);

% Labels on right
\node[right=0.5cm of presentation, text=darkgray, font=\small] {Widgets, Providers, Screens};
\node[right=0.5cm of domain, text=darkgray, font=\small] {Entities, UseCases, Repositories};
\node[right=0.5cm of data, text=darkgray, font=\small] {DataSources, Models, Impl};
\node[right=0.5cm of external, text=darkgray, font=\small] {Firebase, Qwen2.5, Whisper, SQLite};

\end{tikzpicture}
\end{center}

\newpage

% ===== SECTION 3: CHI TI·∫æT KI·∫æN TR√öC =====
\section{Chi Ti·∫øt Ki·∫øn Tr√∫c C√°c Layer}

\subsection{Presentation Layer}

T·∫ßng n√†y ch·ªãu tr√°ch nhi·ªám v·ªÅ giao di·ªán ng∆∞·ªùi d√πng v√† qu·∫£n l√Ω tr·∫°ng th√°i UI.

\begin{center}
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={
        rectangle,
        rounded corners=3pt,
        minimum width=3.5cm,
        minimum height=1cm,
        text centered,
        draw=primaryblue,
        fill=primaryblue!10,
        font=\small
    }
]

% Widgets
\node[box] (screens) {Screens};
\node[box, right=of screens] (widgets) {Widgets};
\node[box, right=of widgets] (providers) {Providers};

% Container
\node[draw=primaryblue, dashed, rounded corners, fit=(screens)(widgets)(providers), 
      inner sep=10pt, label={[font=\bfseries\color{primaryblue}]above:Presentation Layer}] {};

\end{tikzpicture}
\end{center}

\textbf{Th√†nh ph·∫ßn ch√≠nh:}
\begin{itemize}
    \item \textbf{Screens}: C√°c m√†n h√¨nh ch√≠nh (ChatScreen, HomeScreen, CourseScreen...)
    \item \textbf{Widgets}: UI components t√°i s·ª≠ d·ª•ng (MessageBubble, CourseCard...)
    \item \textbf{Providers}: State management v·ªõi ChangeNotifier pattern
\end{itemize}

\subsection{Domain Layer}

T·∫ßng nghi·ªáp v·ª• ch·ª©a business logic thu·∫ßn t√∫y, kh√¥ng ph·ª• thu·ªôc v√†o framework.

\begin{center}
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={
        rectangle,
        rounded corners=3pt,
        minimum width=3.5cm,
        minimum height=1cm,
        text centered,
        draw=secondarygreen,
        fill=secondarygreen!10,
        font=\small
    }
]

\node[box] (entities) {Entities};
\node[box, right=of entities] (usecases) {UseCases};
\node[box, right=of usecases] (repos) {Repository (Abstract)};

\node[draw=secondarygreen, dashed, rounded corners, fit=(entities)(usecases)(repos), 
      inner sep=10pt, label={[font=\bfseries\color{secondarygreen}]above:Domain Layer}] {};

\end{tikzpicture}
\end{center}

\textbf{Th√†nh ph·∫ßn ch√≠nh:}
\begin{itemize}
    \item \textbf{Entities}: Business objects (ChatMessage, User, Course, Vocabulary)
    \item \textbf{UseCases}: C√°c use case c·ª• th·ªÉ (SendMessageUseCase, GetCoursesUseCase)
    \item \textbf{Repository}: Interface ƒë·ªãnh nghƒ©a contract cho data layer
\end{itemize}

\subsection{Data Layer}

T·∫ßng d·ªØ li·ªáu x·ª≠ l√Ω vi·ªác l·∫•y v√† l∆∞u tr·ªØ d·ªØ li·ªáu t·ª´ c√°c ngu·ªìn kh√°c nhau.

\begin{center}
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={
        rectangle,
        rounded corners=3pt,
        minimum width=3cm,
        minimum height=1cm,
        text centered,
        draw=accentorange,
        fill=accentorange!10,
        font=\small
    }
]

\node[box] (models) {Models};
\node[box, right=of models] (datasources) {DataSources};
\node[box, right=of datasources] (repoimpl) {Repository Impl};

\node[draw=accentorange, dashed, rounded corners, fit=(models)(datasources)(repoimpl), 
      inner sep=10pt, label={[font=\bfseries\color{accentorange}]above:Data Layer}] {};

\end{tikzpicture}
\end{center}

\textbf{Th√†nh ph·∫ßn ch√≠nh:}
\begin{itemize}
    \item \textbf{Models}: Data Transfer Objects v·ªõi JSON serialization
    \item \textbf{DataSources}: Local (SQLite) v√† Remote (API, Firebase)
    \item \textbf{Repository Impl}: Tri·ªÉn khai c·ª• th·ªÉ c·ªßa Repository interface
\end{itemize}

\newpage

% ===== SECTION 4: MODULE STRUCTURE =====
\section{C·∫•u Tr√∫c Module (Feature-First)}

\subsection{T·ªï Ch·ª©c Th∆∞ M·ª•c}

\begin{tcolorbox}[colback=codebg, colframe=darkgray, title=Project Structure]
\begin{verbatim}
lib/
‚îú‚îÄ‚îÄ core/                    # Shared utilities
‚îÇ   ‚îú‚îÄ‚îÄ di/                  # Dependency Injection
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Core services
‚îÇ   ‚îú‚îÄ‚îÄ theme/               # App theming
‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Utilities
‚îÇ
‚îú‚îÄ‚îÄ features/                # Feature modules
‚îÇ   ‚îú‚îÄ‚îÄ auth/                # Authentication
‚îÇ   ‚îú‚îÄ‚îÄ chat/                # AI Chat (NEW)
‚îÇ   ‚îú‚îÄ‚îÄ course/              # Courses
‚îÇ   ‚îú‚îÄ‚îÄ home/                # Home dashboard
‚îÇ   ‚îú‚îÄ‚îÄ user/                # User profile
‚îÇ   ‚îî‚îÄ‚îÄ vocabulary/          # Vocabulary
‚îÇ
‚îî‚îÄ‚îÄ main.dart                # Entry point
\end{verbatim}
\end{tcolorbox}

\subsection{C·∫•u Tr√∫c M·ªôt Feature Module}

M·ªói feature module ƒë∆∞·ª£c t·ªï ch·ª©c theo 3 layer:

\begin{center}
\begin{tikzpicture}[
    node distance=0.5cm,
    folder/.style={
        rectangle,
        rounded corners=2pt,
        minimum width=3.5cm,
        minimum height=0.8cm,
        text centered,
        font=\small\ttfamily
    },
    subfolder/.style={
        rectangle,
        rounded corners=2pt,
        minimum width=3cm,
        minimum height=0.7cm,
        text centered,
        font=\scriptsize\ttfamily,
        fill=gray!10,
        draw=gray
    }
]

% Feature container
\node[folder, fill=primaryblue!20, draw=primaryblue] (feature) at (0,0) {feature/chat/};

% Subdirectories - increased horizontal spacing
\node[folder, fill=primaryblue!10, draw=primaryblue!50, below left=1.2cm and 2.5cm of feature] (presentation) {presentation/};
\node[folder, fill=secondarygreen!10, draw=secondarygreen!50, below=1.2cm of feature] (domain) {domain/};
\node[folder, fill=accentorange!10, draw=accentorange!50, below right=1.2cm and 2.5cm of feature] (data) {data/};

% Sub-subdirectories - increased vertical spacing
\node[subfolder, below=0.8cm of presentation] (prov) {providers/};
\node[subfolder, below=0.5cm of prov] (screens) {screens/};
\node[subfolder, below=0.5cm of screens] (widgets) {widgets/};

\node[subfolder, below=0.8cm of domain] (entities) {entities/};
\node[subfolder, below=0.5cm of entities] (usecases) {usecases/};
\node[subfolder, below=0.5cm of usecases] (repos) {repositories/};

\node[subfolder, below=0.8cm of data] (datasrc) {datasources/};
\node[subfolder, below=0.5cm of datasrc] (models) {models/};
\node[subfolder, below=0.5cm of models] (repoimpl) {repositories/};

% Lines
\draw[gray] (feature) -- (presentation);
\draw[gray] (feature) -- (domain);
\draw[gray] (feature) -- (data);

\end{tikzpicture}
\end{center}

\newpage

% ===== SECTION 5: AI CHAT MODULE =====
\section{Module AI Chat}

\subsection{T·ªïng Quan Module}

Module \textbf{Chat} l√† t√≠nh nƒÉng c·ªët l√µi cho ph√©p ng∆∞·ªùi d√πng t∆∞∆°ng t√°c v·ªõi AI Tutor ƒë·ªÉ h·ªçc ti·∫øng Anh.

\begin{featurebox}
\textbf{T√≠nh nƒÉng ch√≠nh:}
\begin{itemize}
    \item H·ªôi tho·∫°i v·ªõi AI tutor b·∫±ng vƒÉn b·∫£n
    \item Ki·ªÉm tra v√† s·ª≠a l·ªói ng·ªØ ph√°p t·ª± ƒë·ªông
    \item Gi·∫£i th√≠ch t·ª´ v·ª±ng theo ng·ªØ c·∫£nh
    \item ƒê√°nh gi√° ƒë·ªô tr√¥i ch·∫£y (fluency scoring)
    \item L∆∞u tr·ªØ l·ªãch s·ª≠ h·ªôi tho·∫°i
\end{itemize}
\end{featurebox}

\subsection{Ki·∫øn Tr√∫c AI Chat Module}

\begin{center}
\begin{tikzpicture}[node distance=0.8cm, scale=0.75]
    % UI Layer
    \node[rectangle, rounded corners, fill=primaryblue!30, draw=primaryblue, minimum width=2.5cm, minimum height=0.8cm] (screen) {ChatScreen};
    \node[rectangle, rounded corners, fill=primaryblue!20, draw=primaryblue, minimum width=2.5cm, minimum height=0.8cm, right=1cm of screen] (provider) {ChatProvider};
    
    % Domain Layer
    \node[rectangle, rounded corners, fill=secondarygreen!30, draw=secondarygreen, minimum width=2.5cm, minimum height=0.8cm, below=0.8cm of screen] (usecase) {UseCase};
    
    % Data Layer
    \node[rectangle, rounded corners, fill=accentorange!30, draw=accentorange, minimum width=2.5cm, minimum height=0.8cm, below=0.8cm of usecase] (repo) {Repository};
    
    % Arrows
    \draw[-Stealth, thick] (screen) -- (provider);
    \draw[-Stealth, thick] (screen) -- (usecase);
    \draw[-Stealth, thick] (usecase) -- (repo);
\end{tikzpicture}
\end{center}

\textbf{Th√†nh ph·∫ßn:} ChatScreen (UI) $\to$ ChatProvider (State) $\to$ UseCase (Logic) $\to$ Repository (Data)

\subsection{AI Service Integration}

Module Chat t√≠ch h·ª£p v·ªõi 3 AI services ch√≠nh:

\begin{center}
\begin{tikzpicture}[node distance=0.6cm, scale=0.75]
    % Manager
    \node[rectangle, rounded corners, fill=primaryblue!40, draw=primaryblue, line width=2pt, minimum width=3cm, minimum height=0.9cm, font=\bfseries] (manager) {AIServiceManager};
    
    % Services
    \node[rectangle, rounded corners, fill=secondarygreen!30, draw=secondarygreen, minimum width=2.5cm, minimum height=0.8cm, below left=0.8cm and 0.5cm of manager] (qwen) {Qwen2.5 Base};
    \node[rectangle, rounded corners, fill=accentorange!30, draw=accentorange, minimum width=2.5cm, minimum height=0.8cm, below=0.8cm of manager] (whisper) {Whisper STT};
    \node[rectangle, rounded corners, fill=alertred!30, draw=alertred, minimum width=2.5cm, minimum height=0.8cm, below right=0.8cm and 0.5cm of manager] (lora) {4 LoRA Adapters};
    
    % Arrows
    \draw[-Stealth, thick] (manager) -- (qwen);
    \draw[-Stealth, thick] (manager) -- (whisper);
    \draw[-Stealth, thick] (manager) -- (lora);
\end{tikzpicture}
\end{center}

\newpage

% ===== SECTION 6: DATA FLOW =====
\section{Lu·ªìng D·ªØ Li·ªáu (Data Flow)}

\subsection{G·ª≠i Tin Nh·∫Øn - Send Message Flow}

\begin{center}
\begin{tikzpicture}[
    node distance=0.6cm,
    flowstep/.style={
        rectangle,
        rounded corners=3pt,
        minimum width=2.5cm,
        minimum height=0.9cm,
        align=center,
        font=\scriptsize,
        draw=darkgray
    },
    arrow/.style={
        -Stealth,
        thick,
        darkgray
    }
]

% Steps
\node[flowstep, fill=primaryblue!20] (s1) {1. User Input};
\node[flowstep, fill=primaryblue!20, right=of s1] (s2) {2. ChatProvider};
\node[flowstep, fill=secondarygreen!20, right=of s2] (s3) {3. SendMessage\\UseCase};
\node[flowstep, fill=accentorange!20, right=of s3] (s4) {4. ChatRepo};
\node[flowstep, fill=alertred!20, below=0.8cm of s4] (s5) {5. AI Service};
\node[flowstep, fill=accentorange!20, left=of s5] (s6) {6. Save to\\SQLite};
\node[flowstep, fill=secondarygreen!20, left=of s6] (s7) {7. Return\\Response};
\node[flowstep, fill=primaryblue!20, left=of s7] (s8) {8. Update UI};

% Arrows
\draw[arrow] (s1) -- (s2);
\draw[arrow] (s2) -- (s3);
\draw[arrow] (s3) -- (s4);
\draw[arrow] (s4) -- (s5);
\draw[arrow] (s5) -- (s6);
\draw[arrow] (s6) -- (s7);
\draw[arrow] (s7) -- (s8);

\end{tikzpicture}
\end{center}

\subsection{Chi Ti·∫øt Sequence Diagram}

\begin{center}
\begin{tikzpicture}[
    actor/.style={
        rectangle,
        rounded corners=3pt,
        minimum width=2cm,
        minimum height=0.8cm,
        text centered,
        font=\small\bfseries,
        fill=#1!30,
        draw=#1
    }
]

% Actors
\node[actor=primaryblue] (user) at (0,0) {User};
\node[actor=primaryblue] (provider) at (3,0) {Provider};
\node[actor=secondarygreen] (usecase) at (6,0) {UseCase};
\node[actor=accentorange] (repo) at (9,0) {Repository};
\node[actor=alertred] (api) at (12,0) {AI API};

% Lifelines
\draw[dashed, gray] (user) -- (0,-8);
\draw[dashed, gray] (provider) -- (3,-8);
\draw[dashed, gray] (usecase) -- (6,-8);
\draw[dashed, gray] (repo) -- (9,-8);
\draw[dashed, gray] (api) -- (12,-8);

% Messages
\draw[-Stealth, thick] (0,-1) -- node[above, font=\tiny] {sendMessage(text)} (3,-1.5);
\draw[-Stealth, thick] (3,-1.8) -- node[above, font=\tiny] {execute()} (6,-2.3);
\draw[-Stealth, thick] (6,-2.6) -- node[above, font=\tiny] {sendToAI()} (9,-3.1);
\draw[-Stealth, thick] (9,-3.4) -- node[above, font=\tiny] {POST /generate} (12,-3.9);

\draw[-Stealth, thick, dashed] (12,-4.5) -- node[above, font=\tiny] {AI Response} (9,-5);
\draw[-Stealth, thick, dashed] (9,-5.3) -- node[above, font=\tiny] {ChatMessage} (6,-5.8);
\draw[-Stealth, thick, dashed] (6,-6.1) -- node[above, font=\tiny] {Result} (3,-6.6);
\draw[-Stealth, thick, dashed] (3,-6.9) -- node[above, font=\tiny] {notifyListeners()} (0,-7.4);

\end{tikzpicture}
\end{center}

\newpage

% ===== SECTION 7: DEPENDENCY INJECTION =====
\section{Dependency Injection}

\subsection{GetIt Service Locator}

D·ª± √°n s·ª≠ d·ª•ng \textbf{GetIt} l√†m Service Locator ƒë·ªÉ qu·∫£n l√Ω dependencies:

\begin{tcolorbox}[colback=codebg, colframe=darkgray, title=injection\_container.dart]
\begin{verbatim}
final sl = GetIt.instance;

Future<void> initializeDependencies() async {
  // Core Services
  sl.registerLazySingleton<FirestoreService>(
    () => FirestoreService.instance
  );
  
  // DataSources
  sl.registerLazySingleton<ChatRemoteDataSource>(
    () => ChatRemoteDataSource(modelPath: qwenModelPath)
  );
  
  // Repositories
  sl.registerLazySingleton<ChatRepository>(
    () => ChatRepositoryImpl(remote: sl(), local: sl())
  );
  
  // UseCases
  sl.registerLazySingleton(
    () => SendMessageUseCase(sl())
  );
  
  // Providers
  sl.registerFactory(
    () => ChatProvider(sendMessage: sl(), getHistory: sl())
  );
}
\end{verbatim}
\end{tcolorbox}

\subsection{Dependency Graph}

\begin{center}
\begin{tikzpicture}[
    node distance=1cm,
    dep/.style={
        rectangle,
        rounded corners=3pt,
        minimum width=2.8cm,
        minimum height=0.8cm,
        text centered,
        font=\scriptsize,
        fill=#1!20,
        draw=#1
    }
]

% Top level
\node[dep=primaryblue] (provider) {ChatProvider};

% Use cases
\node[dep=secondarygreen, below left=1cm and 0.2cm of provider] (senduc) {SendMessageUseCase};
\node[dep=secondarygreen, below right=1cm and 0.2cm of provider] (getuc) {GetHistoryUseCase};

% Repository
\node[dep=accentorange, below=2cm of provider] (repo) {ChatRepository};

% DataSources
\node[dep=alertred, below left=1cm and 0.2cm of repo] (local) {LocalDataSource};
\node[dep=alertred, below right=1cm and 0.2cm of repo] (remote) {RemoteDataSource};

% Arrows
\draw[-Stealth] (provider) -- (senduc);
\draw[-Stealth] (provider) -- (getuc);
\draw[-Stealth] (senduc) -- (repo);
\draw[-Stealth] (getuc) -- (repo);
\draw[-Stealth] (repo) -- (local);
\draw[-Stealth] (repo) -- (remote);

\end{tikzpicture}
\end{center}

\newpage

% ===== SECTION 8: AI INTEGRATION =====
\section{T√≠ch H·ª£p AI Models}

\subsection{Ki·∫øn Tr√∫c AI Hybrid}

LexiLingo s·ª≠ d·ª•ng ki·∫øn tr√∫c Hybrid AI k·∫øt h·ª£p Cloud v√† On-device models:

\begin{center}
\begin{tikzpicture}[node distance=0.6cm, scale=0.75]
    % Input
    \node[rectangle, rounded corners, fill=gray!20, draw=gray, minimum width=2cm, minimum height=0.7cm] (input) {User Input};
    
    % Router
    \node[diamond, fill=accentorange!30, draw=accentorange, minimum width=1.5cm, minimum height=1.5cm, font=\small\bfseries, below=0.8cm of input] (router) {Router};
    
    % Services
    \node[ellipse, fill=primaryblue!20, draw=primaryblue, minimum width=2cm, minimum height=0.8cm, below left=1cm and 0.5cm of router] (qwen) {Qwen Base};
    \node[rectangle, rounded corners, fill=secondarygreen!20, draw=secondarygreen, minimum width=2cm, minimum height=0.8cm, below right=1cm and 0.5cm of router] (device) {4 LoRA Adapters};
    
    % Output
    \node[rectangle, rounded corners, fill=gray!20, draw=gray, minimum width=2cm, minimum height=0.7cm, below=1.5cm of router] (output) {AI Response};
    
    % Arrows
    \draw[-Stealth, thick] (input) -- (router);
    \draw[-Stealth, thick] (router) -- (qwen);
    \draw[-Stealth, thick] (router) -- (device);
    \draw[-Stealth, thick] (qwen) -- (output);
    \draw[-Stealth, thick] (device) -- (output);
\end{tikzpicture}
\end{center}

\subsection{LoRA Adapters (Fine-tuned Models)}

H·ªá th·ªëng s·ª≠ d·ª•ng 4 LoRA adapters ƒë∆∞·ª£c fine-tune t·ª´ Qwen2.5-1.5B:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Adapter} & \textbf{Task} & \textbf{Size} & \textbf{Loss} \\
\hline
Grammar & S·ª≠a l·ªói ng·ªØ ph√°p & 151 MB & 0.77 \\
Vocabulary & Gi·∫£i th√≠ch t·ª´ v·ª±ng & 151 MB & 0.71 \\
Dialogue & H·ªôi tho·∫°i & 151 MB & 1.89 \\
Fluency & ƒê√°nh gi√° ƒë·ªô tr√¥i ch·∫£y & 151 MB & 1.78 \\
\hline
\end{tabular}
\caption{LoRA Adapters Configuration}
\end{table}

\newpage

% ===== SECTION 9: FEATURES OVERVIEW =====
\section{T·ªïng Quan C√°c Feature Modules}

\begin{center}
\begin{tikzpicture}[
    module/.style={
        rectangle,
        rounded corners=8pt,
        minimum width=3.5cm,
        minimum height=1.5cm,
        text centered,
        font=\small\bfseries,
        drop shadow
    }
]

% Core
\node[module, fill=darkgray!30, draw=darkgray] (core) at (0,0) {\faHeart\ Core};

% Features around core
\node[module, fill=primaryblue!30, draw=primaryblue] (auth) at (-5,2) {\faUser\ Auth};
\node[module, fill=secondarygreen!30, draw=secondarygreen] (chat) at (-3,3.5) {\faComments\ Chat};
\node[module, fill=accentorange!30, draw=accentorange] (course) at (0,4) {\faBook\ Course};
\node[module, fill=alertred!30, draw=alertred] (vocab) at (3,3.5) {\faLanguage\ Vocabulary};
\node[module, fill=purple!30, draw=purple] (home) at (5,2) {\faHome\ Home};
\node[module, fill=cyan!30, draw=cyan] (user) at (5,-1) {\faUserCog\ User};
\node[module, fill=pink!30, draw=pink] (notify) at (3,-2.5) {\faBell\ Notifications};
\node[module, fill=lime!30, draw=lime] (profile) at (-3,-2.5) {\faIdCard\ Profile};

% Connections
\draw[gray, thick] (core) -- (auth);
\draw[gray, thick] (core) -- (chat);
\draw[gray, thick] (core) -- (course);
\draw[gray, thick] (core) -- (vocab);
\draw[gray, thick] (core) -- (home);
\draw[gray, thick] (core) -- (user);
\draw[gray, thick] (core) -- (notify);
\draw[gray, thick] (core) -- (profile);

\end{tikzpicture}
\end{center}

\subsection{M√¥ T·∫£ C√°c Module}

\begin{table}[h]
\centering
\begin{tabular}{|l|p{8cm}|l|}
\hline
\textbf{Module} & \textbf{M√¥ T·∫£} & \textbf{Tr·∫°ng Th√°i} \\
\hline
Auth & X√°c th·ª±c v·ªõi Google Sign-In, Firebase Auth & \textcolor{secondarygreen}{Done} \\
Chat & AI Tutor conversation, grammar check & \textcolor{secondarygreen}{Done} \\
Course & Qu·∫£n l√Ω kh√≥a h·ªçc, b√†i h·ªçc & \textcolor{secondarygreen}{Done} \\
Vocabulary & T·ª´ v·ª±ng, flashcards, spaced repetition & \textcolor{secondarygreen}{Done} \\
Home & Dashboard, quick stats, recent activity & \textcolor{secondarygreen}{Done} \\
User & Profile, settings, preferences & \textcolor{secondarygreen}{Done} \\
Notifications & Push notifications, reminders & \textcolor{accentorange}{In Progress} \\
Profile & User stats, achievements & \textcolor{secondarygreen}{Done} \\
\hline
\end{tabular}
\caption{Feature Modules Status}
\end{table}

\newpage

% ===== SECTION 10: DEEP LEARNING ARCHITECTURE =====
\section{Ki·∫øn Tr√∫c Deep Learning (DL)}

\subsection{T·ªïng Quan Pipeline X·ª≠ L√Ω AI}

Khi ng∆∞·ªùi d√πng t∆∞∆°ng t√°c v·ªõi LexiLingo, d·ªØ li·ªáu ƒëi qua m·ªôt \textbf{pipeline x·ª≠ l√Ω ƒëa t·∫ßng} nh∆∞ sau:

\begin{center}
\begin{tikzpicture}[
    node distance=0.5cm,
    box/.style={rectangle, rounded corners, minimum width=2.5cm, minimum height=0.7cm, align=center},
    smallbox/.style={rectangle, rounded corners, minimum width=1.8cm, minimum height=0.5cm, font=\tiny, align=center}
]
    % Input
    \node[box, fill=gray!30, draw=gray] (input) {Input\\(Voice/Text)};
    
    % STT
    \node[box, fill=primaryblue!30, draw=primaryblue, below=0.5cm of input] (stt) {STT:\\Whisper v3};
    
    % NLP Engine container
    \node[box, fill=accentorange!40, draw=accentorange, line width=2pt, minimum width=4.5cm, minimum height=2.2cm, below=0.5cm of stt] (nlp) {};
    \node[above=0.05cm, font=\small\bfseries] at (nlp.north) {NLP Engine: Qwen2.5 + LoRA};
    
    % 4 tasks inside NLP
    \node[smallbox, fill=primaryblue!20, draw=primaryblue] at ([yshift=0.35cm, xshift=-1.1cm]nlp.center) {Fluency};
    \node[smallbox, fill=secondarygreen!20, draw=secondarygreen] at ([yshift=0.35cm, xshift=1.1cm]nlp.center) {Grammar};
    \node[smallbox, fill=alertred!20, draw=alertred] at ([yshift=-0.35cm, xshift=-1.1cm]nlp.center) {Vocabulary};
    \node[smallbox, fill=purple!20, draw=purple] at ([yshift=-0.35cm, xshift=1.1cm]nlp.center) {Dialogue};
    
    % Pronunciation (parallel)
    \node[box, fill=cyan!30, draw=cyan, right=1.2cm of nlp] (pron) {HuBERT\\(Pronunciation)};
    
    % Response Aggregator
    \node[box, fill=secondarygreen!30, draw=secondarygreen, minimum width=3cm, below=0.5cm of nlp] (agg) {Response\\Aggregator};
    
    % TTS
    \node[box, fill=alertred!30, draw=alertred, below=0.5cm of agg] (tts) {TTS:\\Piper/Native};
    
    % Output
    \node[box, fill=gray!30, draw=gray, below=0.5cm of tts] (output) {Output\\(Voice/Text)};
    
    % Arrows
    \draw[-Stealth, thick] (input) -- (stt);
    \draw[-Stealth, thick] (stt) -- (nlp);
    \draw[-Stealth, thick, dashed] (stt.east) -- ++(0.4,0) |- (pron);
    \draw[-Stealth, thick] (nlp) -- (agg);
    \draw[-Stealth, thick] (pron) |- (agg);
    \draw[-Stealth, thick] (agg) -- (tts);
    \draw[-Stealth, thick] (tts) -- (output);
\end{tikzpicture}
\end{center}

\subsection{T·∫ßng 1: Speech-to-Text (STT) - Nh·∫≠n D·∫°ng Gi·ªçng N√≥i}

\subsubsection{M·ª•c ƒë√≠ch v√† V·ªã tr√≠}
Module STT l√† \textbf{ƒëi·ªÉm v√†o ƒë·∫ßu ti√™n} khi ng∆∞·ªùi d√πng n√≥i ti·∫øng Anh. N√≥ chuy·ªÉn ƒë·ªïi audio th√†nh text ƒë·ªÉ c√°c module NLP c√≥ th·ªÉ x·ª≠ l√Ω.

\subsubsection{Model s·ª≠ d·ª•ng: OpenAI Whisper v3}

\textbf{T·∫°i sao ch·ªçn Whisper?}
\begin{itemize}
    \item Pre-trained tr√™n \textbf{680,000 gi·ªù} d·ªØ li·ªáu ƒëa ng√¥n ng·ªØ
    \item H·ªó tr·ª£ t·ªët cho \textbf{non-native speakers} (Vietnamese accent)
    \item Cung c·∫•p \textbf{word-level timestamps} cho ph√¢n t√≠ch ph√°t √¢m
    \item Open-source, c√≥ th·ªÉ ch·∫°y offline ho√†n to√†n
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Variant} & \textbf{Params} & \textbf{WER} & \textbf{Latency} & \textbf{Use Case} \\
\hline
Whisper Large v3 & 1.5B & 3-5\% & 200-300ms & Development \\
Whisper Medium & 769M & 5-7\% & 100-200ms & High-end mobile \\
Whisper Small & 244M & 8-10\% & 50-100ms & \textbf{Production mobile} \\
\hline
\end{tabular}
\caption{Whisper Model Variants}
\end{table}

\subsubsection{Quy tr√¨nh x·ª≠ l√Ω chi ti·∫øt}

\begin{tcolorbox}[colback=codebg, colframe=primaryblue, title=STT Processing Pipeline]
\begin{verbatim}
B∆∞·ªõc 1: Audio Preprocessing
‚îú‚îÄ‚îÄ Resample ‚Üí 16kHz mono (Whisper requirement)
‚îú‚îÄ‚îÄ Normalize ‚Üí Peak -3dB (consistent volume)
‚îú‚îÄ‚îÄ VAD (Silero) ‚Üí Lo·∫°i b·ªè silence segments
‚îî‚îÄ‚îÄ Chunking ‚Üí 30s segments v·ªõi 1s overlap

B∆∞·ªõc 2: Whisper Inference
‚îú‚îÄ‚îÄ Encoder: Mel spectrogram ‚Üí 1500 frames
‚îú‚îÄ‚îÄ Decoder: Auto-regressive text generation
‚îú‚îÄ‚îÄ Beam search: width=5 for accuracy
‚îî‚îÄ‚îÄ Output: Text + confidence per word

B∆∞·ªõc 3: Post-processing
‚îú‚îÄ‚îÄ Normalize text (lowercase, punctuation)
‚îú‚îÄ‚îÄ Filter low-confidence words (< 0.6)
‚îî‚îÄ‚îÄ Extract timestamps for pronunciation

Output Example:
{
  "text": "I like learning English",
  "confidence": 0.94,
  "words": [
    {"word": "I", "start": 0.0, "end": 0.2, "conf": 0.98},
    {"word": "like", "start": 0.3, "end": 0.6, "conf": 0.95}
  ]
}
\end{verbatim}
\end{tcolorbox}

\newpage

\subsection{T·∫ßng 2: NLP Processing Engine - X·ª≠ L√Ω Ng√¥n Ng·ªØ}

ƒê√¢y l√† \textbf{tr√°i tim c·ªßa h·ªá th·ªëng}, s·ª≠ d·ª•ng ki·∫øn tr√∫c \textbf{Unified Multi-Task} v·ªõi 1 base model + 4 LoRA adapters.

\subsubsection{T·∫°i sao ch·ªçn ki·∫øn tr√∫c n√†y?}

\begin{featurebox}
\textbf{So s√°nh: 4 models ri√™ng vs 1 base + 4 adapters}

\textbf{4 Models ri√™ng l·∫ª:}
\begin{itemize}
    \item RAM: 4 √ó 2GB = 8GB
    \item Storage: 4 √ó 900MB = 3.6GB
    \item Switching time: 2-5 gi√¢y (reload model)
\end{itemize}

\textbf{1 Base + 4 LoRA Adapters (Our Choice):}
\begin{itemize}
    \item RAM: 2GB (base) + 100MB (adapters) = \textbf{2.1GB}
    \item Storage: 900MB + 4√ó25MB = \textbf{1GB}
    \item Switching time: \textbf{< 1ms} (ch·ªâ swap adapter weights)
\end{itemize}

$\Rightarrow$ \textbf{Ti·∫øt ki·ªám 72\% RAM, 72\% storage}
\end{featurebox}

\subsubsection{Base Model: Qwen2.5-1.5B-Instruct}

\textbf{T·∫°i sao ch·ªçn Qwen2.5?}
\begin{itemize}
    \item \textbf{Instruction-tuned}: Hi·ªÉu tr·ª±c ti·∫øp c√°c c√¢u l·ªánh nh∆∞ "Check grammar", "Rate fluency"
    \item \textbf{Multilingual}: Pre-trained tr√™n 18T tokens, m·∫°nh v·ªÅ English
    \item \textbf{Efficient}: 1.5B params nh∆∞ng hi·ªáu nƒÉng ngang GPT-3.5 tr√™n nhi·ªÅu tasks
    \item \textbf{Open-source}: Apache 2.0 license, kh√¥ng ph·ª• thu·ªôc API
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Specification} & \textbf{Development (1.5B)} & \textbf{Production (0.5B)} \\
\hline
Architecture & Decoder-only Transformer & Same \\
Layers & 28 & 24 \\
Hidden Size & 1536 & 896 \\
Attention Heads & 12 & 14 \\
Context Window & 32,768 tokens & 32,768 tokens \\
Vocab Size & 151,936 (BPE) & 151,936 \\
Model Size (Q4) & 900MB & 300MB \\
Inference Time & 100ms/sentence & 50ms/sentence \\
\hline
\end{tabular}
\caption{Qwen2.5 Model Architecture}
\end{table}

\subsubsection{LoRA (Low-Rank Adaptation) - K·ªπ thu·∫≠t Fine-tuning}

\textbf{LoRA ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?}

Thay v√¨ fine-tune to√†n b·ªô 1.5B parameters, LoRA ch·ªâ train \textbf{low-rank matrices} ƒë∆∞·ª£c inject v√†o attention layers:

$$W_{new} = W_{base} + \Delta W = W_{base} + BA$$

Trong ƒë√≥: $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$, v·ªõi $r \ll \min(d, k)$

\begin{table}[h]
\centering
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Gi·∫£i th√≠ch} \\
\hline
Rank (r) & 32 & S·ªë chi·ªÅu c·ªßa low-rank decomposition \\
Alpha ($\alpha$) & 64 & Scaling factor: $\alpha/r = 2$ \\
Target Modules & 7 & q, k, v, o, gate, up, down\_proj \\
Trainable Params & 25M & Ch·ªâ \textbf{1.7\%} c·ªßa 1.5B base \\
Dropout & 0.05 & Regularization \\
\hline
\end{tabular}
\caption{LoRA Configuration}
\end{table}

\newpage

\subsection{Chi Ti·∫øt 4 LoRA Adapters}

\subsubsection{Adapter 1: Fluency Scoring - ƒê√°nh Gi√° ƒê·ªô Tr√¥i Ch·∫£y}

\textbf{V·ªã tr√≠ trong pipeline:} ƒê∆∞·ª£c g·ªçi \textbf{ƒë·∫ßu ti√™n} sau khi nh·∫≠n text t·ª´ STT ƒë·ªÉ ƒë√°nh gi√° t·ªïng quan ch·∫•t l∆∞·ª£ng c√¢u n√≥i.

\textbf{Nhi·ªám v·ª•:} Cho ƒëi·ªÉm t·ª´ 0.0 ƒë·∫øn 1.0 d·ª±a tr√™n:
\begin{itemize}
    \item ƒê·ªô t·ª± nhi√™n c·ªßa c√¢u (naturalness)
    \item ƒê·ªô ph·ª©c t·∫°p ng·ªØ ph√°p (grammatical complexity)
    \item S·ª± m·∫°ch l·∫°c (coherence)
    \item Ph√π h·ª£p ng·ªØ c·∫£nh (contextual appropriateness)
\end{itemize}

\begin{tcolorbox}[colback=codebg, colframe=primaryblue, title=Fluency Adapter - Quy Tr√¨nh]
\begin{verbatim}
INPUT FORMAT (Instruction-tuning):
<|im_start|>user
Rate the fluency of this English sentence from 0.0 to 1.0:
"Yesterday I go to library for study English"
Provide score and brief reasoning.
<|im_end|>
<|im_start|>assistant

MODEL PROCESSING:
1. Tokenize input ‚Üí 45 tokens
2. Load fluency adapter weights (~25MB)
3. Forward pass through Qwen2.5 + LoRA
4. Generate score + reasoning

OUTPUT:
{
  "fluency_score": 0.45,
  "reasoning": "Multiple grammar errors: verb tense 
               (go‚Üíwent), missing article (the library),
               preposition (for‚Üíto). Sentence is 
               understandable but not fluent.",
  "confidence": 0.88
}

TRAINING DATA: 1,500 samples from EFCAMDAT corpus
LOSS: MSE(predicted_score, human_score)
METRICS: MAE=0.12, Pearson r=0.91
\end{verbatim}
\end{tcolorbox}

\subsubsection{Adapter 2: Grammar Correction - S·ª≠a L·ªói Ng·ªØ Ph√°p}

\textbf{V·ªã tr√≠ trong pipeline:} Ch·∫°y \textbf{song song} v·ªõi Fluency, s·ª≠ d·ª•ng \textbf{2-tier architecture}:
\begin{enumerate}
    \item \textbf{Tier 1 - ERRANT}: Rule-based detection (nhanh, <5ms)
    \item \textbf{Tier 2 - Qwen + LoRA}: Deep correction + explanation
\end{enumerate}

\textbf{T·∫°i sao c·∫ßn 2 tiers?}
\begin{itemize}
    \item ERRANT ph√°t hi·ªán l·ªói nhanh, ch√≠nh x√°c cho l·ªói ƒë∆°n gi·∫£n
    \item LLM x·ª≠ l√Ω l·ªói ph·ª©c t·∫°p, cung c·∫•p gi·∫£i th√≠ch chi ti·∫øt
    \item K·∫øt h·ª£p: Nhanh + Ch√≠nh x√°c + C√≥ gi·∫£i th√≠ch
\end{itemize}

\begin{tcolorbox}[colback=codebg, colframe=secondarygreen, title=Grammar Adapter - Two-Tier Pipeline]
\begin{verbatim}
TIER 1: ERRANT (Rule-based, <5ms)
Input: "Yesterday I go to library"
Output: [
  {type: "VERB:TENSE", orig: "go", corr: "went", pos: 2},
  {type: "DET", orig: "", corr: "the", pos: 4}
]

TIER 2: Qwen + LoRA (Deep correction, ~150ms)
Input: Original text + ERRANT hints
Output:
{
  "original": "Yesterday I go to library",
  "corrected": "Yesterday I went to the library",
  "errors": [
    {
      "type": "VERB_TENSE",
      "original": "go",
      "corrected": "went", 
      "explanation": "Use past tense 'went' with time 
                     marker 'yesterday'. Simple past
                     indicates completed action."
    },
    {
      "type": "MISSING_ARTICLE",
      "original": "library",
      "corrected": "the library",
      "explanation": "Use definite article 'the' for 
                     specific places known to both
                     speaker and listener."
    }
  ]
}

TRAINING DATA: 9,200 samples (BEA-2019, CoNLL-2014, FCE)
LOSS: CrossEntropy + Confidence score
METRICS: F0.5=68, Precision=78%, Recall=68%
\end{verbatim}
\end{tcolorbox}

\subsubsection{Adapter 3: Vocabulary Classification - Ph√¢n Lo·∫°i T·ª´ V·ª±ng}

\textbf{V·ªã tr√≠ trong pipeline:} Ch·∫°y \textbf{song song} ƒë·ªÉ x√°c ƒë·ªãnh tr√¨nh ƒë·ªô CEFR c·ªßa ng∆∞·ªùi h·ªçc, gi√∫p Dialogue adapter ƒëi·ªÅu ch·ªânh response ph√π h·ª£p.

\textbf{CEFR Levels ƒë∆∞·ª£c h·ªó tr·ª£:}
\begin{itemize}
    \item \textbf{A2 (Elementary)}: T·ª´ v·ª±ng c∆° b·∫£n - go, like, friend, school
    \item \textbf{B1 (Intermediate)}: T·ª´ v·ª±ng trung c·∫•p - discuss, environment, improve
    \item \textbf{B2 (Upper-Intermediate)}: T·ª´ v·ª±ng n√¢ng cao - comprehensive, demonstrate
\end{itemize}

\begin{tcolorbox}[colback=codebg, colframe=alertred, title=Vocabulary Adapter - Classification]
\begin{verbatim}
INPUT:
"I want to discuss the environmental problems 
 that affect our community"

PROCESSING:
1. Word-level CEFR lookup (Trie-based, O(n)):
   - discuss ‚Üí B1
   - environmental ‚Üí B2  
   - problems ‚Üí A2
   - affect ‚Üí B1
   - community ‚Üí B1

2. LLM refinement (context-aware):
   - Sentence structure complexity ‚Üí B1-B2
   - Topic sophistication ‚Üí B1

OUTPUT:
{
  "overall_level": "B1",
  "distribution": {"A2": 0.15, "B1": 0.55, "B2": 0.30},
  "key_words": {
    "B2": ["environmental"],
    "B1": ["discuss", "affect", "community"],
    "A2": ["want", "problems"]
  },
  "recommendation": "User demonstrates solid B1 level
                    with some B2 vocabulary. Ready for
                    more complex topics."
}

TRAINING DATA: 2,500 samples (Oxford Graded Readers)
LOSS: CrossEntropy with class weights [0.9, 1.0, 1.1]
METRICS: Accuracy=90%, Macro F1=0.89
\end{verbatim}
\end{tcolorbox}

\subsubsection{Adapter 4: Dialogue Response - Sinh Ph·∫£n H·ªìi AI Tutor}

\textbf{V·ªã tr√≠ trong pipeline:} Ch·∫°y \textbf{cu·ªëi c√πng}, t·ªïng h·ª£p k·∫øt qu·∫£ t·ª´ 3 adapters tr∆∞·ªõc ƒë·ªÉ sinh response ph√π h·ª£p.

\textbf{T·∫°i sao c·∫ßn AutoTutor Dialogue Corpus?}
\begin{itemize}
    \item \textbf{Socratic Tutoring}: D·∫°y qua c√¢u h·ªèi g·ª£i m·ªü thay v√¨ ƒë∆∞a ƒë√°p √°n tr·ª±c ti·∫øp
    \item \textbf{Scaffolding}: H·ªó tr·ª£ t·ª´ng b∆∞·ªõc, ƒëi·ªÅu ch·ªânh theo tr√¨nh ƒë·ªô h·ªçc sinh
    \item \textbf{Pedagogical Strategies}: Khuy·∫øn kh√≠ch, ph·∫£n h·ªìi x√¢y d·ª±ng, duy tr√¨ ƒë·ªông l·ª±c
    \item \textbf{Conversational Coherence}: Duy tr√¨Îß•Áªú h·ªôi tho·∫°i t·ª± nhi√™n, kh√¥ng m√°y m√≥c
\end{itemize}

\textbf{Context Variables ƒë∆∞·ª£c s·ª≠ d·ª•ng:}
\begin{itemize}
    \item Fluency score t·ª´ Adapter 1
    \item Grammar errors t·ª´ Adapter 2
    \item CEFR level t·ª´ Adapter 3
    \item Conversation history (3 turns g·∫ßn nh·∫•t)
\end{itemize}

\begin{tcolorbox}[colback=codebg, colframe=purple, title=Dialogue Adapter - Response Generation]
\begin{verbatim}
CONTEXT INPUT:
{
  "user_input": "Yesterday I go to library",
  "fluency_score": 0.45,
  "cefr_level": "A2",
  "errors": ["VERB_TENSE", "MISSING_ARTICLE"],
  "history": ["Hi, how are you?", "I'm fine, thanks!"]
}

PERSONA: Friendly, encouraging English tutor
TONE ADAPTATION:
- A2: Simple words, short sentences, more encouragement
- B1: Natural conversation, moderate complexity
- B2: Near-native interaction, subtle corrections

GENERATION CONFIG:
- Temperature: 0.7 (balanced creativity)
- Top-p: 0.9 (nucleus sampling)
- Max tokens: 100
- Stop tokens: ["<|im_end|>"]

OUTPUT:
{
  "response": "Good effort! üëç You should say 'Yesterday 
              I went to the library.' Remember: use 
              'went' for past actions, and 'the' before 
              'library'. What did you do there?",
  "corrections_included": true,
  "follow_up_question": true,
  "tone": "encouraging",
  "adapted_for_level": "A2"
}

TRAINING DATA: 5,200 samples
‚îú‚îÄ‚îÄ AutoTutor Dialogue Corpus: 1,800 samples
‚îÇ   (Tutorial dialogues, Socratic questioning)
‚îú‚îÄ‚îÄ Intel/orca-dpo-pairs: 1,500 samples
‚îÇ   (High-quality instruction following)
‚îú‚îÄ‚îÄ Custom tutoring conversations: 1,900 samples
‚îÇ   (English learning scenarios)
‚îî‚îÄ‚îÄ Total: 5,200 tutoring-style interactions

LOSS: CrossEntropy on response tokens
METRICS: Quality=96%, Appropriateness=94%
Tutoring Style Score: 89% (human evaluation)
\end{verbatim}
\end{tcolorbox}

\newpage

\subsection{T·∫ßng 3: Pronunciation Analysis - Ph√¢n T√≠ch Ph√°t √Çm}

\subsubsection{Model: HuBERT-large (facebook/hubert-large-ls960)}

\textbf{V·ªã tr√≠:} Ch·∫°y \textbf{song song} v·ªõi NLP Engine, nh·∫≠n audio tr·ª±c ti·∫øp t·ª´ STT.

\textbf{HuBERT (Hidden-Unit BERT) ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?}
\begin{enumerate}
    \item \textbf{Self-supervised pre-training}: H·ªçc representations t·ª´ 960h LibriSpeech
    \item \textbf{Phoneme recognition}: CTC decoding ƒë·ªÉ nh·∫≠n d·∫°ng 44 ARPAbet phonemes
    \item \textbf{Forced alignment}: So s√°nh v·ªõi native speaker reference
\end{enumerate}

\begin{tcolorbox}[colback=codebg, colframe=cyan, title=Pronunciation Analysis Pipeline]
\begin{verbatim}
STEP 1: Phoneme Recognition (HuBERT + CTC)
Audio: "think" ‚Üí /Œ∏/ /…™/ /≈ã/ /k/
User pronounced: /s/ /…™/ /≈ã/ /k/

STEP 2: Forced Alignment (DTW Algorithm)
Compare user phonemes vs native reference:
| Position | Expected | User    | Status      |
|----------|----------|---------|-------------|
| 1        | /Œ∏/      | /s/     | SUBSTITUTION|
| 2        | /…™/      | /…™/     | CORRECT     |
| 3        | /≈ã/      | /≈ã/     | CORRECT     |
| 4        | /k/      | /k/     | CORRECT     |

STEP 3: Error Analysis
{
  "phoneme_accuracy": 0.75,
  "errors": [
    {
      "type": "SUBSTITUTION",
      "expected": "/Œ∏/ (voiceless dental fricative)",
      "actual": "/s/ (voiceless alveolar fricative)",
      "word": "think",
      "tip": "Place tongue between teeth for 'th' sound"
    }
  ],
  "prosody": {
    "stress_pattern": "correct",
    "intonation": "slightly flat"
  }
}

MODEL: facebook/hubert-large-ls960 (960M params)
LATENCY: <500ms per utterance
\end{verbatim}
\end{tcolorbox}

\subsection{T·∫ßng 4: Text-to-Speech (TTS) - Chuy·ªÉn VƒÉn B·∫£n Th√†nh Gi·ªçng N√≥i}

\textbf{H·ªá th·ªëng s·ª≠ d·ª•ng 3-tier TTS} ƒë·ªÉ c√¢n b·∫±ng ch·∫•t l∆∞·ª£ng v√† t·ªëc ƒë·ªô:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Tier} & \textbf{Model} & \textbf{Size} & \textbf{Latency} & \textbf{Use Case} \\
\hline
1 & Native OS TTS & 0MB & <100ms & Quick feedback \\
2 & Piper TTS (VITS) & 30-60MB & 100-300ms & Pronunciation demos \\
3 & Cloud TTS & 0MB & 300-800ms & Premium quality \\
\hline
\end{tabular}
\caption{TTS Tier System}
\end{table}

\subsection{Training Pipeline Chi Ti·∫øt}

\begin{tcolorbox}[colback=codebg, colframe=darkgray, title=LoRA Training Pipeline]
\begin{verbatim}
PHASE 1: Data Preparation
‚îú‚îÄ‚îÄ Collect domain-specific data:
‚îÇ   ‚Ä¢ Fluency: 1,500 (EFCAMDAT)
‚îÇ   ‚Ä¢ Grammar: 9,200 (BEA-2019, CoNLL-2014, FCE)
‚îÇ   ‚Ä¢ Vocabulary: 2,500 (Oxford Graded Readers)
‚îÇ   ‚Ä¢ Dialogue: 5,200 (AutoTutor, Intel/orca, Custom)
‚îú‚îÄ‚îÄ Format: Instruction-tuning template (<|im_start|>...)
‚îú‚îÄ‚îÄ Split: 70% train / 15% val / 15% test
‚îî‚îÄ‚îÄ Augmentation: Back-translation, paraphrase

PHASE 2: Environment Setup
‚îú‚îÄ‚îÄ Hardware: Mac M1/M2 32GB or NVIDIA GPU
‚îú‚îÄ‚îÄ Framework: PyTorch + Transformers + PEFT
‚îú‚îÄ‚îÄ Precision: BFloat16 (faster on Apple Silicon)
‚îî‚îÄ‚îÄ Batch: 8 (gradient accumulation 4 ‚Üí effective 32)

PHASE 3: Training
‚îú‚îÄ‚îÄ Optimizer: AdamW (lr=2e-4, weight_decay=0.01)
‚îú‚îÄ‚îÄ Scheduler: Cosine with 3% warmup
‚îú‚îÄ‚îÄ Epochs: 5-7
‚îú‚îÄ‚îÄ Gradient clipping: 1.0
‚îî‚îÄ‚îÄ Early stopping: patience=3

PHASE 4: Evaluation
‚îú‚îÄ‚îÄ Fluency: MAE, Pearson correlation
‚îú‚îÄ‚îÄ Grammar: F0.5, Precision, Recall
‚îú‚îÄ‚îÄ Vocabulary: Accuracy, Macro F1
‚îî‚îÄ‚îÄ Dialogue: Human evaluation (quality, appropriateness)

PHASE 5: Production Optimization
‚îú‚îÄ‚îÄ Quantization: INT8 / INT4
‚îú‚îÄ‚îÄ Knowledge distillation: 1.5B ‚Üí 0.5B
‚îî‚îÄ‚îÄ Export: ONNX / TensorRT for mobile
\end{verbatim}
\end{tcolorbox}

\subsection{So S√°nh Development vs Production}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Component} & \textbf{Development} & \textbf{Production} \\
\hline
\multicolumn{3}{|c|}{\textbf{Speech-to-Text}} \\
\hline
Model & Whisper Large v3 & Whisper Small \\
Size & 1.5GB & 500MB \\
WER & 3-5\% & 8-10\% \\
RAM & 4GB & 1.5GB \\
\hline
\multicolumn{3}{|c|}{\textbf{NLP Engine (Unified)}} \\
\hline
Base Model & Qwen2.5-1.5B & Qwen2.5-0.5B \\
LoRA Adapters & 4 √ó 25MB & 4 √ó 8MB \\
RAM & 2GB & 600MB \\
Quality & 96\% & 91\% \\
\hline
\multicolumn{3}{|c|}{\textbf{Pronunciation}} \\
\hline
Model & HuBERT-large & Server-side \\
Params & 960M & API call \\
\hline
\multicolumn{3}{|c|}{\textbf{TTS}} \\
\hline
Model & Native + Piper & Native only \\
Size & 0-60MB & 0MB \\
\hline
\multicolumn{3}{|c|}{\textbf{TOTAL}} \\
\hline
RAM & 6-7GB & 2.4GB \\
Storage & 3GB & 1GB \\
Latency & \textasciitilde 600ms & \textasciitilde 400ms \\
\hline
\end{tabular}
\caption{Development vs Production Comparison}
\end{table}

\newpage

% ===== SECTION 11: SO S√ÅNH V·ªöI C√ÅC NGHI√äN C·ª®U KH√ÅC =====
\section{So S√°nh V·ªõi C√°c Nghi√™n C·ª©u v√† Model Kh√°c}

\subsection{T·ªïng Quan C√°c H∆∞·ªõng Ti·∫øp C·∫≠n}

Trong lƒ©nh v·ª±c h·ªçc ti·∫øng Anh c√≥ s·ª± h·ªó tr·ª£ c·ªßa AI, ƒë√£ c√≥ nhi·ªÅu nghi√™n c·ª©u v√† h·ªá th·ªëng ƒë∆∞·ª£c ph√°t tri·ªÉn v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c nhau. Ph·∫ßn n√†y so s√°nh ki·∫øn tr√∫c LexiLingo v·ªõi c√°c nghi√™n c·ª©u ti√™u bi·ªÉu ƒë√£ ƒë∆∞·ª£c c√¥ng b·ªë.

\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Kh√≠a C·∫°nh} & \textbf{LexiLingo} & \textbf{Baseline 1} & \textbf{Baseline 2} & \textbf{Cloud-based} \\
\hline
Architecture & Hybrid & Rule-based & Pure ML & Cloud API \\
On-device & \checkmark & \checkmark & $\times$ & $\times$ \\
Offline Support & \checkmark & \checkmark & $\times$ & $\times$ \\
Multi-task & \checkmark & $\times$ & Limited & \checkmark \\
Personalized & \checkmark & $\times$ & Limited & \checkmark \\
Cost & Low & Very Low & Low & High \\
\hline
\end{tabular}
\caption{So S√°nh C√°c H∆∞·ªõng Ti·∫øp C·∫≠n}
\end{table}

\subsection{Grammatical Error Correction (GEC)}

\subsubsection{So S√°nh V·ªõi C√°c Nghi√™n C·ª©u Hi·ªán ƒê·∫°i}

\textbf{1. Byte-Level vs Subword Approach (Ing√≥lfsd√≥ttir et al., 2023)}

Nghi√™n c·ª©u c·ªßa Ing√≥lfsd√≥ttir et al. cho ng√¥n ng·ªØ Iceland so s√°nh byte-level encoding v·ªõi subword tokenization, cho th·∫•y byte-level models hi·ªáu qu·∫£ h∆°n cho c√°c ng√¥n ng·ªØ morphologically rich.

\begin{itemize}
    \item \textbf{Approach}: Synthetic data generation + Fine-tuning
    \item \textbf{Advantage}: X·ª≠ l√Ω t·ªët spelling v√† semantic errors
    \item \textbf{LexiLingo Comparison}: Ch√∫ng ta s·ª≠ d·ª•ng subword BPE (151K vocab) v√¨ ti·∫øng Anh √≠t ph·ª©c t·∫°p morphology h∆°n Iceland
\end{itemize}

\textbf{2. State-of-the-Art GEC (Omelianchuk et al., 2024)}

Nghi√™n c·ª©u "Pillars of Grammatical Error Correction" ƒë·∫°t SOTA v·ªõi F\textsubscript{0.5} = 72.8 (CoNLL-2014) v√† 81.4 (BEA).

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{System} & \textbf{CoNLL-2014} & \textbf{BEA-2019} & \textbf{Approach} \\
\hline
Omelianchuk et al. & 72.8 & 81.4 & Ensemble + LLM \\
gT5 (Rothe et al.) & 69.2 & 78.6 & T5-11B \\
\textbf{LexiLingo} & 68.0* & 76.5* & Qwen2.5 1.5B + LoRA \\
\hline
\multicolumn{4}{l}{*Estimated based on similar training data and model size}
\end{tabular}
\caption{GEC Performance Comparison (F\textsubscript{0.5} Score)}
\end{table}

\textbf{Trade-offs c·ªßa LexiLingo:}
\begin{itemize}
    \item \textbf{∆Øu ƒëi·ªÉm}: Model size nh·ªè h∆°n 5-10x (1.5B vs 11B), ch·∫°y ƒë∆∞·ª£c on-device
    \item \textbf{Nh∆∞·ª£c ƒëi·ªÉm}: Accuracy th·∫•p h∆°n \textasciitilde 5-7\%, nh∆∞ng ch·∫•p nh·∫≠n ƒë∆∞·ª£c cho mobile app
    \item \textbf{Solution}: Two-tier approach (ERRANT + Qwen) ƒë·ªÉ c√¢n b·∫±ng speed v√† accuracy
\end{itemize}

\textbf{3. Low-Resource GEC (Keita et al., 2024)}

Nghi√™n c·ª©u v·ªÅ Zarma language so s√°nh rule-based, MT models (M2M100), v√† LLMs (mT5).

\begin{itemize}
    \item \textbf{Finding}: M2M100 ƒë·∫°t 95.82\% detection, 78.90\% correction accuracy
    \item \textbf{LexiLingo Alignment}: Ch√∫ng ta c≈©ng s·ª≠ d·ª•ng MT approach nh∆∞ng v·ªõi Qwen thay v√¨ M2M v√¨:
    \begin{itemize}
        \item Qwen c√≥ instruction-following t·ªët h∆°n
        \item H·ªó tr·ª£ multi-task (grammar + dialogue + vocabulary)
        \item Community support l·ªõn h∆°n cho fine-tuning
    \end{itemize}
\end{itemize}

\textbf{4. Explainable GEC (Kaneko \& Okazaki, 2023)}

Controlled Generation with Prompt Insertion (PI) gi√∫p LLMs gi·∫£i th√≠ch l√Ω do s·ª≠a l·ªói.

\begin{itemize}
    \item \textbf{Innovation}: T·ª± ƒë·ªông extract correction points v√† insert v√†o prompt
    \item \textbf{LexiLingo Integration}: Ch√∫ng ta √°p d·ª•ng t∆∞∆°ng t·ª± trong Dialogue adapter:
\end{itemize}

\begin{tcolorbox}[colback=codebg, colframe=secondarygreen, title=Explainable Correction Example]
\begin{verbatim}
INPUT: "Yesterday I go to library"
CORRECTION POINTS: [("go" ‚Üí "went", VERB:TENSE)]

DIALOGUE RESPONSE WITH EXPLANATION:
"Good try! You used 'go' but since you're talking 
about yesterday (past time), you need the past tense 
'went'. Try: 'Yesterday I went to the library.'"
\end{verbatim}
\end{tcolorbox}

\newpage

\subsection{Pronunciation Assessment}

\subsubsection{Datasets v√† Benchmarks}

\textbf{1. SpeechOcean762 (Zhang et al., 2021)}

Corpus g·ªìm 5,000 utterances t·ª´ 250 non-native speakers v·ªõi annotations ·ªü sentence, word, v√† phoneme level.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Feature} & \textbf{SpeechOcean762} & \textbf{LexiLingo Dataset} \\
\hline
Speakers & 250 & TBD (collecting) \\
Utterances & 5,000 & Target: 10,000 \\
Annotation Levels & 3 (sent/word/phone) & 2 (word/phone) \\
Expert Annotators & 5 & 3 \\
Age Groups & Adults + Children & Adults only (initial) \\
License & Open-source & Open-source \\
\hline
\end{tabular}
\caption{Pronunciation Dataset Comparison}
\end{table}

\textbf{2. HuBERT vs Alternatives}

\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Size} & \textbf{Pre-training} & \textbf{Phoneme Acc} & \textbf{Use Case} \\
\hline
HuBERT-large & 960M & 960h LibriSpeech & 92\% & \textbf{LexiLingo} \\
Wav2Vec 2.0 & 317M & 960h LibriSpeech & 88\% & Alternative \\
ECAPA-TDNN & 24M & VoxCeleb & N/A & Accent class. \\
Whisper & 1.5B & 680K hours & 85\%* & STT primary \\
\hline
\multicolumn{5}{l}{*When used for pronunciation, not optimal for this task}
\end{tabular}
\caption{Pronunciation Models Comparison}
\end{table}

\textbf{L√Ω do ch·ªçn HuBERT:}
\begin{itemize}
    \item \textbf{Self-supervised}: H·ªçc representations t·ªët t·ª´ unlabeled audio
    \item \textbf{Phoneme-focused}: Pre-training target l√† phoneme prediction
    \item \textbf{Forced alignment}: D·ªÖ d√†ng compare v·ªõi native reference
    \item \textbf{Fine-tuning friendly}: C√≥ th·ªÉ adapt cho Vietnamese-accented English
\end{itemize}

\textbf{3. Multi-aspect Assessment (Do et al., 2024)}

Acoustic Feature Mixup for balanced scoring across multiple aspects:

\begin{itemize}
    \item \textbf{Innovation}: Mixup strategies ƒë·ªÉ address data imbalance
    \item \textbf{LexiLingo Adoption}: Ch√∫ng ta s·ª≠ d·ª•ng t∆∞∆°ng t·ª± cho fluency scoring:
    \begin{itemize}
        \item Linear interpolation v·ªõi in-batch average
        \item Goodness-of-pronunciation (GOP) features
        \item Fine-grained error-rate t·ª´ ASR comparison
    \end{itemize}
\end{itemize}

\subsection{Dialogue Systems v√† AI Tutoring}

\subsubsection{Neural Dialog Tutoring (Macina et al., 2023)}

Nghi√™n c·ª©u ph√¢n t√≠ch challenges trong neural dialog tutoring:

\textbf{Key Findings:}
\begin{itemize}
    \item LLMs perform well trong constrained scenarios (√≠t concepts, r√µ r√†ng strategies)
    \item Struggle v·ªõi unconstrained scenarios v√† equitable tutoring
    \item 45\% conversations c√≥ model reasoning errors
\end{itemize}

\textbf{LexiLingo Approach:}
\begin{itemize}
    \item \textbf{Constrained Domain}: Focus v√†o English learning (specific concepts)
    \item \textbf{Explicit Strategies}: Socratic questioning, error correction, vocabulary expansion
    \item \textbf{Context-Aware}: S·ª≠ d·ª•ng fluency/grammar/vocabulary context ƒë·ªÉ guide response
\end{itemize}

\begin{tcolorbox}[colback=codebg, colframe=purple, title=Constrained vs Unconstrained Tutoring]
\begin{verbatim}
UNCONSTRAINED (Challenging):
Student: "Tell me about climate change"
‚Üí Too open-ended, many concepts, unclear goal

CONSTRAINED (LexiLingo):
Student: "Yesterday I go library"
Context: {fluency: 0.45, errors: [VERB:TENSE, DET]}
‚Üí Clear goal: Fix grammar, improve fluency
‚Üí Response: "Good try! Let's fix two things..."
\end{verbatim}
\end{tcolorbox}

\subsubsection{TUTORING Bot (Chae et al., 2023)}

Instruction-grounded conversational agent v·ªõi multi-task learning:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Feature} & \textbf{TUTORING Bot} & \textbf{LexiLingo} \\
\hline
Multi-task Learning & \checkmark & \checkmark \\
Instruction Following & \checkmark & \checkmark \\
Teaching Action Inference & \checkmark & \checkmark \\
Progress Monitoring & \checkmark & \checkmark \\
Domain & English learning & English learning \\
Architecture & Single LLM & 4 LoRA adapters \\
Context Integration & Instruction only & Multi-modal context \\
\hline
\end{tabular}
\caption{Tutoring System Comparison}
\end{table}

\textbf{LexiLingo Advantages:}
\begin{itemize}
    \item \textbf{Richer Context}: Integrate fluency, grammar, vocabulary signals
    \item \textbf{Modular Design}: Separate adapters cho separate concerns
    \item \textbf{Fast Switching}: < 1ms adapter swap vs reload entire model
\end{itemize}

\subsubsection{AutoTutor Dialogue Corpus}

\textbf{Why AutoTutor for Training Data?}

AutoTutor l√† m·ªôt intelligent tutoring system v·ªõi 20+ years research:

\begin{itemize}
    \item \textbf{Socratic Questioning}: D·∫°y qua c√¢u h·ªèi g·ª£i m·ªü thay v√¨ tr·ª±c ti·∫øp ƒë∆∞a ƒë√°p √°n
    \item \textbf{Scaffolding}: H·ªó tr·ª£ d·∫ßn d·∫ßn (fading) khi h·ªçc sinh progress
    \item \textbf{Conversational Coherence}: Duy tr√¨ multi-turn dialogues t·ª± nhi√™n
    \item \textbf{Proven Effectiveness}: Nhi·ªÅu studies cho th·∫•y learning gains
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Training Data} & \textbf{Samples} & \textbf{Focus} & \textbf{Quality} \\
\hline
AutoTutor Corpus & 1,800 & Socratic tutoring & High \\
Intel/orca-dpo-pairs & 1,500 & Instruction following & High \\
Custom conversations & 1,900 & English learning & Medium \\
\hline
\textbf{Total} & \textbf{5,200} & \textbf{Mixed} & \textbf{High} \\
\hline
\end{tabular}
\caption{Dialogue Training Data Sources}
\end{table}

\subsection{Fluency v√† Writing Assessment}

\subsubsection{Automated Essay Scoring (AES)}

\textbf{1. Multi-dimensional Scoring (Sun \& Wang, 2024)}

Automated essay scoring across multiple dimensions (vocabulary, grammar, coherence):

\begin{itemize}
    \item \textbf{Approach}: Fine-tuning + Multiple regression
    \item \textbf{Metrics}: Precision, F1, Quadratic Weighted Kappa
    \item \textbf{LexiLingo Similarity}: Ch√∫ng ta c≈©ng score multiple dimensions nh∆∞ng real-time
\end{itemize}

\textbf{2. CEFR Speaking Assessment (Scaria et al., 2024)}

EvalYaks - LoRA fine-tuned models for CEFR B2 assessment:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{System} & \textbf{Base Model} & \textbf{Accuracy} & \textbf{Variation} \\
\hline
EvalYaks & Mistral 7B & 96\% & 0.35 levels \\
\textbf{LexiLingo} & Qwen2.5 1.5B & 90\%* & 0.4 levels* \\
Best Baseline & GPT-4 & 88\% & 0.45 levels \\
\hline
\multicolumn{4}{l}{*Estimated based on similar evaluation setup}
\end{tabular}
\caption{CEFR Assessment Performance}
\end{table}

\textbf{Key Insight}: Smaller models (1.5-7B) v·ªõi high-quality CEFR-aligned data c√≥ th·ªÉ outperform larger models.

\subsubsection{LLM-as-a-Judge (Son et al., 2024)}

Nghi√™n c·ª©u v·ªÅ limitations c·ªßa LLMs as evaluators:

\textbf{Findings:}
\begin{itemize}
    \item LLMs fail to detect errors trong > 50\% cases
    \item Reference-based evaluation t·ªët h∆°n single-answer/pairwise
    \item Need careful prompt design
\end{itemize}

\textbf{LexiLingo Solution:}
\begin{itemize}
    \item \textbf{Hybrid Approach}: ERRANT rules + LLM refinement
    \item \textbf{Reference-based}: So s√°nh v·ªõi native speaker phonemes (pronunciation)
    \item \textbf{Explicit Criteria}: Clear rubrics trong prompt (fluency 0-1, grammar error types)
\end{itemize}

\newpage

\subsection{Speech Recognition cho Language Learning}

\subsubsection{Whisper vs Alternatives}

\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{WER} & \textbf{Multilingual} & \textbf{Size} & \textbf{Latency} & \textbf{Offline} \\
\hline
Whisper Large & 3-5\% & 99 langs & 1.5GB & 300ms & \checkmark \\
Whisper Small & 8-10\% & 99 langs & 500MB & 100ms & \checkmark \\
Google STT & 2-4\% & 125 langs & Cloud & 200ms & $\times$ \\
Azure STT & 3-5\% & 100 langs & Cloud & 250ms & $\times$ \\
\hline
\end{tabular}
\caption{STT Models Comparison}
\end{table}

\textbf{T·∫°i sao ch·ªçn Whisper?}
\begin{enumerate}
    \item \textbf{Offline-first}: Crucial cho educational apps (kh√¥ng ph·ª• thu·ªôc network)
    \item \textbf{Robust}: Pre-trained tr√™n 680K hours diverse data
    \item \textbf{Timestamps}: Cung c·∫•p word-level timestamps cho pronunciation analysis
    \item \textbf{Open-source}: Kh√¥ng c√≥ cost per request, full control
\end{enumerate}

\subsubsection{Accented English Recognition}

\textbf{AESRC2020 Challenge (Shi et al., 2021)}

160 hours accented English t·ª´ 8 countries, test tr√™n 2 unseen accents:

\begin{itemize}
    \item \textbf{Challenge}: Model generalization cho unseen accents
    \item \textbf{LexiLingo Focus}: Vietnamese-accented English
    \item \textbf{Strategy}: 
    \begin{itemize}
        \item Collect Vietnamese speaker data
        \item Fine-tune Whisper Small tr√™n accent-specific data
        \item Use accent classification (Wav2Vec 2.0) ƒë·ªÉ detect Vietnamese accent
        \item Switch to fine-tuned model khi detect Vietnamese accent
    \end{itemize}
\end{itemize}

\subsection{Ki·∫øn Tr√∫c Unified Multi-Task}

\subsubsection{So S√°nh V·ªõi C√°c Ki·∫øn Tr√∫c Kh√°c}

\begin{table}[h]
\centering
\small
\begin{tabular}{|p{3cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
\hline
\textbf{Architecture} & \textbf{Separate Models} & \textbf{Multi-task Single Model} & \textbf{LexiLingo (LoRA)} \\
\hline
Number of Models & 4 independent & 1 shared & 1 base + 4 adapters \\
\hline
Total Size & 8GB & 2GB & 2.1GB \\
\hline
Task Switching & 2-5s (reload) & Instant & < 1ms (swap) \\
\hline
Training & Independent & Joint training & Sequential LoRA \\
\hline
Flexibility & High & Low & High \\
\hline
Maintenance & Hard & Easy & Moderate \\
\hline
Performance & Best per task & Moderate & Near-best \\
\hline
\end{tabular}
\caption{Multi-Task Architecture Comparison}
\end{table}

\textbf{Advantages c·ªßa LoRA Architecture:}
\begin{enumerate}
    \item \textbf{Modularity}: C√≥ th·ªÉ update t·ª´ng adapter independently
    \item \textbf{Efficiency}: Share base model weights (1.5B params)
    \item \textbf{Scalability}: D·ªÖ th√™m adapters cho new tasks (e.g., writing style, idioms)
    \item \textbf{Performance}: G·∫ßn v·ªõi separate models nh∆∞ng 4x nh·ªè h∆°n
\end{enumerate}

\subsection{K·∫øt Lu·∫≠n So S√°nh}

\begin{tcolorbox}[colback=lightgray, colframe=primaryblue, title=LexiLingo Position in Research Landscape]
\textbf{Strengths:}
\begin{itemize}
    \item \textbf{Mobile-First}: Optimized cho on-device inference (1-2GB RAM)
    \item \textbf{Hybrid Intelligence}: K·∫øt h·ª£p rule-based + neural approaches
    \item \textbf{Multi-Modal}: Integrate speech, text, phoneme analysis
    \item \textbf{Practical Trade-offs}: 5-7\% accuracy loss cho 5-10x smaller size
\end{itemize}

\textbf{Areas for Improvement:}
\begin{itemize}
    \item Grammar correction F0.5: 68 vs SOTA 72.8 (gap: 4.8 points)
    \item Pronunciation analysis: Currently server-side, need on-device solution
    \item Dataset size: 5.2K dialogue samples vs 50K+ in commercial systems
    \item Accent adaptation: Currently Vietnamese only, need multi-accent support
\end{itemize}

\textbf{Novel Contributions:}
\begin{itemize}
    \item First Vietnamese-focused English learning app v·ªõi on-device AI
    \item Two-tier GEC (ERRANT + Qwen) cho speed-accuracy balance
    \item Context-aware dialogue (fluency + grammar + vocabulary signals)
    \item Practical LoRA architecture cho resource-constrained devices
\end{itemize}
\end{tcolorbox}

\newpage

% ===== SECTION 12: CONCLUSION =====
\section{K·∫øt Lu·∫≠n}

\subsection{T√≥m T·∫Øt Ki·∫øn Tr√∫c}

\begin{infobox}[ƒêi·ªÉm N·ªïi B·∫≠t]
\begin{itemize}
    \item \textbf{Clean Architecture}: T√°ch bi·ªát r√µ r√†ng Presentation - Domain - Data
    \item \textbf{Feature-First}: T·ªï ch·ª©c code theo feature, d·ªÖ maintain
    \item \textbf{Dependency Injection}: S·ª≠ d·ª•ng GetIt cho loose coupling
    \item \textbf{Hybrid AI}: K·∫øt h·ª£p Cloud + On-device models
    \item \textbf{Cross-Platform}: Flutter cho iOS, Android, Web
\end{itemize}
\end{infobox}

\subsection{Roadmap Ph√°t Tri·ªÉn}

\begin{enumerate}
    \item \textbf{Phase 1 (Current)}: Core features - Chat, Course, Vocabulary
    \item \textbf{Phase 2}: Voice interaction - STT/TTS integration
    \item \textbf{Phase 3}: Advanced AI - On-device inference v·ªõi LoRA adapters
    \item \textbf{Phase 4}: Social features - Leaderboard, community
\end{enumerate}

\subsection{Th√¥ng Tin D·ª± √Ån}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Th√¥ng Tin} & \textbf{Chi Ti·∫øt} \\
\midrule
Version & v0.2.0 \\
Flutter SDK & 3.29+ \\
Dart SDK & 3.8.1+ \\
Platforms & iOS, Android, Web \\
Repository & \href{https://github.com/InfinityZero3000/LexiLingo}{github.com/InfinityZero3000/LexiLingo} \\
Branch & feature \\
\bottomrule
\end{tabular}
\end{table}

\vfill

\begin{center}
\textit{Document generated on \today}\\[0.5cm]
{\Large\color{primaryblue}\textbf{LexiLingo} - Learn English with AI}
\end{center}

\end{document}
