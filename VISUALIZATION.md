# ğŸ“Š LexiLingo Complete Pipeline Architecture

## Training â†’ Deployment Flow (v2.0 Updated)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      COMPLETE PIPELINE OVERVIEW                          â”‚
â”‚                    Training â†’ Merge â†’ Deploy â†’ Use                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ“ PHASE 1: TRAINING (Kaggle/Colab with GPU)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚     Qwen2.5-1.5B-Instruct (Base Model)              â”‚
  â”‚     + 4-bit NF4 Quantization                        â”‚
  â”‚     + Unified LoRA Adapter (r=32, Î±=64)            â”‚
  â”‚     + Unsloth Optimization (2x faster)             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ 30,806 training samples
                     â”‚ (5 tasks unified)
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Training Dataset                                  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  â€¢ Fluency Scoring (23.6%)                          â”‚
  â”‚  â€¢ Vocabulary Classification (23.0%)                â”‚
  â”‚  â€¢ Grammar Correction (19.1%)                       â”‚
  â”‚  â€¢ Dialogue Generation (21.6%)                      â”‚
  â”‚  â€¢ Explanation Task ğŸ†• (12.7%)                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Training time: 4-5h (P100 + Unsloth)
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   âœ… Trained Model with LoRA Adapter                â”‚
  â”‚      Location: /kaggle/working/outputs/unified      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                        â”‚


ğŸ“¦ PHASE 2: MERGE & EXPORT (Kaggle/Colab)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                                                        â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Merge LoRA Adapter with Base Model                 â”‚
  â”‚  model.save_pretrained_merged(...)                  â”‚
  â”‚  save_method="merged_16bit" (CRITICAL â­)           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Output: lexilingo_qwen25_1.5b_merged/
                     â”‚ Size: ~3.0 GB (FP16, lossless)
                     â”‚ Files: model.safetensors, tokenizer.json, config.json
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  âœ… Merged Model (Full Precision)                   â”‚
  â”‚     Ready for GGUF conversion                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                 â”‚
    â–¼ Option A                        â–¼ Option B
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚HuggingFace  â”‚                 â”‚  Zip File    â”‚
â”‚Upload       â”‚                 â”‚  (Kaggle)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                 â”‚
    â”‚ huggingface-cli download         â”‚ unzip & extract
    â”‚ your-username/lexilingo-...     â”‚ ~/Downloads/...
    â”‚                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚


ğŸ–¥ï¸ PHASE 3: CONVERSION (Local Mac with llama.cpp)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Install llama.cpp (One-time setup)                 â”‚
  â”‚  git clone https://github.com/ggerganov/llama.cpp   â”‚
  â”‚  cd llama.cpp && make                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Download Merged Model                              â”‚
  â”‚  $ huggingface-cli download or unzip                â”‚
  â”‚                                                     â”‚
  â”‚  Location: ~/Projects/llama.cpp/models/             â”‚
  â”‚            lexilingo_qwen25_1.5b_merged/            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Convert to GGUF F16                                â”‚
  â”‚  $ python3 convert_hf_to_gguf.py \                  â”‚
  â”‚      ./models/lexilingo_merged/ \                   â”‚
  â”‚      --outfile ./models/lexilingo_f16.gguf \        â”‚
  â”‚      --outtype f16                                  â”‚
  â”‚                                                     â”‚
  â”‚  Time: 2-3 minutes                                  â”‚
  â”‚  Output: lexilingo_f16.gguf (~3.0 GB, lossless)    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Quantize to Q4_K_M                                 â”‚
  â”‚  $ ./llama-quantize \                               â”‚
  â”‚      ./models/lexilingo_f16.gguf \                  â”‚
  â”‚      ./models/lexilingo_q4_km.gguf \                â”‚
  â”‚      Q4_K_M                                         â”‚
  â”‚                                                     â”‚
  â”‚  Time: 1 minute                                     â”‚
  â”‚  Output: lexilingo_q4_km.gguf (~1.0 GB)            â”‚
  â”‚  Compression: 3x smaller, <2% quality loss         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚


ğŸš€ PHASE 4: DEPLOYMENT (Local Mac)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Option 1: CLI - Direct Inference                   â”‚
  â”‚  $ ./llama-cli -m ./models/lexilingo_q4_km.gguf \   â”‚
  â”‚      -p "Test prompt" -n 64                         â”‚
  â”‚                                                     â”‚
  â”‚  Speed: 10-15 tok/s                                 â”‚
  â”‚  Best for: Quick testing                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Option 2: Server Mode (RECOMMENDED)                â”‚
  â”‚  $ ./llama-server -m ./models/lexilingo_q4_km.gguf \â”‚
  â”‚      --port 8080 --ctx-size 2048                    â”‚
  â”‚                                                     â”‚
  â”‚  Server ready at: http://localhost:8080             â”‚
  â”‚  API: /v1/chat/completions                          â”‚
  â”‚  Best for: Production use                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚


ğŸ’» PHASE 5: INTEGRATION (Any Machine/Language)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Python Client (Recommended)                        â”‚
  â”‚  from export.lexilingo_client import LexiLingoClientâ”‚
  â”‚                                                     â”‚
  â”‚  with LexiLingoClient(..., mode="server") as client:â”‚
  â”‚      result = client.analyze_fluency(...)           â”‚
  â”‚      result = client.classify_vocabulary(...)       â”‚
  â”‚      result = client.correct_grammar(...)           â”‚
  â”‚      result = client.generate_dialogue(...)         â”‚
  â”‚      result = client.explain_error(...)             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  REST API (Any Language)                            â”‚
  â”‚  curl http://localhost:8080/v1/chat/completions \  â”‚
  â”‚      -H "Content-Type: application/json" \          â”‚
  â”‚      -d '{"messages": [...], "max_tokens": 128}'    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  âœ… READY FOR PRODUCTION USE                        â”‚
  â”‚     â€¢ Low latency (100-500ms per request)           â”‚
  â”‚     â€¢ Low memory (2-4 GB RAM)                       â”‚
  â”‚     â€¢ Cross-platform (Mac/Linux/Windows)            â”‚
  â”‚     â€¢ Scalable (can run multiple instances)         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Model Architecture: Input â†’ Processing â†’ Output

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   UNIFIED INFERENCE PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

USER REQUEST
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                          â”‚
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚  TASK IDENTIFICATION            â”‚
                                       â”‚  (From input format)            â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                                 â”‚                                 â”‚
                â–¼                                 â–¼                                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  "Analyze    â”‚              â”‚"Classify the     â”‚              â”‚"Correct this â”‚
        â”‚  fluency: ..." â”‚              â”‚ vocabulary: ..."  â”‚              â”‚ sentence: ..." â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
               â”‚                              â”‚                                 â”‚
               â–¼                              â–¼                                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ FLUENCY TASK â”‚              â”‚VOCABULARY TASK   â”‚              â”‚ GRAMMAR TASK â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
               â”‚                              â”‚                                 â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  UNIFIED LORA ADAPTER          â”‚
                              â”‚  (Qwen2.5-1.5B-Instruct)      â”‚
                              â”‚  LoRA r=32, Î±=64              â”‚
                              â”‚  Unsloth Optimized            â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                             â”‚                                 â”‚
                â–¼                             â–¼                                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Score: 4.5  â”‚              â”‚ Level: B1        â”‚              â”‚ Fixed: "He   â”‚
        â”‚  /5.0        â”‚              â”‚                  â”‚              â”‚ went to..."  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Additional Tasks:
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              "User: What's the weather?"                     â”‚
        â”‚                                                              â”‚
        â”‚  â†“ DIALOGUE TASK â†“                                          â”‚
        â”‚                                                              â”‚
        â”‚  Response: "I don't have real-time weather data but..."     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        "Error: 'He go' â†’ Correct: 'He goes'"                â”‚
        â”‚                                                              â”‚
        â”‚  â†“ EXPLANATION TASK (VIETNAMESE) â†“                          â”‚
        â”‚                                                              â”‚
        â”‚  "Khi chá»§ tá»« lÃ  'He' (sá»‘ Ã­t), Ä‘á»™ng tá»« pháº£i thÃªm 's'..."    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Task Processing Details

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DETAILED TASK PROCESSING                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ FLUENCY ANALYSIS
   Input:    "The cat sat on the mat."
   Process:  Analyze grammatical correctness, natural flow, clarity
   Output:   Score: 5.0/5.0
   Model:    Regression task (0.0-5.0 range)

2ï¸âƒ£ VOCABULARY CLASSIFICATION
   Input:    "The phenomenon is fascinating."
   Process:  Determine CEFR level from vocabulary complexity
   Output:   Level: B2
   Model:    Classification task (A1, A2, B1, B2, C1, C2)

3ï¸âƒ£ GRAMMAR CORRECTION
   Input:    "She don't like apples."
   Process:  Identify errors, apply corrections
   Output:   "She doesn't like apples."
   Model:    Sequence-to-sequence task

4ï¸âƒ£ DIALOGUE GENERATION
   Input:    "User: What's the weather like?"
   Process:  Generate contextually appropriate response
   Output:   "I don't have access to real-time weather data, but..."
   Model:    Conversational task

5ï¸âƒ£ EXPLANATION (VIETNAMESE TUTOR) ğŸ†•
   Input:    "Error: 'I goes' â†’ Correct: 'I go'"
   Process:  Explain grammar rule in Vietnamese, friendly tone
   Output:   "Khi chá»§ tá»« lÃ  'I' (sá»‘ Ã­t), Ä‘á»™ng tá»« khÃ´ng thÃªm 's' nhÃ© em..."
   Model:    Explanation generation task
```

---

## Quantization & Compression Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             MODEL SIZE & QUALITY TRADEOFFS                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRAINING PHASE:
   Qwen2.5-1.5B-Instruct (Base)
   + 4-bit NF4 Quantization
   = ~1.5 GB VRAM for GPU training
   
EXPORT PHASE:
   Merged Model (FP16)
   Size: ~3.0 GB
   Precision: 100% (no loss)
   Use: GGUF conversion baseline

CONVERSION PHASE:
   GGUF F16
   Size: ~3.0 GB
   Precision: 100% (lossless from FP16)
   Loss: 0%
   
DEPLOYMENT PHASE (3x COMPRESSION):
   Q4_K_M â­ RECOMMENDED
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Size: ~1.0 GB       â”‚
   â”‚ Quality Loss: <2%   â”‚
   â”‚ Speed: 10-15 tok/s  â”‚
   â”‚ RAM: 2-4 GB         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   Alternative Options:
   
   Q4_K_S (Faster)
   Size: ~0.9 GB | Loss: 3-5% | Speed: 12-18 tok/s
   
   Q5_K_M (Better Quality)
   Size: ~1.2 GB | Loss: <1% | Speed: 8-12 tok/s
   
   Q8_0 (Lossless)
   Size: ~2.0 GB | Loss: <0.5% | Speed: 8-10 tok/s

COMPARISON:
   Original (FP32): 6 GB
   FP16: 3 GB (50% smaller)
   Q4_K_M: 1 GB (5x smaller overall!)
```

---

## Performance & Resource Requirements

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TRAINING vs INFERENCE REQUIREMENTS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRAINING (Kaggle/Colab GPU)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   GPU Memory:         8 GB (P100) / 16 GB (V100)
   Batch Size:         8 (with Unsloth)
   Gradient Steps:     4
   Training Time:      4-5 hours
   Dataset:            30,806 samples
   Model:              Qwen2.5-1.5B-Instruct
   Optimization:       Unsloth (2x faster, 70% less VRAM)
   Output:             LoRA adapter + Merged model

INFERENCE - Merged Model (CPU, transformers)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Model Size:         3.0 GB (FP16)
   RAM Required:       ~8 GB
   Speed:              3-5 tokens/second
   Latency (per 50 tok): 10-15 seconds
   CPU Cores:          8+ recommended
   Best for:           Development only

INFERENCE - GGUF Q4_K_M (CPU, llama.cpp) â­ BEST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Model Size:         1.0 GB (quantized)
   RAM Required:       2-4 GB
   Speed:              10-15 tokens/second âš¡ (2-3x faster!)
   Latency (per 50 tok): 3-5 seconds
   CPU Cores:          6+ (works well with i9)
   Setup:              llama.cpp server
   Best for:           Production use

Mac Intel i9 Benchmark
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   CPU: 10 cores @ 2.4 GHz
   RAM: 32 GB
   Model: GGUF Q4_K_M
   
   Task 1 - Fluency (20 tokens):     2-3 seconds
   Task 2 - Vocabulary (10 tokens):  1-2 seconds
   Task 3 - Grammar (50 tokens):     3-5 seconds
   Task 4 - Dialogue (100 tokens):   6-8 seconds
   Task 5 - Explanation (200 tokens): 12-15 seconds
```

---

## Deployment Options Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DEPLOYMENT OPTIONS MATRIX                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. LOCAL CLI (Simplest)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ./llama-cli -m model.gguf -p "prompt" -n 64            â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Pros:   No setup, quick testing                         â”‚
   â”‚ Cons:   No persistence, manual for each request         â”‚
   â”‚ Speed:  10-15 tok/s                                     â”‚
   â”‚ Use:    Development & debugging                         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. LOCAL SERVER (Recommended) â­
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ./llama-server -m model.gguf --port 8080               â”‚
   â”‚ â†’ Accessible via REST API                              â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Pros:   Persistent, concurrent requests, REST API      â”‚
   â”‚ Cons:   Requires server setup                           â”‚
   â”‚ Speed:  10-15 tok/s (same model)                        â”‚
   â”‚ Use:    Production on single machine                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. PYTHON CLIENT (Integration)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ from export.lexilingo_client import LexiLingoClient    â”‚
   â”‚ client = LexiLingoClient(model_path, mode="server")    â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Pros:   Easy Python integration, task-specific methods  â”‚
   â”‚ Cons:   Python only                                     â”‚
   â”‚ Speed:  10-15 tok/s                                     â”‚
   â”‚ Use:    Python applications & services                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. REST API (Cross-Language)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ curl http://localhost:8080/v1/chat/completions         â”‚
   â”‚     -H "Content-Type: application/json"                â”‚
   â”‚     -d '{...}'                                          â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Pros:   Language-agnostic, any client                  â”‚
   â”‚ Cons:   Standard REST (not optimized for LexiLingo)    â”‚
   â”‚ Speed:  10-15 tok/s                                     â”‚
   â”‚ Use:    Web services, cross-language integration        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. CLOUD DEPLOYMENT (Optional Future)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ RunPod, Hugging Face Inference, Google Cloud, AWS       â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Pros:   Scalable, managed infrastructure                â”‚
   â”‚ Cons:   Cost, latency                                   â”‚
   â”‚ Speed:  Variable (GPU available)                        â”‚
   â”‚ Use:    Large-scale production                          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure & Tools

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PROJECT STRUCTURE (v2.0)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LexiLingo/DL-Model-Support/
â”‚
â”œâ”€ ğŸ“š TRAINING & SETUP
â”‚  â”œâ”€ scripts/finetune_qwen_lora_kaggle.v1.0.ipynb  (Main training)
â”‚  â”‚   â””â”€ Phase 1: Train with Unsloth
â”‚  â”‚   â””â”€ Phase 2: Merge LoRA + Export (NEW!)
â”‚  â”‚       â”œâ”€ Merge: save_pretrained_merged(..., save_method="merged_16bit")
â”‚  â”‚       â”œâ”€ Export Option A: HuggingFace upload
â”‚  â”‚       â””â”€ Export Option B: Zip download
â”‚  â”‚
â”‚  â”œâ”€ requirements.txt
â”‚  â”‚   â””â”€ All dependencies for training
â”‚  â”‚
â”‚  â”œâ”€ config/
â”‚  â”‚   â”œâ”€ llm_config.yaml
â”‚  â”‚   â”œâ”€ stt_config.yaml
â”‚  â”‚   â”œâ”€ tts_config.yaml
â”‚  â”‚   â””â”€ llm_config.dev.yaml
â”‚  â”‚
â”‚  â””â”€ datasets/
â”‚      â”œâ”€ cefr/
â”‚      â”‚   â””â”€ ENGLISH_CERF_WORDS.csv (Vocabulary reference)
â”‚      â”‚
â”‚      â””â”€ datasets/
â”‚          â”œâ”€ train.jsonl (26,880 samples)
â”‚          â”œâ”€ val.jsonl (1,412 samples)
â”‚          â”œâ”€ train_with_explanation.jsonl (30,806 samples) ğŸ†•
â”‚          â”œâ”€ val_with_explanation.jsonl (1,618 samples) ğŸ†•
â”‚          â”œâ”€ vietnamese_explanations.jsonl (4,132 samples)
â”‚          â”œâ”€ unified_training_data.json
â”‚          â”œâ”€ dialogue_data.json
â”‚          â”œâ”€ fluency_data.json
â”‚          â”œâ”€ grammar_data.json
â”‚          â”œâ”€ vocabulary_data.json
â”‚          â””â”€ merge_explanation_report.json (Statistics)
â”‚
â”œâ”€ ğŸ”„ CONVERSION & DEPLOYMENT (NEW!)
â”‚  â”œâ”€ scripts/deploy_lexilingo.sh  â­ AUTOMATION SCRIPT
â”‚  â”‚   â”œâ”€ Download merged model (HF or zip)
â”‚  â”‚   â”œâ”€ Convert to GGUF F16
â”‚  â”‚   â”œâ”€ Quantize to Q4_K_M
â”‚  â”‚   â”œâ”€ Test inference
â”‚  â”‚   â””â”€ Start server
â”‚  â”‚
â”‚  â”œâ”€ export/lexilingo_client.py  â­ PYTHON CLIENT
â”‚  â”‚   â”œâ”€ LexiLingoCliClient (CLI mode)
â”‚  â”‚   â”œâ”€ LexiLingoServerClient (Server mode)
â”‚  â”‚   â””â”€ LexiLingoClient (High-level API)
â”‚  â”‚       â”œâ”€ analyze_fluency()
â”‚  â”‚       â”œâ”€ classify_vocabulary()
â”‚  â”‚       â”œâ”€ correct_grammar()
â”‚  â”‚       â”œâ”€ generate_dialogue()
â”‚  â”‚       â””â”€ explain_error()
â”‚  â”‚
â”‚  â””â”€ model/
â”‚      â”œâ”€ logging_middleware.py
â”‚      â”œâ”€ adapters/
â”‚      â”‚   â”œâ”€ dialogue_lora_adapter/
â”‚      â”‚   â”œâ”€ fluency_lora_adapter/
â”‚      â”‚   â”œâ”€ grammar_lora_adapter/
â”‚      â”‚   â””â”€ vocabulary_lora_adapter/
â”‚      â”‚
â”‚      â””â”€ outputs/
â”‚          â”œâ”€ dialogue/
â”‚          â”œâ”€ fluency/
â”‚          â”œâ”€ grammar/
â”‚          â”œâ”€ vocabulary/
â”‚          â””â”€ unified/  (Main output)
â”‚
â”œâ”€ ğŸ“– DOCUMENTATION (NEW!)
â”‚  â”œâ”€ docs/DEPLOYMENT_FLOW.md  â­ Complete guide
â”‚  â”‚   â”œâ”€ Phase 1: Training setup
â”‚  â”‚   â”œâ”€ Phase 2: Merge & Export
â”‚  â”‚   â”œâ”€ Phase 3: Convert to GGUF
â”‚  â”‚   â”œâ”€ Phase 4: Deploy
â”‚  â”‚   â”œâ”€ Troubleshooting
â”‚  â”‚   â””â”€ Performance metrics
â”‚  â”‚
â”‚  â”œâ”€ docs/STEPS_AFTER_TRAINING.md  â­ Step-by-step
â”‚  â”‚   â”œâ”€ 7 complete steps
â”‚  â”‚   â”œâ”€ Code examples
â”‚  â”‚   â”œâ”€ Commands reference
â”‚  â”‚   â””â”€ Performance benchmarks
â”‚  â”‚
â”‚  â”œâ”€ docs/EXPLANATION_TASK.md
â”‚  â”‚   â””â”€ Vietnamese teaching methodology
â”‚  â”‚
â”‚  â”œâ”€ docs/Training_Optimization_Guide.md
â”‚  â”‚   â””â”€ Unsloth optimization details
â”‚  â”‚
â”‚  â”œâ”€ docs/UNSLOTH_INTEGRATION_COMPLETE.md
â”‚  â”‚   â””â”€ Unsloth setup & benefits
â”‚  â”‚
â”‚  â”œâ”€ DEPLOYMENT_FLOW.md (Main architecture)
â”‚  â”œâ”€ MODEL_UPDATE_COMPLETE.md (Summary)
â”‚  â”œâ”€ QUICK_REFERENCE.md (Cheat sheet)
â”‚  â”œâ”€ README.md (Project overview)
â”‚  â””â”€ VISUALIZATION.md (This file)
â”‚
â””â”€ ğŸ§ª TESTING
   â”œâ”€ scripts/test_qwen3_simple.py
   â”œâ”€ scripts/test_qwen3_quality.py
   â”œâ”€ scripts/test_qwen_mac_intel.py
   â”œâ”€ scripts/README_TESTING.md
   â””â”€ scripts/README_DATASETS.md
```

---

## Quick Start Command Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QUICK START COMMANDS                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ STEP 1: TRAIN (Kaggle/Colab)
   1. Upload notebook: scripts/finetune_qwen_lora_kaggle.v1.0.ipynb
   2. Run all cells
   3. Total time: 4-5 hours (GPU)
   Output: unified_model/ folder with LoRA adapter

ğŸ“¦ STEP 2: MERGE & EXPORT (Kaggle/Colab)
   # New cells in notebook:
   model.save_pretrained_merged(
       "/kaggle/working/lexilingo_qwen25_1.5b_merged",
       tokenizer,
       save_method="merged_16bit"
   )
   
   # Option A: Push to HuggingFace
   model.push_to_hub("your-username/lexilingo-qwen25-1.5b")
   
   # Option B: Download zip from Kaggle Output
   Output: ~3.0 GB merged model

ğŸ–¥ï¸ STEP 3: SETUP LOCAL (One-time)
   $ cd ~/Projects
   $ git clone https://github.com/ggerganov/llama.cpp.git
   $ cd llama.cpp
   $ make
   Output: llama-cli, llama-quantize, llama-server ready

ğŸ“¥ STEP 4: DOWNLOAD MODEL
   # Option A: From HuggingFace
   $ huggingface-cli download your-username/lexilingo-qwen25-1.5b \
       --local-dir ~/Projects/llama.cpp/models/lexilingo_merged
   
   # Option B: Extract Kaggle zip
   $ unzip ~/Downloads/lexilingo_merged.zip -d ~/Projects/llama.cpp/models/

ğŸ”„ STEP 5: CONVERT & DEPLOY (Automated!)
   $ cd ~/Documents/RepoGitHub/LexiLingo/DL-Model-Support
   
   # Using automation script (RECOMMENDED):
   $ ./scripts/deploy_lexilingo.sh -m hf -u your-username
   
   # Or manual steps:
   $ cd ~/Projects/llama.cpp
   
   # Convert to GGUF F16
   $ python3 convert_hf_to_gguf.py \
       ./models/lexilingo_merged/ \
       --outfile ./models/lexilingo_f16.gguf \
       --outtype f16
   
   # Quantize to Q4_K_M
   $ ./llama-quantize \
       ./models/lexilingo_f16.gguf \
       ./models/lexilingo_q4_km.gguf \
       Q4_K_M

ğŸš€ STEP 6: RUN SERVER
   $ ./llama-server \
       -m ./models/lexilingo_q4_km.gguf \
       --port 8080 \
       --ctx-size 2048
   
   Server ready at: http://localhost:8080

ğŸ’» STEP 7: USE PYTHON CLIENT
   from export.lexilingo_client import LexiLingoClient
   
   with LexiLingoClient("models/lexilingo_q4_km.gguf", mode="server") as client:
       # Fluency
       result = client.analyze_fluency("The cat sat on the mat.")
       print(f"Score: {result.score}")
       
       # Vocabulary
       result = client.classify_vocabulary("The phenomenon is fascinating.")
       print(f"Level: {result.level}")
       
       # Grammar
       result = client.correct_grammar("She don't like apples.")
       print(f"Fixed: {result.corrected_sentence}")
       
       # Dialogue
       result = client.generate_dialogue("What's the weather?")
       print(f"Response: {result.response}")
       
       # Explanation
       result = client.explain_error("I goes", "I go")
       print(f"Explanation: {result.explanation}")

ğŸ§ª STEP 8: TEST (Optional)
   $ ./llama-cli -m ./models/lexilingo_q4_km.gguf \
       -p "Analyze fluency: The cat sat on the mat." \
       -n 64

ğŸ“Š STEP 9: MONITOR
   # Check if server is running
   curl http://localhost:8080/health
   
   # Check model info
   ./llama-cli -m ./models/lexilingo_q4_km.gguf --version
```

---

## Version History & Updates

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VERSION TIMELINE                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

v1.0 (Initial Release)
â”œâ”€ 4 tasks: fluency, vocabulary, grammar, dialogue
â”œâ”€ Qwen2.5-1.5B-Instruct base model
â”œâ”€ Single LoRA adapter (unified)
â”œâ”€ 26,880 training samples
â””â”€ Basic training pipeline

v1.1 (Explanation Task Addition)
â”œâ”€ Added 5th task: Vietnamese grammar explanation (tutor mode)
â”œâ”€ +3,926 explanation samples
â”œâ”€ 30,806 total training samples
â”œâ”€ Friendly tone (Vietnamese pronouns: em, con, nha)
â””â”€ Training: 4-5 hours with Unsloth

v2.0 (Complete Deployment Pipeline) ğŸ†• CURRENT
â”œâ”€ Phase 1: Training with Unsloth (2x faster)
â”œâ”€ Phase 2: Merge LoRA + Export
â”‚   â”œâ”€ save_method="merged_16bit" (lossless)
â”‚   â”œâ”€ HuggingFace upload option
â”‚   â””â”€ Zip download option
â”œâ”€ Phase 3: Convert to GGUF F16 (3.0 GB)
â”œâ”€ Phase 4: Quantize to Q4_K_M (1.0 GB)
â”œâ”€ Phase 5: Deploy with llama.cpp server
â”œâ”€ New Tools:
â”‚   â”œâ”€ deploy_lexilingo.sh (automation script)
â”‚   â”œâ”€ lexilingo_client.py (Python client)
â”‚   â”œâ”€ DEPLOYMENT_FLOW.md (complete guide)
â”‚   â””â”€ STEPS_AFTER_TRAINING.md (step-by-step)
â”œâ”€ 3x model compression (5x overall from FP32)
â”œâ”€ 2-3x faster inference on CPU
â””â”€ Production-ready deployment

ROADMAP (Future)
â”œâ”€ v2.1: Multi-language support
â”œâ”€ v3.0: Larger models (3B, 7B variants)
â”œâ”€ v3.1: Fine-tuning on custom data
â”œâ”€ v4.0: REST API optimization
â”œâ”€ v5.0: Cloud deployment templates
â””â”€ v6.0: Mobile deployment (ONNX)
```

---

## Summary: What Changed in v2.0

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MAJOR UPDATES IN VERSION 2.0                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âŒ BEFORE (v1.0)
   Training Model (LoRA only)
   â”‚
   â””â”€â†’ Download from Kaggle
       â””â”€â†’ Can't easily use on local machine
           â””â”€â†’ Complex conversion process
               â””â”€â†’ Manual CLI usage only

âœ… AFTER (v2.0) - Complete Pipeline
   Training Model with LoRA
   â”‚
   â”œâ”€â†’ Merge with base model (merged_16bit)
   â”‚
   â”œâ”€â†’ Push to HuggingFace OR download zip
   â”‚
   â”œâ”€â†’ Convert to GGUF F16 (lossless)
   â”‚
   â”œâ”€â†’ Quantize to Q4_K_M (3x compression)
   â”‚
   â”œâ”€â†’ Deploy with llama.cpp server
   â”‚
   â”œâ”€â†’ Use Python client (easy integration)
   â”‚
   â””â”€â†’ Production-ready! ğŸš€

KEY IMPROVEMENTS:

1. ğŸ“¦ Export Format
   Before: LoRA adapter only (~50MB)
   After:  Full merged model + GGUF + Quantized (~1GB total)

2. ğŸ”„ Conversion
   Before: Manual convert_hf_to_gguf.py steps
   After:  Automated deploy_lexilingo.sh script

3. ğŸš€ Deployment
   Before: CLI only, one request at a time
   After:  Server mode (concurrent), Python client

4. âš¡ Performance
   Before: 3-5 tok/s on CPU (transformers)
   After:  10-15 tok/s on CPU (llama.cpp) â† 2-3x faster!

5. ğŸ’¾ Size
   Before: 3.0 GB merged model
   After:  1.0 GB quantized (3x smaller)

6. ğŸ“š Documentation
   Before: Minimal docs
   After:  Complete guides:
           â”œâ”€ DEPLOYMENT_FLOW.md
           â”œâ”€ STEPS_AFTER_TRAINING.md
           â”œâ”€ deploy_lexilingo.sh
           â””â”€ lexilingo_client.py

7. ğŸ Integration
   Before: Complex manual setup
   After:  Simple Python API:
           ```python
           with LexiLingoClient(...) as client:
               result = client.analyze_fluency("...")
           ```
```

---

**Updated:** 2026-01-28  
**Version:** 2.0 (Complete Pipeline)  
**Status:** âœ… Production Ready

ğŸ“– **See also:**
- [docs/DEPLOYMENT_FLOW.md](docs/DEPLOYMENT_FLOW.md) - Full deployment guide
- [docs/STEPS_AFTER_TRAINING.md](docs/STEPS_AFTER_TRAINING.md) - Step-by-step instructions
- [scripts/deploy_lexilingo.sh](scripts/deploy_lexilingo.sh) - Automation script
- [export/lexilingo_client.py](export/lexilingo_client.py) - Python client library
