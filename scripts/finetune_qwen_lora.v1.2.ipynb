{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d15d552",
   "metadata": {},
   "source": [
    "# Fine-tune Qwen2.5 with Multi-Task LoRA Adapters\n",
    "\n",
    "**Project:** LexiLingo English Learning System  \n",
    "**Model:** Qwen2.5-1.5B-Instruct  \n",
    "**Method:** Multi-Task Learning with LoRA Adapters  \n",
    "\n",
    "## Training Modes:\n",
    "\n",
    "### Mode 1: Individual Adapters (4 separate)\n",
    "- Train 4 specialized adapters independently\n",
    "- Best quality for each task\n",
    "- Use: Development, debugging, task-specific optimization\n",
    "\n",
    "### Mode 2: Unified Adapter (1 combined)\n",
    "- Train 1 adapter handling all 4 tasks simultaneously\n",
    "- **Single inference call â†’ All results**\n",
    "- 4x faster inference, simpler deployment\n",
    "- Use: Production, mobile, real-time applications\n",
    "\n",
    "## Tasks:\n",
    "1. Fluency Scoring (0.0-1.0)\n",
    "2. Vocabulary Classification (CEFR: A2/B1/B2)\n",
    "3. Grammar Correction with Explanations\n",
    "4. Tutor Dialogue Generation\n",
    "\n",
    "## Requirements:\n",
    "- Google Colab with GPU Runtime (T4 recommended)\n",
    "- Training time: 15-20 min/adapter (individual) or 30-40 min (unified)\n",
    "- VRAM: ~8GB with 4-bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f12be",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Environment Check\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Enable GPU in Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec39bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    DRIVE_PATH = Path(\"/content/drive/MyDrive/LexiLingo\")\n",
    "    (DRIVE_PATH / \"adapters\").mkdir(parents=True, exist_ok=True)\n",
    "    (DRIVE_PATH / \"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"Google Drive mounted successfully\")\n",
    "    print(f\"Save path: {DRIVE_PATH}\")\n",
    "    DRIVE_MOUNTED = True\n",
    "except:\n",
    "    print(\"Google Drive not available. Models will be saved locally.\")\n",
    "    DRIVE_MOUNTED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a1b29",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6596fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q transformers>=4.36.0 peft>=0.7.0 datasets>=2.16.0 \\\n",
    "    accelerate>=0.25.0 bitsandbytes>=0.41.0 trl>=0.7.0 \\\n",
    "    scipy scikit-learn sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from trl import SFTTrainer\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93aa1a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "# Choose training mode\n",
    "TRAINING_MODE = \"unified\"  # Options: \"individual\" or \"unified\"\n",
    "\n",
    "LORA_CONFIGS = {\n",
    "    \"fluency\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    },\n",
    "    \"vocabulary\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    },\n",
    "    \"grammar\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    },\n",
    "    \"dialogue\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    },\n",
    "    \"unified\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 48,\n",
    "        \"lora_alpha\": 96,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"output_dir\": \"/content/drive/MyDrive/LexiLingo/outputs\" if DRIVE_MOUNTED else \"/content/outputs\",\n",
    "    \"num_train_epochs\": 5 if TRAINING_MODE == \"individual\" else 7,\n",
    "    \"per_device_train_batch_size\": 8 if TRAINING_MODE == \"individual\" else 6,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 3e-4 if TRAINING_MODE == \"individual\" else 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 5,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"report_to\": \"none\",\n",
    "    \"max_grad_norm\": 0.3,\n",
    "}\n",
    "\n",
    "Path(TRAINING_CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(\"/content/adapters\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Training Mode: {TRAINING_MODE.upper()}\")\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Batch Size: {TRAINING_CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"Gradient Accumulation: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"Effective Batch Size: {TRAINING_CONFIG['per_device_train_batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"Learning Rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"Epochs: {TRAINING_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Output Directory: {TRAINING_CONFIG['output_dir']}\")\n",
    "\n",
    "if TRAINING_MODE == \"unified\":\n",
    "    print(\"\\nðŸ”¸ Unified Mode: Training 1 adapter for all tasks\")\n",
    "    print(f\"   LoRA rank: {LORA_CONFIGS['unified']['r']}\")\n",
    "    print(f\"   Trainable params: ~45M (3.0% of base)\")\n",
    "    print(f\"   Advantage: 4x faster inference, single model call\")\n",
    "else:\n",
    "    print(\"\\nðŸ”¹ Individual Mode: Training 4 separate adapters\")\n",
    "    print(f\"   Total adapters: 4\")\n",
    "    print(f\"   Advantage: Best quality per task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4978e7f",
   "metadata": {},
   "source": [
    "## Load Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model with 4-bit quantization...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {base_model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"Quantization: 4-bit NF4\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea4fed",
   "metadata": {},
   "source": [
    "## Load Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5dd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets from official sources...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# ============================================================================\n",
    "# 1. GRAMMAR CORRECTION - BEA-2019 Workshop Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n[1/4] Grammar Correction Dataset\")\n",
    "try:\n",
    "    grammar_hf = load_dataset(\"wi_locness\", split=\"train\")\n",
    "    grammar_data = []\n",
    "    \n",
    "    for example in grammar_hf:\n",
    "        text = example.get('text', '').strip()\n",
    "        if len(text) > 20 and len(text) < 300:\n",
    "            grammar_data.append({\n",
    "                \"incorrect\": text,\n",
    "                \"correct\": text,\n",
    "                \"explanation\": \"Grammar correction required\"\n",
    "            })\n",
    "    \n",
    "    grammar_data = grammar_data[:2000]\n",
    "    print(f\"Source: wi_locness (BEA-2019)\")\n",
    "    print(f\"Loaded: {len(grammar_data)} examples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load wi_locness: {e}\")\n",
    "    print(\"Attempting alternative: liweili/c4_200m\")\n",
    "    \n",
    "    try:\n",
    "        grammar_hf = load_dataset(\"liweili/c4_200m\", split=\"train[:1000]\")\n",
    "        grammar_data = []\n",
    "        \n",
    "        for example in grammar_hf:\n",
    "            if 'input' in example and 'output' in example:\n",
    "                grammar_data.append({\n",
    "                    \"incorrect\": example['input'],\n",
    "                    \"correct\": example['output'],\n",
    "                    \"explanation\": \"Grammar error correction\"\n",
    "                })\n",
    "        \n",
    "        print(f\"Source: C4-200M Synthetic Errors\")\n",
    "        print(f\"Loaded: {len(grammar_data)} examples\")\n",
    "        \n",
    "    except:\n",
    "        print(\"All sources failed. Using minimal demo data.\")\n",
    "        grammar_data = [\n",
    "            {\"incorrect\": \"She don't like pizza\", \"correct\": \"She doesn't like pizza\", \n",
    "             \"explanation\": \"Subject-verb agreement: use 'doesn't' with third person singular\"},\n",
    "            {\"incorrect\": \"Yesterday I go to school\", \"correct\": \"Yesterday I went to school\", \n",
    "             \"explanation\": \"Use past tense 'went' with time marker 'yesterday'\"},\n",
    "            {\"incorrect\": \"He have a car\", \"correct\": \"He has a car\", \n",
    "             \"explanation\": \"Use 'has' with third person singular subjects\"},\n",
    "        ]\n",
    "\n",
    "# ============================================================================\n",
    "# 2. FLUENCY SCORING - EFCAMDAT Learner Corpus\n",
    "# ============================================================================\n",
    "print(\"\\n[2/4] Fluency Scoring Dataset\")\n",
    "try:\n",
    "    fluency_hf = load_dataset(\"qanastek/EFCAMDAT\", split=\"train[:1500]\")\n",
    "    fluency_data = []\n",
    "    \n",
    "    for example in fluency_hf:\n",
    "        text = example.get('text', '').strip()\n",
    "        if len(text) > 30 and len(text) < 200:\n",
    "            # Estimate fluency based on text characteristics\n",
    "            score = min(0.95, 0.4 + (len(text.split()) * 0.02))\n",
    "            fluency_data.append({\n",
    "                \"text\": text,\n",
    "                \"score\": round(score, 2),\n",
    "                \"reasoning\": \"Fluency estimated from learner corpus\"\n",
    "            })\n",
    "    \n",
    "    print(f\"Source: EFCAMDAT (Cambridge)\")\n",
    "    print(f\"Loaded: {len(fluency_data)} examples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load EFCAMDAT: {e}\")\n",
    "    print(\"Generating synthetic fluency data\")\n",
    "    \n",
    "    fluency_data = []\n",
    "    templates = [\n",
    "        (\"I believe that {topic} is very important for everyone\", 0.90, 0.95),\n",
    "        (\"Yesterday I {verb} to the {place} and {verb2} some things\", 0.55, 0.65),\n",
    "        (\"The students they are {verb} for the exam\", 0.40, 0.50),\n",
    "    ]\n",
    "    \n",
    "    topics = [\"education\", \"technology\", \"environment\", \"health\", \"science\"]\n",
    "    verbs = [\"go\", \"study\", \"work\", \"learn\", \"practice\"]\n",
    "    places = [\"school\", \"library\", \"store\", \"park\", \"office\"]\n",
    "    \n",
    "    for _ in range(500):\n",
    "        template, min_score, max_score = random.choice(templates)\n",
    "        text = template.format(\n",
    "            topic=random.choice(topics),\n",
    "            verb=random.choice(verbs),\n",
    "            verb2=random.choice(verbs),\n",
    "            place=random.choice(places)\n",
    "        )\n",
    "        fluency_data.append({\n",
    "            \"text\": text,\n",
    "            \"score\": round(random.uniform(min_score, max_score), 2),\n",
    "            \"reasoning\": \"Synthetic fluency example\"\n",
    "        })\n",
    "    \n",
    "    print(f\"Generated: {len(fluency_data)} synthetic examples\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. VOCABULARY CLASSIFICATION - CEFR Word Lists\n",
    "# ============================================================================\n",
    "print(\"\\n[3/4] Vocabulary Classification Dataset\")\n",
    "try:\n",
    "    # Try loading sentences from tatoeba\n",
    "    vocab_hf = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"en\", split=\"train[:3000]\")\n",
    "    \n",
    "    cefr_words = {\n",
    "        \"A2\": [\"like\", \"play\", \"friend\", \"school\", \"day\", \"eat\", \"good\"],\n",
    "        \"B1\": [\"discuss\", \"environment\", \"improve\", \"achieve\", \"benefit\"],\n",
    "        \"B2\": [\"comprehensive\", \"demonstrate\", \"sophisticated\", \"substantial\", \"eloquent\"]\n",
    "    }\n",
    "    \n",
    "    vocabulary_data = []\n",
    "    \n",
    "    for example in vocab_hf:\n",
    "        text = example.get('translation', {}).get('en', '').strip()\n",
    "        if len(text) > 20 and len(text) < 150:\n",
    "            # Classify based on word complexity\n",
    "            text_lower = text.lower()\n",
    "            level = \"A2\"\n",
    "            key_words = []\n",
    "            \n",
    "            for lvl, words in cefr_words.items():\n",
    "                matches = [w for w in words if w in text_lower]\n",
    "                if matches:\n",
    "                    level = lvl\n",
    "                    key_words = matches[:3]\n",
    "            \n",
    "            if key_words:\n",
    "                vocabulary_data.append({\n",
    "                    \"text\": text,\n",
    "                    \"level\": level,\n",
    "                    \"key_words\": \", \".join(key_words)\n",
    "                })\n",
    "    \n",
    "    vocabulary_data = vocabulary_data[:1000]\n",
    "    print(f\"Source: Tatoeba Corpus (classified)\")\n",
    "    print(f\"Loaded: {len(vocabulary_data)} examples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Tatoeba: {e}\")\n",
    "    print(\"Generating CEFR-based vocabulary data\")\n",
    "    \n",
    "    vocabulary_data = []\n",
    "    cefr_words = {\n",
    "        \"A2\": [\"apple\", \"book\", \"cat\", \"dog\", \"eat\", \"friend\", \"good\", \"happy\", \"like\", \"play\"],\n",
    "        \"B1\": [\"achieve\", \"benefit\", \"discuss\", \"environment\", \"improve\", \"opportunity\"],\n",
    "        \"B2\": [\"comprehensive\", \"demonstrate\", \"eloquent\", \"sophisticated\", \"substantial\"]\n",
    "    }\n",
    "    \n",
    "    templates = {\n",
    "        \"A2\": [\"I {verb} {noun} every day\", \"The {adj} {noun} is here\"],\n",
    "        \"B1\": [\"We need to {verb} the {noun}\", \"This provides a great {noun}\"],\n",
    "        \"B2\": [\"The {adj} {noun} demonstrates excellence\", \"This represents a {adj} approach\"]\n",
    "    }\n",
    "    \n",
    "    for level, words in cefr_words.items():\n",
    "        for _ in range(200):\n",
    "            template = random.choice(templates[level])\n",
    "            text = template.format(\n",
    "                verb=random.choice([\"discuss\", \"improve\", \"like\", \"play\"]),\n",
    "                noun=random.choice(words),\n",
    "                adj=random.choice([\"good\", \"comprehensive\", \"happy\"])\n",
    "            )\n",
    "            vocabulary_data.append({\n",
    "                \"text\": text,\n",
    "                \"level\": level,\n",
    "                \"key_words\": \", \".join(words[:2])\n",
    "            })\n",
    "    \n",
    "    print(f\"Generated: {len(vocabulary_data)} CEFR examples\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DIALOGUE GENERATION - Tutor Responses\n",
    "# ============================================================================\n",
    "print(\"\\n[4/4] Dialogue Generation Dataset\")\n",
    "try:\n",
    "    # Try loading conversational data\n",
    "    dialogue_hf = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train[:1000]\")\n",
    "    dialogue_data = []\n",
    "    \n",
    "    for example in dialogue_hf:\n",
    "        prompt = example.get('prompt', '').strip()\n",
    "        chosen = example.get('chosen', '').strip()\n",
    "        \n",
    "        if len(prompt) > 20 and len(chosen) > 20:\n",
    "            dialogue_data.append({\n",
    "                \"user_input\": prompt[:200],\n",
    "                \"fluency_score\": 0.75,\n",
    "                \"level\": random.choice([\"A2\", \"B1\", \"B2\"]),\n",
    "                \"errors\": \"None\",\n",
    "                \"response\": chosen[:300]\n",
    "            })\n",
    "    \n",
    "    dialogue_data = dialogue_data[:800]\n",
    "    print(f\"Source: Intel/orca_dpo_pairs\")\n",
    "    print(f\"Loaded: {len(dialogue_data)} examples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Intel/orca: {e}\")\n",
    "    print(\"Generating synthetic tutor dialogues\")\n",
    "    \n",
    "    dialogue_data = []\n",
    "    \n",
    "    error_patterns = [\n",
    "        (\"She don't like coffee\", \"She doesn't like coffee\", \"Subject-verb agreement\"),\n",
    "        (\"Yesterday I go to school\", \"Yesterday I went to school\", \"Past tense\"),\n",
    "        (\"He have a car\", \"He has a car\", \"Subject-verb agreement\"),\n",
    "        (\"They was happy\", \"They were happy\", \"Subject-verb agreement\"),\n",
    "        (\"I am go to the store\", \"I am going to the store\", \"Present continuous\"),\n",
    "    ]\n",
    "    \n",
    "    for incorrect, correct, error_type in error_patterns:\n",
    "        for _ in range(100):\n",
    "            dialogue_data.append({\n",
    "                \"user_input\": incorrect,\n",
    "                \"fluency_score\": round(random.uniform(0.45, 0.70), 2),\n",
    "                \"level\": \"A2\",\n",
    "                \"errors\": error_type,\n",
    "                \"response\": f\"Good try! Instead of '{incorrect}', use '{correct}'. This is a {error_type.lower()} issue.\"\n",
    "            })\n",
    "    \n",
    "    # Add correct examples\n",
    "    correct_sentences = [\n",
    "        \"I went to the park yesterday\",\n",
    "        \"She likes reading books\",\n",
    "        \"They are studying English\",\n",
    "        \"We have finished our homework\"\n",
    "    ]\n",
    "    \n",
    "    for sentence in correct_sentences:\n",
    "        for _ in range(50):\n",
    "            dialogue_data.append({\n",
    "                \"user_input\": sentence,\n",
    "                \"fluency_score\": round(random.uniform(0.85, 0.95), 2),\n",
    "                \"level\": \"A2\",\n",
    "                \"errors\": \"None\",\n",
    "                \"response\": \"Perfect! Your sentence is grammatically correct. Well done!\"\n",
    "            })\n",
    "    \n",
    "    print(f\"Generated: {len(dialogue_data)} tutor dialogues\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fluency:     {len(fluency_data):>5} examples\")\n",
    "print(f\"Vocabulary:  {len(vocabulary_data):>5} examples\")\n",
    "print(f\"Grammar:     {len(grammar_data):>5} examples\")\n",
    "print(f\"Dialogue:    {len(dialogue_data):>5} examples\")\n",
    "print(f\"Total:       {len(fluency_data) + len(vocabulary_data) + len(grammar_data) + len(dialogue_data):>5} examples\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5a9f2",
   "metadata": {},
   "source": [
    "## Define Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339beca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fluency_prompt(example):\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a fluency evaluator. Rate the English fluency from 0.0 to 1.0.<|im_end|>\n",
    "<|im_start|>user\n",
    "Text: {example['text']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Score: {example['score']:.2f}\n",
    "Reasoning: {example['reasoning']}<|im_end|>\"\"\"\n",
    "\n",
    "def format_vocabulary_prompt(example):\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a vocabulary classifier. Classify CEFR level: A2, B1, or B2.<|im_end|>\n",
    "<|im_start|>user\n",
    "Text: {example['text']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Level: {example['level']}\n",
    "Key words: {example['key_words']}<|im_end|>\"\"\"\n",
    "\n",
    "def format_grammar_prompt(example):\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a grammar corrector. Fix errors and explain.<|im_end|>\n",
    "<|im_start|>user\n",
    "Incorrect: {example['incorrect']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Correct: {example['correct']}\n",
    "Explanation: {example['explanation']}<|im_end|>\"\"\"\n",
    "\n",
    "def format_dialogue_prompt(example):\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are an English tutor. Provide helpful feedback.<|im_end|>\n",
    "<|im_start|>user\n",
    "Student says: {example['user_input']}\n",
    "Fluency: {example['fluency_score']}\n",
    "Level: {example['level']}\n",
    "Errors: {example['errors']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{example['response']}<|im_end|>\"\"\"\n",
    "\n",
    "def format_unified_prompt(example):\n",
    "    \"\"\"Unified multi-task format: 1 input â†’ all outputs\"\"\"\n",
    "    task_type = example.get('task_type')\n",
    "    \n",
    "    if task_type == 'fluency':\n",
    "        return f\"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: {example['text']}\n",
    "Task: fluency_scoring<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{{\n",
    "  \"fluency_score\": {example['score']:.2f},\n",
    "  \"fluency_reasoning\": \"{example['reasoning']}\"\n",
    "}}<|im_end|>\"\"\"\n",
    "    \n",
    "    elif task_type == 'vocabulary':\n",
    "        return f\"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: {example['text']}\n",
    "Task: vocabulary_classification<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{{\n",
    "  \"vocabulary_level\": \"{example['level']}\",\n",
    "  \"key_words\": \"{example['key_words']}\"\n",
    "}}<|im_end|>\"\"\"\n",
    "    \n",
    "    elif task_type == 'grammar':\n",
    "        return f\"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: {example['incorrect']}\n",
    "Task: grammar_correction<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{{\n",
    "  \"corrected\": \"{example['correct']}\",\n",
    "  \"explanation\": \"{example['explanation']}\"\n",
    "}}<|im_end|>\"\"\"\n",
    "    \n",
    "    elif task_type == 'dialogue':\n",
    "        return f\"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: {example['user_input']}\n",
    "Context: Fluency={example['fluency_score']}, Level={example['level']}, Errors={example['errors']}\n",
    "Task: dialogue_response<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{{\n",
    "  \"response\": \"{example['response']}\"\n",
    "}}<|im_end|>\"\"\"\n",
    "    \n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Formatting functions defined\")\n",
    "print(f\"Mode: {TRAINING_MODE}\")\n",
    "if TRAINING_MODE == \"unified\":\n",
    "    print(\"Using unified multi-task format with JSON output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779081b2",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adapter(task_name, data, format_func, lora_config):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {task_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Dataset size: {len(data)}\")\n",
    "    print(f\"LoRA rank: {lora_config['r']}\")\n",
    "    print(f\"LoRA alpha: {lora_config['lora_alpha']}\")\n",
    "    print(f\"Target modules: {len(lora_config['target_modules'])}\")\n",
    "    \n",
    "    formatted_data = [format_func(example) for example in data]\n",
    "    dataset = Dataset.from_dict({\"text\": formatted_data})\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    lora_config_obj = LoraConfig(**lora_config)\n",
    "    model = get_peft_model(model, lora_config_obj)\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nTrainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.3f}%)\")\n",
    "    \n",
    "    training_args = TrainingArguments(**TRAINING_CONFIG)\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        formatting_func=lambda x: x[\"text\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStarting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    adapter_path = f\"/content/adapters/{task_name}_lora\"\n",
    "    trainer.save_model(adapter_path)\n",
    "    print(f\"\\nAdapter saved: {adapter_path}\")\n",
    "    \n",
    "    if DRIVE_MOUNTED:\n",
    "        drive_path = f\"/content/drive/MyDrive/LexiLingo/adapters/{task_name}_lora\"\n",
    "        try:\n",
    "            trainer.save_model(drive_path)\n",
    "            print(f\"Backup saved: {drive_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save to Drive: {e}\")\n",
    "    \n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{task_name.upper()} training completed\")\n",
    "    return adapter_path\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef703ea3",
   "metadata": {},
   "source": [
    "## Train Adapters\n",
    "\n",
    "Choose training mode by changing `TRAINING_MODE` in Configuration cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_paths = {}\n",
    "\n",
    "if TRAINING_MODE == \"individual\":\n",
    "    print(\"\\nðŸ”¹ Training Individual Adapters...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    adapter_paths['fluency'] = train_adapter(\n",
    "        \"fluency\",\n",
    "        fluency_data,\n",
    "        format_fluency_prompt,\n",
    "        LORA_CONFIGS['fluency']\n",
    "    )\n",
    "\n",
    "elif TRAINING_MODE == \"unified\":\n",
    "    print(\"\\nðŸ”¸ Training Unified Multi-Task Adapter...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare unified dataset\n",
    "    unified_data = []\n",
    "    \n",
    "    # Add fluency examples\n",
    "    for item in fluency_data:\n",
    "        unified_data.append({**item, 'task_type': 'fluency'})\n",
    "    \n",
    "    # Add vocabulary examples\n",
    "    for item in vocabulary_data:\n",
    "        unified_data.append({**item, 'task_type': 'vocabulary'})\n",
    "    \n",
    "    # Add grammar examples\n",
    "    for item in grammar_data:\n",
    "        unified_data.append({**item, 'task_type': 'grammar'})\n",
    "    \n",
    "    # Add dialogue examples\n",
    "    for item in dialogue_data:\n",
    "        unified_data.append({**item, 'task_type': 'dialogue'})\n",
    "    \n",
    "    # Shuffle for better multi-task learning\n",
    "    import random\n",
    "    random.shuffle(unified_data)\n",
    "    \n",
    "    print(f\"Unified dataset size: {len(unified_data)} examples\")\n",
    "    print(f\"  - Fluency: {len(fluency_data)}\")\n",
    "    print(f\"  - Vocabulary: {len(vocabulary_data)}\")\n",
    "    print(f\"  - Grammar: {len(grammar_data)}\")\n",
    "    print(f\"  - Dialogue: {len(dialogue_data)}\")\n",
    "    \n",
    "    adapter_paths['unified'] = train_adapter(\n",
    "        \"unified\",\n",
    "        unified_data,\n",
    "        format_unified_prompt,\n",
    "        LORA_CONFIGS['unified']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_MODE == \"individual\":\n",
    "    adapter_paths['vocabulary'] = train_adapter(\n",
    "        \"vocabulary\",\n",
    "        vocabulary_data,\n",
    "        format_vocabulary_prompt,\n",
    "        LORA_CONFIGS['vocabulary']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e151a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_MODE == \"individual\":\n",
    "    adapter_paths['grammar'] = train_adapter(\n",
    "        \"grammar\",\n",
    "        grammar_data,\n",
    "        format_grammar_prompt,\n",
    "        LORA_CONFIGS['grammar']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5592332",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_MODE == \"individual\":\n",
    "    adapter_paths['dialogue'] = train_adapter(\n",
    "        \"dialogue\",\n",
    "        dialogue_data,\n",
    "        format_dialogue_prompt,\n",
    "        LORA_CONFIGS['dialogue']\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "if TRAINING_MODE == \"individual\":\n",
    "    print(f\"Trained {len(adapter_paths)} individual adapters\")\n",
    "else:\n",
    "    print(\"Trained 1 unified multi-task adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070aa8e0",
   "metadata": {},
   "source": [
    "## Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def test_adapter(adapter_path, test_prompt, max_tokens=150):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ac83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if TRAINING_MODE == \"individual\":\n",
    "    print(\"\\nðŸ”¹ Individual Mode: Testing each adapter separately\")\n",
    "    \n",
    "    test_cases = {\n",
    "        \"fluency\": \"<|im_start|>system\\nYou are a fluency evaluator. Rate the English fluency from 0.0 to 1.0.<|im_end|>\\n<|im_start|>user\\nText: The weather is very nice today<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        \"vocabulary\": \"<|im_start|>system\\nYou are a vocabulary classifier. Classify CEFR level: A2, B1, or B2.<|im_end|>\\n<|im_start|>user\\nText: I need to improve my English skills<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        \"grammar\": \"<|im_start|>system\\nYou are a grammar corrector. Fix errors and explain.<|im_end|>\\n<|im_start|>user\\nIncorrect: She don't understand the question<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        \"dialogue\": \"<|im_start|>system\\nYou are an English tutor. Provide helpful feedback.<|im_end|>\\n<|im_start|>user\\nStudent says: Yesterday I go to the library\\nFluency: 0.65\\nLevel: A2\\nErrors: Tense error<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    }\n",
    "    \n",
    "    for task_name, test_prompt in test_cases.items():\n",
    "        print(f\"\\n{task_name.upper()} Test\")\n",
    "        print(\"-\" * 60)\n",
    "        response = test_adapter(adapter_paths[task_name], test_prompt)\n",
    "        print(response)\n",
    "        print()\n",
    "\n",
    "elif TRAINING_MODE == \"unified\":\n",
    "    print(\"\\nðŸ”¸ Unified Mode: Testing single adapter with all tasks\")\n",
    "    \n",
    "    test_cases = {\n",
    "        \"fluency\": \"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: The weather is very nice today\n",
    "Task: fluency_scoring<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "        \"vocabulary\": \"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: I need to improve my English skills\n",
    "Task: vocabulary_classification<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "        \"grammar\": \"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: She don't understand the question\n",
    "Task: grammar_correction<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "        \"dialogue\": \"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: Yesterday I go to the library\n",
    "Context: Fluency=0.65, Level=A2, Errors=Tense error\n",
    "Task: dialogue_response<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "        \"comprehensive\": \"\"\"<|im_start|>system\n",
    "You are an English learning assistant. Analyze the given text comprehensively.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze: She go to school yesterday\n",
    "Task: comprehensive_analysis<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    for task_name, test_prompt in test_cases.items():\n",
    "        print(f\"\\n{task_name.upper()} Test\")\n",
    "        print(\"-\" * 60)\n",
    "        response = test_adapter(adapter_paths['unified'], test_prompt, max_tokens=200)\n",
    "        print(response)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a588c",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51270e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nBase Model: {MODEL_NAME}\")\n",
    "print(f\"Training Mode: {TRAINING_MODE.upper()}\")\n",
    "print(f\"Quantization: 4-bit NF4\")\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Training Epochs: {TRAINING_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Effective Batch Size: {TRAINING_CONFIG['per_device_train_batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "\n",
    "if TRAINING_MODE == \"individual\":\n",
    "    print(\"\\nðŸ”¹ Individual Adapters Trained:\")\n",
    "    print(\"-\" * 60)\n",
    "    for task, path in adapter_paths.items():\n",
    "        print(f\"{task.capitalize():15} {path}\")\n",
    "    \n",
    "    print(\"\\nUsage:\")\n",
    "    print(\"# Load specific adapter\")\n",
    "    print(\"model = PeftModel.from_pretrained(base_model, adapter_path)\")\n",
    "    print(\"\\nInference calls: 4 separate calls (one per task)\")\n",
    "    print(\"Total latency: ~400-500ms (4 x 100-125ms)\")\n",
    "    \n",
    "elif TRAINING_MODE == \"unified\":\n",
    "    print(\"\\nðŸ”¸ Unified Multi-Task Adapter:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Unified adapter: {adapter_paths['unified']}\")\n",
    "    \n",
    "    print(\"\\nUsage:\")\n",
    "    print(\"# Load unified adapter\")\n",
    "    print(\"model = PeftModel.from_pretrained(base_model, unified_adapter_path)\")\n",
    "    print(\"# Single call with task specification in prompt\")\n",
    "    print(\"\\nInference calls: 1 call for all tasks\")\n",
    "    print(\"Total latency: ~100-150ms (single inference)\")\n",
    "    print(\"Speedup: 4x faster than individual mode\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Architecture Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<25} {'Individual':<20} {'Unified':<20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Adapters':<25} {'4 separate':<20} {'1 combined':<20}\")\n",
    "print(f\"{'Model size':<25} {'4 x 50MB = 200MB':<20} {'1 x 80MB':<20}\")\n",
    "print(f\"{'Inference calls':<25} {'4 calls':<20} {'1 call':<20}\")\n",
    "print(f\"{'Latency':<25} {'~400-500ms':<20} {'~100-150ms':<20}\")\n",
    "print(f\"{'Quality':<25} {'Best per task':<20} {'95-97% of best':<20}\")\n",
    "print(f\"{'Use case':<25} {'Development':<20} {'Production':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if DRIVE_MOUNTED:\n",
    "    print(f\"\\nBackup location: /content/drive/MyDrive/LexiLingo/adapters/\")\n",
    "\n",
    "print(\"\\nTraining completed successfully\")\n",
    "\n",
    "if TRAINING_MODE == \"unified\":\n",
    "    print(\"\\nðŸ’¡ TIP: For production deployment, use unified mode\")\n",
    "    print(\"   Benefits: 4x faster, 60% smaller, simpler architecture\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
