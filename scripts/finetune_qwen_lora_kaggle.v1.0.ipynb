{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773cdf35",
   "metadata": {},
   "source": [
    "# LexiLingo: Unified LoRA Adapter Fine-tuning (Kaggle Edition)\n",
    "\n",
    "**Platform:** Kaggle Notebooks (GPU: P100/T4)  \n",
    "**Version:** 1.0.1 - CUDA Error Fixed (Jan 2026)\n",
    "\n",
    "**Mục đích:** Fine-tune Qwen2.5-1.5B-Instruct với **1 unified LoRA adapter** để xử lý đồng thời 4 tasks:\n",
    "1. **Fluency Scoring** (0.0-1.0)\n",
    "2. **Vocabulary Level Classification** (A1, A2, B1, B2, C1, C2)\n",
    "3. **Grammar Error Correction** (GEC)\n",
    "4. **Dialogue Generation** (conversational responses)\n",
    "\n",
    "---\n",
    "\n",
    "## CRITICAL BUG FIX - Version 1.0.1\n",
    "\n",
    "### Issue Resolved:\n",
    "**Error:** `AcceleratorError: CUDA error: an illegal memory access was encountered`\n",
    "\n",
    "**Root Cause:**\n",
    "- PyTorch DataParallel wrapper was being applied to 4-bit quantized model\n",
    "- bitsandbytes quantization is NOT compatible with DataParallel\n",
    "- Multi-GPU operations cause memory access violations with quantized layers\n",
    "\n",
    "**Solution Applied:**\n",
    "1.  Set `CUDA_VISIBLE_DEVICES=0` BEFORE importing torch/transformers\n",
    "2.  Explicit device pinning: `device_map={\"\": 0}` (no \"auto\")\n",
    "3.  Disabled all distributed training parameters\n",
    "4.  Added verification checks to prevent DataParallel wrapping\n",
    "5.  Optimized memory allocation settings\n",
    "\n",
    "**Result:** Stable single-GPU training with 4-bit quantization\n",
    "\n",
    "---\n",
    "\n",
    "## Kaggle-Specific Features\n",
    "\n",
    "### Storage:\n",
    "- **Input datasets:** `/kaggle/input/` (read-only, uploaded as Kaggle dataset)\n",
    "- **Output/Checkpoints:** `/kaggle/working/` (auto-saves as output after session ends)\n",
    "- **Model cache:** `/kaggle/working/.cache/` (Hugging Face models)\n",
    "\n",
    "### Auto-Save Checkpoints:\n",
    "- Every **100 steps** (periodic save)\n",
    "- All saved to `/kaggle/working/unified_model/`\n",
    "- Safe shutdown handling on session interruption\n",
    "\n",
    "### Resume Training:\n",
    "1. Upload previous session's output as input dataset\n",
    "2. Set `resume_from_checkpoint=\"auto\"` (default)\n",
    "3. Training continues from saved state\n",
    "\n",
    "---\n",
    "\n",
    "**GPU Allocation:** Kaggle provides 30hrs/week GPU (P100/T4)  \n",
    "**Expected Training Time:** 6-10 hours (HIGH QUALITY config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a4acc",
   "metadata": {},
   "source": [
    "## 1. Setup Environment & Install Dependencies\n",
    "\n",
    " **CRITICAL FIXES APPLIED** - Version 1.0.1 (Jan 2026)\n",
    "\n",
    "### Fixed Issues:\n",
    "1. **CUDA Illegal Memory Access Error** - RESOLVED \n",
    "   - Root cause: DataParallel wrapper conflicting with 4-bit quantization\n",
    "   - Fix: Force single GPU mode via environment variables BEFORE imports\n",
    "   - Added explicit device pinning (device_map={\"\": 0})\n",
    "\n",
    "2. **Multi-GPU Conflicts** - RESOLVED \n",
    "   - Set CUDA_VISIBLE_DEVICES=0 at notebook start\n",
    "   - Disabled all distributed training parameters\n",
    "   - Added verification checks throughout pipeline\n",
    "\n",
    "3. **Quantization Stability** - ENHANCED \n",
    "   - Optimized BitsAndBytesConfig for single GPU\n",
    "   - Added memory allocator configuration\n",
    "   - Disabled dataloader multi-processing\n",
    "\n",
    "### Must Enable Internet First:\n",
    "\n",
    "Kaggle blocks internet by default. TRL library is REQUIRED but not pre-installed.\n",
    "\n",
    "**How to Enable Internet:**\n",
    "1. Right sidebar -> Settings\n",
    "2. Scroll to Internet section\n",
    "3. Toggle switch to ON (blue)\n",
    "4. Click Save\n",
    "5. Re-run cells below\n",
    "\n",
    "Without internet, this notebook CANNOT run. TRL is required for SFTTrainer.\n",
    "\n",
    "**Expected Training Time:** 6-10 hours on P100/T4 (HIGH QUALITY config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70547099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check internet connectivity\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERNET CONNECTION CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    urllib.request.urlopen('https://pypi.org', timeout=5)\n",
    "    INTERNET_AVAILABLE = True\n",
    "    print(\"\\nInternet: ENABLED\")\n",
    "    print(\"  Ready to install packages from PyPI\")\n",
    "except:\n",
    "    INTERNET_AVAILABLE = False\n",
    "    print(\"\\nInternet: DISABLED\")\n",
    "    print(\"\\n\" + \"!\"*70)\n",
    "    print(\"ERROR: Cannot proceed without internet!\")\n",
    "    print(\"!\"*70)\n",
    "    print(\"\\nTRL library is REQUIRED but not pre-installed on Kaggle.\")\n",
    "    print(\"\\nYou MUST enable internet to continue:\")\n",
    "    print(\"  1. Right sidebar -> Settings \")\n",
    "    print(\"  2. Scroll to 'Internet' section\")\n",
    "    print(\"  3. Toggle to ON (blue)\")\n",
    "    print(\"  4. Click Save\")\n",
    "    print(\"  5. Re-run this cell\")\n",
    "    print(\"\\n\" + \"!\"*70)\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206276a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Set environment variables BEFORE importing torch/transformers\n",
    "# This prevents DataParallel and multi-GPU issues with quantized models\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENVIRONMENT SETUP - SINGLE GPU MODE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Force single GPU execution (prevents DataParallel with quantization)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "\n",
    "# Disable tokenizers parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set memory allocator for better stability\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "print(\"Environment variables set:\")\n",
    "print(f\"  CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'not set')}\")\n",
    "print(f\"  WORLD_SIZE: {os.environ.get('WORLD_SIZE', 'not set')}\")\n",
    "print(f\"  PYTORCH_CUDA_ALLOC_CONF: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF', 'not set')}\")\n",
    "print(\"\\nSingle GPU mode enforced BEFORE library import\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Install packages (REQUIRES INTERNET)\n",
    "if not INTERNET_AVAILABLE:\n",
    "    print(\"\\n\" + \"!\" * 70)\n",
    "    print(\"CANNOT INSTALL PACKAGES - INTERNET IS DISABLED\")\n",
    "    print(\"!\" * 70)\n",
    "    print(\"\\nPlease enable internet in Settings (see instructions above)\")\n",
    "    print(\"\\nNotebook cannot proceed without TRL library.\")\n",
    "    print(\"!\" * 70 + \"\\n\")\n",
    "\n",
    "    print(\"Checking pre-installed packages:\\n\")\n",
    "    try:\n",
    "        import transformers, accelerate, datasets, peft\n",
    "        print(f\"transformers: {transformers.__version__}\")\n",
    "        print(f\"accelerate: {accelerate.__version__}\")\n",
    "        print(f\"datasets: {datasets.__version__}\")\n",
    "        print(f\"peft: {peft.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Some packages missing: {e}\")\n",
    "\n",
    "    try:\n",
    "        import trl\n",
    "        print(f\"trl: {trl.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"trl: NOT INSTALLED (REQUIRED)\")\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STOPPING: Enable internet and re-run from the top\")\n",
    "        print(\"=\" * 70)\n",
    "        raise\n",
    "else:\n",
    "    print(\"Installing required packages...\\n\")\n",
    "    print(\"This may take 2-3 minutes on first run.\\n\")\n",
    "\n",
    "    # Kaggle supports shell-style pip installs.\n",
    "    # NOTE: TensorFlow/XLA cuDNN/cuBLAS log spam can appear in Kaggle sometimes; it is not fatal.\n",
    "    !pip install -q -U \\\n",
    "      transformers>=4.41.0 \\\n",
    "      accelerate>=0.29.0 \\\n",
    "      datasets>=2.18.0 \\\n",
    "      peft>=0.10.0 \\\n",
    "      trl>=0.9.6 \\\n",
    "      bitsandbytes>=0.43.1 \\\n",
    "      sentencepiece\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ALL PACKAGES INSTALLED\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    import transformers, accelerate, datasets, peft, trl\n",
    "    print(\"\\nInstalled versions:\")\n",
    "    print(f\"  transformers: {transformers.__version__}\")\n",
    "    print(f\"  accelerate: {accelerate.__version__}\")\n",
    "    print(f\"  datasets: {datasets.__version__}\")\n",
    "    print(f\"  peft: {peft.__version__}\")\n",
    "    print(f\"  trl: {trl.__version__}\")\n",
    "    print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Kaggle environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KAGGLE ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check paths\n",
    "kaggle_working = Path(\"/kaggle/working\")\n",
    "kaggle_input = Path(\"/kaggle/input\")\n",
    "\n",
    "print(f\"\\nWorking directory: {kaggle_working}\")\n",
    "print(f\"  Exists: {kaggle_working.exists()}\")\n",
    "print(f\"  Writable: {os.access(kaggle_working, os.W_OK)}\")\n",
    "\n",
    "print(f\"\\nInput directory: {kaggle_input}\")\n",
    "print(f\"  Exists: {kaggle_input.exists()}\")\n",
    "\n",
    "if kaggle_input.exists():\n",
    "    datasets = list(kaggle_input.iterdir())\n",
    "    print(f\"  Datasets found: {len(datasets)}\")\n",
    "    for ds in datasets:\n",
    "        print(f\"    - {ds.name}\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4981d82",
   "metadata": {},
   "source": [
    "## 2. Configuration - Kaggle Optimized Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da483db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CONFIGURATION - Set this BEFORE running\n",
    "# ============================================================================\n",
    "# Option 1: Use real dataset (RECOMMENDED for production)\n",
    "USE_TEST_DATA = False  # Set to True to use synthetic test data for debugging\n",
    "\n",
    "# Option 2: Custom dataset path (if auto-detection fails)\n",
    "CUSTOM_DATASET_PATH = None  # Example: \"/kaggle/input/your-dataset-name\"\n",
    "# ============================================================================\n",
    "\n",
    "# Verify CUDA setup AFTER imports\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CUDA VERIFICATION (Post-Import)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCUDA Available: Yes\")\n",
    "    print(f\"  Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"  Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Device capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Verify environment variables are still set\n",
    "    print(f\"\\nEnvironment check:\")\n",
    "    print(f\"  CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'NOT SET')}\")\n",
    "    print(f\"  WORLD_SIZE: {os.environ.get('WORLD_SIZE', 'NOT SET')}\")\n",
    "    \n",
    "    if torch.cuda.device_count() != 1:\n",
    "        print(f\"\\nWARNING: {torch.cuda.device_count()} devices visible!\")\n",
    "        print(\"  This may cause DataParallel issues with quantization\")\n",
    "else:\n",
    "    print(\"CUDA not available - will use CPU (slow)\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# KAGGLE PATHS - Tự động lưu output\n",
    "KAGGLE_WORKING = Path(\"/kaggle/working\")\n",
    "KAGGLE_INPUT = Path(\"/kaggle/input\")\n",
    "\n",
    "# Output directory - sẽ được tự động save làm output của Kaggle session\n",
    "OUTPUT_DIR = KAGGLE_WORKING / \"unified_model\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cache directory cho Hugging Face models\n",
    "CACHE_DIR = KAGGLE_WORKING / \".cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KAGGLE STORAGE CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Checkpoints will be saved here\")\n",
    "print(f\"  Auto-uploaded as Kaggle output after session\")\n",
    "print(f\"\\nCache directory: {CACHE_DIR}\")\n",
    "print(f\"  Hugging Face models cached here\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Dataset Configuration\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if user wants to use test data\n",
    "if USE_TEST_DATA:\n",
    "    print(\"\\nMode: TEST DATA (synthetic)\")\n",
    "    print(\"WARNING: This is for debugging only, not for production training!\")\n",
    "    DATASET_PATH = None  # Will trigger test data creation\n",
    "else:\n",
    "    print(\"\\nMode: REAL DATA (production)\")\n",
    "    \n",
    "    # Use custom path if provided\n",
    "    if CUSTOM_DATASET_PATH:\n",
    "        DATASET_PATH = Path(CUSTOM_DATASET_PATH)\n",
    "        print(f\"Using custom path: {DATASET_PATH}\")\n",
    "    else:\n",
    "        # Auto-detect dataset path\n",
    "        DATASET_PATH = None\n",
    "        if KAGGLE_INPUT.exists():\n",
    "            # Look for common dataset names\n",
    "            possible_names = [\n",
    "                \"lexilingo-training-data\",\n",
    "                \"unified-training-data\", \n",
    "                \"training-data\",\n",
    "                \"lexilingo-dataset\",\n",
    "            ]\n",
    "            \n",
    "            for name in possible_names:\n",
    "                candidate = KAGGLE_INPUT / name\n",
    "                if candidate.exists():\n",
    "                    # Check if it has train.jsonl\n",
    "                    if (candidate / \"train.jsonl\").exists():\n",
    "                        DATASET_PATH = candidate\n",
    "                        print(f\"\\nAuto-detected dataset: {DATASET_PATH.name}\")\n",
    "                        break\n",
    "                    # Check subdirectories\n",
    "                    for subdir in candidate.iterdir():\n",
    "                        if subdir.is_dir() and (subdir / \"train.jsonl\").exists():\n",
    "                            DATASET_PATH = subdir\n",
    "                            print(f\"\\nAuto-detected dataset: {DATASET_PATH}\")\n",
    "                            break\n",
    "            \n",
    "            if DATASET_PATH is None:\n",
    "                # List all available datasets\n",
    "                datasets = list(KAGGLE_INPUT.iterdir())\n",
    "                if datasets:\n",
    "                    print(f\"\\nNo dataset auto-detected\")\n",
    "                    print(f\"\\nAvailable datasets ({len(datasets)}):\")\n",
    "                    for i, ds in enumerate(datasets, 1):\n",
    "                        print(f\"  {i}. {ds.name}\")\n",
    "                        # Check for JSONL files\n",
    "                        jsonl_files = list(ds.rglob(\"*.jsonl\"))\n",
    "                        if jsonl_files:\n",
    "                            print(f\"     Found {len(jsonl_files)} .jsonl files:\")\n",
    "                            for jf in jsonl_files[:5]:\n",
    "                                print(f\"       - {jf.relative_to(ds)}\")\n",
    "                    \n",
    "                    print(f\"\\nTo use a dataset, set CUSTOM_DATASET_PATH at the top of this cell\")\n",
    "                else:\n",
    "                    print(\"\\nNo datasets found in /kaggle/input/\")\n",
    "        else:\n",
    "            print(\"\\nNot running on Kaggle - using local paths\")\n",
    "            # For local testing, use project datasets\n",
    "            local_dataset = Path(\"../scripts/downloaded_datasets\")\n",
    "            if local_dataset.exists():\n",
    "                DATASET_PATH = local_dataset\n",
    "                print(f\"Using local dataset: {DATASET_PATH}\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f79e9",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import signal\n",
    "import atexit\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Transformers & PEFT\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e85f05",
   "metadata": {},
   "source": [
    "## 4. Checkpoint Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    \"\"\"Quản lý checkpoint tự động cho Kaggle\"\"\"\n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.state_file = self.output_dir / \"training_state.json\"\n",
    "        print(f\"CheckpointManager initialized: {self.output_dir}\")\n",
    "    \n",
    "    def find_latest_checkpoint(self):\n",
    "        \"\"\"Tìm checkpoint mới nhất\"\"\"\n",
    "        checkpoints = sorted(\n",
    "            [d for d in self.output_dir.glob(\"checkpoint-*\") if d.is_dir()],\n",
    "            key=lambda x: int(x.name.split(\"-\")[-1])\n",
    "        )\n",
    "        return str(checkpoints[-1]) if checkpoints else None\n",
    "    \n",
    "    def list_all_checkpoints(self):\n",
    "        \"\"\"Liệt kê tất cả checkpoints\"\"\"\n",
    "        checkpoints = sorted(\n",
    "            [d for d in self.output_dir.glob(\"checkpoint-*\") if d.is_dir()],\n",
    "            key=lambda x: int(x.name.split(\"-\")[-1])\n",
    "        )\n",
    "        return [{\"path\": str(cp), \"step\": int(cp.name.split(\"-\")[-1])} for cp in checkpoints]\n",
    "    \n",
    "    def save_training_state(self, **kwargs):\n",
    "        \"\"\"Lưu training state\"\"\"\n",
    "        state = {\n",
    "            \"last_update\": datetime.now().isoformat(),\n",
    "            \"platform\": \"kaggle\",\n",
    "            **kwargs\n",
    "        }\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "        print(f\"Saved training state: {self.state_file}\")\n",
    "    \n",
    "    def load_training_state(self):\n",
    "        \"\"\"Load training state\"\"\"\n",
    "        if self.state_file.exists():\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def print_status(self):\n",
    "        \"\"\"In status checkpoint\"\"\"\n",
    "        checkpoints = self.list_all_checkpoints()\n",
    "        state = self.load_training_state()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CHECKPOINT STATUS (Kaggle)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if checkpoints:\n",
    "            print(f\"Found {len(checkpoints)} checkpoint(s)\")\n",
    "            print(f\"\\nLatest: {checkpoints[-1]['path']}\")\n",
    "            \n",
    "            if state:\n",
    "                print(f\"\\nTraining State:\")\n",
    "                for key, value in state.items():\n",
    "                    print(f\"  - {key}: {value}\")\n",
    "        else:\n",
    "            print(\"No checkpoints found - training from scratch\")\n",
    "        \n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_mgr = CheckpointManager(OUTPUT_DIR)\n",
    "checkpoint_mgr.print_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bda2a",
   "metadata": {},
   "source": [
    "## 5. Graceful Shutdown Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180da86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GracefulShutdownHandler:\n",
    "    \"\"\"Tự động lưu checkpoint khi session bị interrupt\"\"\"\n",
    "    def __init__(self):\n",
    "        self.trainer = None\n",
    "        self.model = None\n",
    "        self.emergency_save_path = OUTPUT_DIR / \"emergency_checkpoint\"\n",
    "        \n",
    "        # Register handlers\n",
    "        signal.signal(signal.SIGINT, self._signal_handler)\n",
    "        signal.signal(signal.SIGTERM, self._signal_handler)\n",
    "        atexit.register(self._emergency_save)\n",
    "        print(\"GracefulShutdownHandler activated\")\n",
    "    \n",
    "    def register_trainer(self, trainer, model):\n",
    "        \"\"\"Register trainer để có thể save khi cần\"\"\"\n",
    "        self.trainer = trainer\n",
    "        self.model = model\n",
    "    \n",
    "    def _signal_handler(self, signum, frame):\n",
    "        \"\"\"Xử lý SIGINT/SIGTERM\"\"\"\n",
    "        print(f\"\\n\\nReceived signal {signum} - saving checkpoint...\")\n",
    "        if self.trainer is not None:\n",
    "            try:\n",
    "                self.trainer.save_model(str(self.emergency_save_path))\n",
    "                print(f\"Emergency checkpoint saved: {self.emergency_save_path}\")\n",
    "            except:\n",
    "                pass\n",
    "        exit(0)\n",
    "    \n",
    "    def _emergency_save(self):\n",
    "        \"\"\"Save khi exit bất thường\"\"\"\n",
    "        if self.trainer is not None and self.model is not None:\n",
    "            if not self.emergency_save_path.exists():\n",
    "                print(\"\\nEmergency exit - saving final checkpoint...\")\n",
    "                try:\n",
    "                    self.emergency_save_path.mkdir(parents=True, exist_ok=True)\n",
    "                    self.model.save_pretrained(str(self.emergency_save_path))\n",
    "                    print(f\"Final checkpoint saved: {self.emergency_save_path}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# Initialize shutdown handler\n",
    "shutdown_handler = GracefulShutdownHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16fba9",
   "metadata": {},
   "source": [
    "## 6. Load Dataset\n",
    "\n",
    "### Dataset Preparation Guide\n",
    "\n",
    "**IMPORTANT: You must have a dataset before training!**\n",
    "\n",
    "---\n",
    "\n",
    "### OPTION 1: Use Test Data (Quick Start - Debugging Only)\n",
    "\n",
    "**Fastest way to test the pipeline:**\n",
    "\n",
    "1. Scroll up to **Cell 7** (Configuration - Kaggle Optimized Paths)\n",
    "2. Find the line: `USE_TEST_DATA = False`\n",
    "3. Change to: `USE_TEST_DATA = True`\n",
    "4. Re-run Cell 7\n",
    "5. Run this cell (Cell 15)\n",
    "\n",
    "**WARNING:** Test data is synthetic and only for debugging. Not suitable for production!\n",
    "\n",
    "---\n",
    "\n",
    "### OPTION 2: Upload Real Dataset (Recommended for Production)\n",
    "\n",
    "**Required Format:** JSONL files with `input` and `output` fields\n",
    "\n",
    "Example lines in train.jsonl / val.jsonl:\n",
    "```json\n",
    "{\"input\": \"Rate fluency: The cat sat on the mat.\", \"output\": \"Fluency Score: 0.85\"}\n",
    "{\"input\": \"Classify level: I am happy today.\", \"output\": \"Vocabulary Level: A1\"}\n",
    "{\"input\": \"Correct: He go to school yesterday.\", \"output\": \"Corrected: He went to school yesterday.\"}\n",
    "{\"input\": \"User: What is your name?\", \"output\": \"Assistant: I am LexiLingo, an AI assistant.\"}\n",
    "```\n",
    "\n",
    "**Steps to Upload:**\n",
    "\n",
    "1. **Create Dataset on Kaggle:**\n",
    "   - Go to [kaggle.com/datasets](https://www.kaggle.com/datasets)\n",
    "   - Click \"New Dataset\"\n",
    "   - Upload `train.jsonl` and `val.jsonl`\n",
    "   - Name it (e.g., \"lexilingo-training-data\")\n",
    "   - Click \"Create\"\n",
    "\n",
    "2. **Add to This Notebook:**\n",
    "   - In this notebook: Right sidebar -> Settings (gear icon)\n",
    "   - Scroll to \"Data\" section  \n",
    "   - Click \"+ Add Data\"\n",
    "   - Search your dataset name\n",
    "   - Click \"Add\"\n",
    "\n",
    "3. **Run This Cell:**\n",
    "   - Notebook will auto-detect your dataset\n",
    "   - Check output to confirm successful loading\n",
    "\n",
    "---\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**If auto-detection fails:**\n",
    "- Set `CUSTOM_DATASET_PATH` in Cell 7\n",
    "- Example: `CUSTOM_DATASET_PATH = \"/kaggle/input/your-dataset-name\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a793386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset(num_train=100, num_val=20):\n",
    "    \"\"\"Create a small test dataset for debugging/testing\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING TEST DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nWARNING: Using synthetic test data for demonstration\")\n",
    "    print(\"For real training, upload your actual dataset to Kaggle\\n\")\n",
    "    \n",
    "    # Sample data for 4 tasks\n",
    "    test_data = {\n",
    "        \"fluency\": [\n",
    "            {\"input\": \"Rate fluency: The cat sits on mat.\", \"output\": \"Fluency Score: 0.85\"},\n",
    "            {\"input\": \"Rate fluency: Me go store buy things.\", \"output\": \"Fluency Score: 0.45\"},\n",
    "        ],\n",
    "        \"vocabulary\": [\n",
    "            {\"input\": \"Classify level: I am happy.\", \"output\": \"Vocabulary Level: A1\"},\n",
    "            {\"input\": \"Classify level: The implementation demonstrates sophisticated algorithms.\", \"output\": \"Vocabulary Level: C2\"},\n",
    "        ],\n",
    "        \"grammar\": [\n",
    "            {\"input\": \"Correct: He go to school yesterday.\", \"output\": \"Corrected: He went to school yesterday.\"},\n",
    "            {\"input\": \"Correct: She have been working here since 2020.\", \"output\": \"Corrected: She has been working here since 2020.\"},\n",
    "        ],\n",
    "        \"dialogue\": [\n",
    "            {\"input\": \"User: What's the weather like?\", \"output\": \"Assistant: I'd be happy to help, but I don't have access to real-time weather data. Please check a weather service.\"},\n",
    "            {\"input\": \"User: How are you?\", \"output\": \"Assistant: I'm functioning well, thank you for asking! How can I assist you today?\"},\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    # Generate training data\n",
    "    train_samples = []\n",
    "    for _ in range(num_train):\n",
    "        for task, examples in test_data.items():\n",
    "            train_samples.append(examples[_ % len(examples)])\n",
    "    \n",
    "    # Generate validation data  \n",
    "    val_samples = []\n",
    "    for _ in range(num_val):\n",
    "        for task, examples in test_data.items():\n",
    "            val_samples.append(examples[_ % len(examples)])\n",
    "    \n",
    "    train_dataset = Dataset.from_list(train_samples)\n",
    "    val_dataset = Dataset.from_list(val_samples)\n",
    "    \n",
    "    print(f\"Test dataset created:\")\n",
    "    print(f\"  Train: {len(train_dataset)} samples\")\n",
    "    print(f\"  Val: {len(val_dataset)} samples\")\n",
    "    print(f\"\\nSample entry:\")\n",
    "    print(f\"  Input: {train_samples[0]['input']}\")\n",
    "    print(f\"  Output: {train_samples[0]['output']}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def load_kaggle_dataset(dataset_path: Path):\n",
    "    \"\"\"Load dataset từ Kaggle input\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Try to find JSONL files\n",
    "    train_file = dataset_path / \"train.jsonl\"\n",
    "    val_file = dataset_path / \"val.jsonl\"\n",
    "    \n",
    "    if not train_file.exists() or not val_file.exists():\n",
    "        # Try alternative paths\n",
    "        for possible_path in dataset_path.rglob(\"train.jsonl\"):\n",
    "            train_file = possible_path\n",
    "            val_file = possible_path.parent / \"val.jsonl\"\n",
    "            break\n",
    "    \n",
    "    if not train_file.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cannot find train.jsonl in {dataset_path}\\n\"\n",
    "            \"Please upload your dataset to Kaggle and update DATASET_PATH\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nLoading from:\")\n",
    "    print(f\"  Train: {train_file}\")\n",
    "    print(f\"  Val: {val_file}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            train_data.append(json.loads(line.strip()))\n",
    "    \n",
    "    with open(val_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            val_data.append(json.loads(line.strip()))\n",
    "    \n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    print(f\"\\nDataset loaded:\")\n",
    "    print(f\"  Train: {len(train_dataset)} samples\")\n",
    "    print(f\"  Val: {len(val_dataset)} samples\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATASET\n",
    "# ============================================================================\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "\n",
    "# Check if user enabled test data mode (set at top of config cell)\n",
    "if USE_TEST_DATA:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"USING TEST DATA MODE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Generating synthetic test dataset for debugging...\")\n",
    "    train_dataset, val_dataset = create_test_dataset(num_train=200, num_val=40)\n",
    "    print(\"WARNING: This is synthetic data - not suitable for production training!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Otherwise try to load real dataset\n",
    "elif DATASET_PATH is not None:\n",
    "    try:\n",
    "        train_dataset, val_dataset = load_kaggle_dataset(DATASET_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR loading dataset: {e}\\n\")\n",
    "        train_dataset = None\n",
    "        val_dataset = None\n",
    "\n",
    "# If no dataset available, show instructions\n",
    "if train_dataset is None or val_dataset is None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NO DATASET AVAILABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nYou have 2 options:\")\n",
    "    print()\n",
    "    print(\"OPTION 1: Use real dataset (RECOMMENDED)\")\n",
    "    print(\"  1. Go to Kaggle.com -> Datasets -> New Dataset\")\n",
    "    print(\"  2. Upload your train.jsonl and val.jsonl files\")\n",
    "    print(\"  3. Add dataset to this notebook (Settings -> Data)\")\n",
    "    print(\"  4. Re-run this cell\")\n",
    "    print()\n",
    "    print(\"OPTION 2: Use test data for debugging\")\n",
    "    print(\"  1. Go to the Configuration cell (Cell 7)\")\n",
    "    print(\"  2. Find the line: USE_TEST_DATA = False\")\n",
    "    print(\"  3. Change it to: USE_TEST_DATA = True\")\n",
    "    print(\"  4. Re-run Configuration cell and this cell\")\n",
    "    print(\"  WARNING: Test data is synthetic - only for debugging!\")\n",
    "    print()\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842946d",
   "metadata": {},
   "source": [
    "## 7. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ee8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIGH QUALITY config (stable on Kaggle P100/T4)\n",
    "# Uses FP16 on pre-Ampere GPUs to avoid bitsandbytes crashes; BF16 only on Ampere+\n",
    "\n",
    "# Verify single-GPU mode and configure matmul kernels\n",
    "if torch.cuda.is_available():\n",
    "    # Verify environment is correctly set\n",
    "    torch.cuda.set_device(0)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CUDA CONFIGURATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Single GPU mode active (device 0)\")\n",
    "    print(f\"Available devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def _detect_compute_dtype():\n",
    "    if not torch.cuda.is_available():\n",
    "        return torch.float32, False\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    supports_bf16 = major >= 8  # Ampere (8.x) or newer\n",
    "    return (torch.bfloat16 if supports_bf16 else torch.float16), supports_bf16\n",
    "\n",
    "COMPUTE_DTYPE, USE_BF16 = _detect_compute_dtype()\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Full quality model\n",
    "\n",
    "# Quantization config (4-bit) - optimized for single GPU\n",
    "QUANTIZATION_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=COMPUTE_DTYPE,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    # Additional stability settings\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    ")\n",
    "\n",
    "# Unified LoRA configuration - HIGH QUALITY (balanced)\n",
    "UNIFIED_LORA_CONFIG = {\n",
    "    \"task_type\": TaskType.CAUSAL_LM,\n",
    "    \"r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    \"inference_mode\": False,\n",
    "}\n",
    "\n",
    "# Training configuration - stability first with EXPLICIT single-GPU settings\n",
    "if torch.cuda.is_available():\n",
    "    TRAINING_CONFIG = {\n",
    "        \"output_dir\": str(OUTPUT_DIR),\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"per_device_train_batch_size\": 1,\n",
    "        \"per_device_eval_batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 24,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"logging_steps\": 10,\n",
    "        \"eval_strategy\": \"steps\",\n",
    "        \"eval_steps\": 100,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 100,\n",
    "        \"save_total_limit\": 3,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"eval_loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"fp16\": (not USE_BF16),\n",
    "        \"bf16\": USE_BF16,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"optim\": \"adamw_8bit\",\n",
    "        \"report_to\": \"none\",\n",
    "        \"seed\": 42,\n",
    "        # CRITICAL: Disable all distributed/parallel training\n",
    "        \"ddp_find_unused_parameters\": False,\n",
    "        \"dataloader_pin_memory\": False,  # Reduces memory pressure\n",
    "        \"dataloader_num_workers\": 0,  # Single-threaded data loading for stability\n",
    "        \"local_rank\": -1,  # Disable distributed training\n",
    "        \"no_cuda\": False,\n",
    "    }\n",
    "else:\n",
    "    # CPU fallback\n",
    "    TRAINING_CONFIG = {\n",
    "        \"output_dir\": str(OUTPUT_DIR),\n",
    "        \"num_train_epochs\": 2,\n",
    "        \"per_device_train_batch_size\": 1,\n",
    "        \"per_device_eval_batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 24,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"logging_steps\": 10,\n",
    "        \"eval_strategy\": \"steps\",\n",
    "        \"eval_steps\": 100,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 100,\n",
    "        \"save_total_limit\": 2,\n",
    "        \"report_to\": \"none\",\n",
    "        \"seed\": 42,\n",
    "        \"dataloader_num_workers\": 0,\n",
    "    }\n",
    "\n",
    "gpu_info = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HIGH QUALITY MODEL CONFIGURATION - STABLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"GPU: {gpu_info}\")\n",
    "print(f\"Compute dtype: {COMPUTE_DTYPE}\")\n",
    "print(f\"bf16 enabled: {USE_BF16}\")\n",
    "print(f\"Batch size: {TRAINING_CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"Grad accumulation: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"Effective batch: {TRAINING_CONFIG['per_device_train_batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"Epochs: {TRAINING_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"\\nSINGLE GPU MODE ENFORCED - No DataParallel\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance estimator for HIGH QUALITY config\n",
    "def estimate_training_time():\n",
    "    \"\"\"Estimate training time for current config\"\"\"\n",
    "    \n",
    "    # Model parameters\n",
    "    model_size = 1.5e9  # 1.5B params\n",
    "    lora_rank = UNIFIED_LORA_CONFIG['r']\n",
    "    \n",
    "    # Training params\n",
    "    train_size = len(train_dataset) if train_dataset else 10000\n",
    "    batch_size = TRAINING_CONFIG['per_device_train_batch_size']\n",
    "    grad_accum = TRAINING_CONFIG['gradient_accumulation_steps']\n",
    "    epochs = TRAINING_CONFIG['num_train_epochs']\n",
    "    \n",
    "    # Calculate steps\n",
    "    effective_batch = batch_size * grad_accum\n",
    "    steps_per_epoch = train_size // effective_batch\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    # Estimate time per step (seconds)\n",
    "    # 1.5B model with LoRA r=32 on P100/T4\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        if 'P100' in gpu_name:\n",
    "            time_per_step = 3.5  # seconds\n",
    "            gpu_type = \"P100\"\n",
    "        elif 'T4' in gpu_name:\n",
    "            time_per_step = 4.5  # seconds\n",
    "            gpu_type = \"T4\"\n",
    "        else:\n",
    "            time_per_step = 5.0  # conservative estimate\n",
    "            gpu_type = \"Unknown GPU\"\n",
    "    else:\n",
    "        time_per_step = 60  # CPU is very slow\n",
    "        gpu_type = \"CPU\"\n",
    "    \n",
    "    total_time_sec = total_steps * time_per_step\n",
    "    total_time_hours = total_time_sec / 3600\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING TIME ESTIMATION - HIGH QUALITY CONFIG\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nDataset & Configuration:\")\n",
    "    print(f\"  Training samples: {train_size:,}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Gradient accumulation: {grad_accum}\")\n",
    "    print(f\"  Effective batch size: {effective_batch}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"\\nTraining Steps:\")\n",
    "    print(f\"  Steps per epoch: {steps_per_epoch:,}\")\n",
    "    print(f\"  Total steps: {total_steps:,}\")\n",
    "    print(f\"\\nHardware:\")\n",
    "    print(f\"  Device: {gpu_type}\")\n",
    "    print(f\"  Estimated time per step: {time_per_step:.2f}s\")\n",
    "    print(f\"\\nEstimated Total Time:\")\n",
    "    print(f\"  {total_time_hours:.1f} hours ({total_time_hours/24:.1f} days)\")\n",
    "    print(f\"\\nCheckpoints:\")\n",
    "    print(f\"  Saved every {TRAINING_CONFIG['save_steps']} steps\")\n",
    "    print(f\"  Total checkpoints: ~{total_steps // TRAINING_CONFIG['save_steps']}\")\n",
    "    print(f\"  Keep best {TRAINING_CONFIG['save_total_limit']} checkpoints\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Quality notes\n",
    "    print(\"QUALITY vs SPEED COMPARISON:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Config          | Model  | Rank | Time   | Quality\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"FAST (0.5B)     | 0.5B   | 16   | ~3-4h  | Good\")\n",
    "    print(\"HIGH QUALITY    | 1.5B   | 32   | ~6-10h | Excellent (CURRENT)\")\n",
    "    print(\"MAX QUALITY     | 1.5B   | 48   | ~8-12h | Best\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"\\nCurrent config: Balanced HIGH QUALITY for production use\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run estimator if dataset is loaded\n",
    "if 'train_dataset' in globals() and train_dataset is not None:\n",
    "    estimate_training_time()\n",
    "else:\n",
    "    print(\"\\nTraining estimator will run after dataset is loaded\")\n",
    "    print(\"Expected training time: 6-10 hours on P100/T4\")\n",
    "    print(\"This is HIGH QUALITY configuration for production deployment\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303f2da",
   "metadata": {},
   "source": [
    "## Configuration Notes - HIGH QUALITY\n",
    "\n",
    "### Current Configuration Summary\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| Model | Qwen2.5-1.5B | Full quality, 3x larger than 0.5B |\n",
    "| LoRA rank | 32 | Balanced (not too low, not too high) |\n",
    "| LoRA alpha | 64 | 2x rank ratio |\n",
    "| Dropout | 0.05 | Lower for production quality |\n",
    "| Batch size | 2 | Fits in 15-16GB GPU |\n",
    "| Grad accum | 12 | Effective batch = 24 |\n",
    "| Epochs | 5 | Better convergence |\n",
    "| Learning rate | 2e-4 | Stable training |\n",
    "| Training time (P100) | ~6-8h | High quality |\n",
    "| Training time (T4) | ~8-10h | High quality |\n",
    "| GPU Memory | ~8-10 GB | 4-bit quantization |\n",
    "\n",
    "### Why This Configuration?\n",
    "\n",
    "1. **Model Size (1.5B)**\n",
    "   - 3x more parameters than 0.5B\n",
    "   - Significantly better quality\n",
    "   - Still fits in Kaggle GPU with 4-bit quantization\n",
    "   - Production-ready accuracy\n",
    "\n",
    "2. **LoRA Rank 32**\n",
    "   - Sweet spot between quality and efficiency\n",
    "   - Better than r=16 (fast config)\n",
    "   - More efficient than r=48 (max config)\n",
    "   - Proven optimal for fine-tuning\n",
    "\n",
    "3. **Training Strategy**\n",
    "   - 5 epochs for good convergence\n",
    "   - Lower learning rate (2e-4) for stability\n",
    "   - More frequent checkpoints (every 100 steps)\n",
    "   - Keep 3 best checkpoints\n",
    "\n",
    "4. **Quality vs Speed**\n",
    "   - 2-3x slower than fast config\n",
    "   - ~15-20% better accuracy\n",
    "   - Better response quality\n",
    "   - Suitable for production\n",
    "\n",
    "### Kaggle GPU Requirements\n",
    "\n",
    "**P100 (16GB) - Recommended:**\n",
    "- Fits comfortably with 4-bit quantization\n",
    "- Training time: ~6-8h\n",
    "- Can handle batch_size=2 easily\n",
    "\n",
    "**T4 (15GB) - Works well:**\n",
    "- Slightly tighter memory\n",
    "- Training time: ~8-10h\n",
    "- Keep batch_size=2 (stable)\n",
    "\n",
    "**Memory Usage Breakdown:**\n",
    "- Base model (4-bit): ~3-4 GB\n",
    "- LoRA adapters: ~1-2 GB\n",
    "- Optimizer states: ~2-3 GB\n",
    "- Activations: ~2-3 GB\n",
    "- Total: ~8-10 GB\n",
    "\n",
    "### Quality Comparison\n",
    "\n",
    "**Fast Config (0.5B, r=16):**\n",
    "- Training: 3-4h\n",
    "- Quality: Good for development\n",
    "- Use case: Quick iterations, testing\n",
    "\n",
    "**High Quality Config (1.5B, r=32) - CURRENT:**\n",
    "- Training: 6-10h\n",
    "- Quality: Excellent for production\n",
    "- Use case: Final deployment, customer-facing\n",
    "\n",
    "**Max Quality Config (1.5B, r=48):**\n",
    "- Training: 8-12h\n",
    "- Quality: Best possible\n",
    "- Use case: Research, maximum accuracy needed\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "This HIGH QUALITY config is optimized for:\n",
    "- Real-world applications\n",
    "- Customer-facing products\n",
    "- Production environments\n",
    "- Balance of quality and efficiency\n",
    "\n",
    "The model will be:\n",
    "- More accurate in fluency scoring\n",
    "- Better at vocabulary classification\n",
    "- More reliable in grammar correction\n",
    "- More natural in dialogue generation\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1. **Enable Internet** in Kaggle Settings (required for TRL)\n",
    "2. **Upload dataset** as Kaggle dataset\n",
    "3. **Monitor training** - checkpoints saved every 100 steps\n",
    "4. **Resume capability** - can continue from any checkpoint\n",
    "5. **Keep best model** - automatically saves 3 best checkpoints\n",
    "\n",
    "### Estimated Results\n",
    "\n",
    "Based on 1.5B model with r=32:\n",
    "- Fluency scoring: ~85-90% accuracy\n",
    "- Vocabulary classification: ~80-85% accuracy\n",
    "- Grammar correction: ~75-80% GLEU score\n",
    "- Dialogue quality: Excellent coherence\n",
    "\n",
    "These are production-ready metrics suitable for real applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe439e",
   "metadata": {},
   "source": [
    "## 8. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e25922",
   "metadata": {},
   "source": [
    "## CRITICAL FIX: Vocab Size & Special Tokens\n",
    "\n",
    "Problem Solved: CUDA device-side assert caused by special tokens exceeding vocabulary size.\n",
    "\n",
    "### Root Cause:\n",
    "- Tokenizer vocab_size: 151643\n",
    "- Model vocab_size: 151936\n",
    "- Special tokens: pad_token_id=151643, eos_token_id=151645\n",
    "- Special tokens were OUTSIDE valid range [0, 151642]\n",
    "\n",
    "### Solution Applied:\n",
    "The code now automatically:\n",
    "1. Detects the maximum special token ID\n",
    "2. Calculates required vocab size: max(tokenizer_vocab, model_vocab, max_special_token + 1)\n",
    "3. Resizes model embeddings to accommodate ALL tokens\n",
    "4. Validates all special tokens are within range\n",
    "\n",
    "### Result:\n",
    "- Model will resize to 151646 (includes eos_token_id=151645)\n",
    "- All token IDs now within valid range\n",
    "- No more CUDA errors during generation\n",
    "\n",
    "After restart kernel, just run cells in order - the fix will apply automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Cache: {CACHE_DIR}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=str(CACHE_DIR),\n",
    ")\n",
    "\n",
    "# Ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"\\nTokenizer loaded:\")\n",
    "print(f\"  vocab_size: {tokenizer.vocab_size}\")\n",
    "print(f\"  pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"  eos_token_id: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# CRITICAL: For 4-bit quantization, MUST use explicit device pinning (no 'auto')\n",
    "# This prevents DataParallel from being used which causes CUDA errors with quantization\n",
    "if torch.cuda.is_available():\n",
    "    device_map = {\"\": 0}  # Pin to GPU 0 explicitly\n",
    "    print(f\"\\nDevice map: {device_map} (pinned to GPU 0)\")\n",
    "else:\n",
    "    device_map = {\"\": \"cpu\"}\n",
    "    print(f\"\\nDevice map: {device_map}\")\n",
    "\n",
    "# Load base model (4-bit). Keep torch_dtype consistent with bitsandbytes compute dtype.\n",
    "print(\"\\nLoading model with 4-bit quantization...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=QUANTIZATION_CONFIG,\n",
    "    device_map=device_map,  # Explicit device pinning\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=str(CACHE_DIR),\n",
    "    torch_dtype=COMPUTE_DTYPE if torch.cuda.is_available() else None,\n",
    "    # CRITICAL: Prevent any multi-GPU operations\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(\"\\nModel loaded:\")\n",
    "print(f\"  model vocab_size: {base_model.config.vocab_size}\")\n",
    "print(f\"  embedding rows: {base_model.get_input_embeddings().weight.shape[0]}\")\n",
    "print(f\"  device: {next(base_model.parameters()).device}\")\n",
    "\n",
    "# CRITICAL FIX: resize embeddings to include ALL special token IDs\n",
    "special_token_ids = [\n",
    "    tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0,\n",
    "    tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 0,\n",
    "    tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0,\n",
    "]\n",
    "max_special_token_id = max(special_token_ids)\n",
    "required_vocab_size = max(tokenizer.vocab_size, base_model.config.vocab_size, max_special_token_id + 1)\n",
    "\n",
    "print(\"\\nVocab alignment:\")\n",
    "print(f\"  max_special_token_id: {max_special_token_id}\")\n",
    "print(f\"  required_vocab_size: {required_vocab_size}\")\n",
    "\n",
    "if base_model.config.vocab_size != required_vocab_size:\n",
    "    print(f\"Resizing embeddings: {base_model.config.vocab_size} -> {required_vocab_size}\")\n",
    "    base_model.resize_token_embeddings(required_vocab_size)\n",
    "    base_model.config.vocab_size = required_vocab_size\n",
    "\n",
    "# Final validation\n",
    "if tokenizer.pad_token_id >= base_model.config.vocab_size or tokenizer.eos_token_id >= base_model.config.vocab_size:\n",
    "    raise ValueError(\n",
    "        \"Special tokens out of range after resize. \"\n",
    "        f\"pad_token_id={tokenizer.pad_token_id}, eos_token_id={tokenizer.eos_token_id}, model_vocab={base_model.config.vocab_size}\"\n",
    "    )\n",
    "\n",
    "# Gradient checkpointing + required grads for 4-bit LoRA\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "base_model.enable_input_require_grads()\n",
    "\n",
    "print(\"\\nModel configured:\")\n",
    "print(f\"  device: {next(base_model.parameters()).device}\")\n",
    "print(f\"  compute dtype: {COMPUTE_DTYPE}\")\n",
    "print(f\"  model vocab_size: {base_model.config.vocab_size}\")\n",
    "print(f\"  tokenizer vocab_size: {tokenizer.vocab_size}\")\n",
    "print(f\"  gradient checkpointing: enabled\")\n",
    "print(f\"  use_cache: {base_model.config.use_cache}\")\n",
    "\n",
    "# Verify model is NOT wrapped in DataParallel\n",
    "if hasattr(base_model, 'module'):\n",
    "    raise RuntimeError(\"Model is wrapped in DataParallel/DDP - this will cause errors with quantization!\")\n",
    "    \n",
    "print(\"\\nModel verified: Single GPU mode, no DataParallel wrapper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef8bff",
   "metadata": {},
   "source": [
    "## 9. Apply LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA config\n",
    "lora_config = LoraConfig(**UNIFIED_LORA_CONFIG)\n",
    "\n",
    "# Apply LoRA to base model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Ensure model is in training mode\n",
    "model.train()\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LoRA ADAPTER APPLIED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTrainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Memory reduction: ~{100 * (1 - trainable_params / total_params):.1f}%\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122dd345",
   "metadata": {},
   "source": [
    "## 10. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_unified_adapter(\n",
    "    train_dataset: Dataset,\n",
    "    eval_dataset: Dataset,\n",
    "    lora_config: dict,\n",
    "    resume_from_checkpoint: Union[str, bool] = \"auto\",\n",
    "):\n",
    "    \"\"\"Fine-tune unified adapter trên Kaggle\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # CRITICAL: Verify single GPU mode before training\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"\\nWARNING: {torch.cuda.device_count()} GPUs detected\")\n",
    "            print(\"Forcing CUDA_VISIBLE_DEVICES=0 to prevent multi-GPU issues\")\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "        \n",
    "        print(f\"\\nCUDA Device Check:\")\n",
    "        print(f\"  Available devices: {torch.cuda.device_count()}\")\n",
    "        print(f\"  Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"  Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Check resume\n",
    "    if resume_from_checkpoint == \"auto\":\n",
    "        resume_checkpoint = checkpoint_mgr.find_latest_checkpoint()\n",
    "        if resume_checkpoint:\n",
    "            print(f\"\\nResuming from: {resume_checkpoint}\")\n",
    "        else:\n",
    "            print(\"\\nNo checkpoint found - training from scratch\")\n",
    "            resume_checkpoint = None\n",
    "    elif resume_from_checkpoint:\n",
    "        resume_checkpoint = resume_from_checkpoint\n",
    "        print(f\"\\nResuming from: {resume_checkpoint}\")\n",
    "    else:\n",
    "        resume_checkpoint = None\n",
    "        print(\"\\nTraining from scratch\")\n",
    "    \n",
    "    # Pre-format dataset to \"text\" column (required by SFTTrainer)\n",
    "    def format_example(example):\n",
    "        return {\"text\": f\"{example['input']}\\n\\n{example['output']}\"}\n",
    "    \n",
    "    print(\"\\nFormatting datasets...\")\n",
    "    formatted_train = train_dataset.map(format_example, remove_columns=train_dataset.column_names)\n",
    "    formatted_eval = eval_dataset.map(format_example, remove_columns=eval_dataset.column_names)\n",
    "    print(f\"  Train: {len(formatted_train)} samples\")\n",
    "    print(f\"  Eval: {len(formatted_eval)} samples\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(**TRAINING_CONFIG)\n",
    "    \n",
    "    # CRITICAL: Verify training args prevent multi-GPU\n",
    "    if hasattr(training_args, 'local_rank') and training_args.local_rank != -1:\n",
    "        print(f\"\\nWARNING: local_rank={training_args.local_rank}, forcing to -1\")\n",
    "        training_args.local_rank = -1\n",
    "    \n",
    "    # Create trainer with pre-formatted dataset\n",
    "    print(\"\\nInitializing SFTTrainer...\")\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=formatted_train,\n",
    "        eval_dataset=formatted_eval,\n",
    "        # CRITICAL: Disable packing and other features that might cause issues\n",
    "        packing=False,\n",
    "        max_seq_length=512,  # Explicit max length\n",
    "        dataset_text_field=\"text\",\n",
    "    )\n",
    "    \n",
    "    # CRITICAL: Verify model is not wrapped in DataParallel\n",
    "    if hasattr(trainer.model, 'module'):\n",
    "        raise RuntimeError(\n",
    "            \"CRITICAL ERROR: Model is wrapped in DataParallel!\\n\"\n",
    "            \"This causes 'illegal memory access' errors with 4-bit quantization.\\n\"\n",
    "            \"Please restart kernel and ensure CUDA_VISIBLE_DEVICES=0 is set.\"\n",
    "        )\n",
    "    \n",
    "    # Register với shutdown handler\n",
    "    shutdown_handler.register_trainer(trainer, model)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING IN PROGRESS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "    print(f\"Checkpoints saved every {TRAINING_CONFIG['save_steps']} steps\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Single GPU mode: enabled\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    \n",
    "    # Save final model\n",
    "    final_path = OUTPUT_DIR / \"unified_lora_adapter\"\n",
    "    final_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAVING FINAL MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    model.save_pretrained(str(final_path))\n",
    "    tokenizer.save_pretrained(str(final_path))\n",
    "    print(f\"\\nFinal model saved: {final_path}\")\n",
    "    \n",
    "    # Save training state\n",
    "    checkpoint_mgr.save_training_state(\n",
    "        final_model=str(final_path),\n",
    "        completed=True,\n",
    "        total_steps=trainer.state.global_step,\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "print(\"Training function ready - Single GPU mode enforced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35922d",
   "metadata": {},
   "source": [
    "## 11. RUN TRAINING\n",
    "\n",
    "**BEFORE RUNNING:** Make sure you have loaded a dataset!\n",
    "\n",
    "### Quick Checklist:\n",
    "- Internet enabled in Kaggle settings  \n",
    "- Dataset uploaded to Kaggle and added to notebook  \n",
    "- Dataset loaded successfully (check cell 6 output)  \n",
    "- Model and LoRA adapter configured  \n",
    "\n",
    "### What Happens:\n",
    "- **Checkpoints:** Auto-saved every 100 steps to `/kaggle/working/unified_model/`\n",
    "- **Output:** All files in `/kaggle/working/` saved as Kaggle output after session\n",
    "- **Resume:** Upload previous output as input, notebook auto-resumes from latest checkpoint\n",
    "\n",
    "### Expected Time:\n",
    "- **P100/T4:** 6-10 hours (HIGH QUALITY config)\n",
    "- **CPU:** Not recommended (very slow)\n",
    "\n",
    "### If Dataset Not Loaded:\n",
    "Cell will show detailed instructions on how to upload and configure your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset is loaded\n",
    "if train_dataset is None or val_dataset is None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ERROR: DATASET NOT LOADED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nTraining cannot start without a dataset!\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"QUICK FIX - Choose ONE option:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nOPTION A: Use Test Data (FASTEST - for debugging only)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"1. Scroll up to Cell 7 (Configuration)\")\n",
    "    print(\"2. Find the line: USE_TEST_DATA = False\")\n",
    "    print(\"3. Change to:     USE_TEST_DATA = True\")\n",
    "    print(\"4. Re-run Cell 7 (Configuration)\")\n",
    "    print(\"5. Re-run Cell 15 (Load Dataset)\")\n",
    "    print(\"6. Come back here and run this cell\")\n",
    "    print()\n",
    "    print(\"WARNING: Test data is synthetic - only for testing the pipeline!\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"\\nOPTION B: Upload Real Dataset (RECOMMENDED for production)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"1. Prepare your data:\")\n",
    "    print(\"   - Format: JSONL files (train.jsonl, val.jsonl)\")\n",
    "    print('   - Structure: {\"input\": \"...\", \"output\": \"...\"}')\n",
    "    print()\n",
    "    print(\"2. Upload to Kaggle:\")\n",
    "    print(\"   - Go to: https://www.kaggle.com/datasets\")\n",
    "    print(\"   - Click: New Dataset\")\n",
    "    print(\"   - Upload: train.jsonl and val.jsonl\")\n",
    "    print(\"   - Name it: lexilingo-training-data (or any name)\")\n",
    "    print()\n",
    "    print(\"3. Add to this notebook:\")\n",
    "    print(\"   - Right sidebar -> Settings (gear icon)\")\n",
    "    print(\"   - Scroll to: Data section\")\n",
    "    print(\"   - Click: + Add Data\")\n",
    "    print(\"   - Search: your dataset name\")\n",
    "    print(\"   - Click: Add\")\n",
    "    print()\n",
    "    print(\"4. Re-run cells:\")\n",
    "    print(\"   - Re-run Cell 7 (Configuration)\")\n",
    "    print(\"   - Re-run Cell 15 (Load Dataset)\")\n",
    "    print(\"   - Come back here and run this cell\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Current Status: NO DATASET\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    raise RuntimeError(\n",
    "        \"\\nDataset not loaded! Choose one option above:\\n\"\n",
    "        \"  A) Set USE_TEST_DATA = True in Cell 7 (for debugging)\\n\"\n",
    "        \"  B) Upload real dataset to Kaggle (for production)\\n\"\n",
    "        \"\\nThen re-run the configuration and dataset loading cells.\"\n",
    "    )\n",
    "\n",
    "# Dataset info\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET READY FOR TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTrain samples: {len(train_dataset):,}\")\n",
    "print(f\"Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "if USE_TEST_DATA:\n",
    "    print(f\"\\nMode: TEST DATA (synthetic)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"WARNING: This is for debugging only!\")\n",
    "    print(\"For production training, use real dataset!\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(f\"\\nMode: REAL DATA (production)\")\n",
    "    print(\"Dataset ready for training\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "print(\"Starting training process...\\n\")\n",
    "trained_model, trainer = finetune_unified_adapter(\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    lora_config=UNIFIED_LORA_CONFIG,\n",
    "    resume_from_checkpoint=\"auto\",  # Auto-resume từ checkpoint nếu có\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput location: {OUTPUT_DIR}\")\n",
    "print(\"This will be saved as Kaggle output automatically\")\n",
    "print(\"\\nTo resume in new session:\")\n",
    "print(\"1. Add this notebook's output as input dataset\")\n",
    "print(\"2. Set resume_from_checkpoint to checkpoint path\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001731c",
   "metadata": {},
   "source": [
    "## 12. Evaluate Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04544b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on validation set\n",
    "print(\"Running evaluation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b6d6d",
   "metadata": {},
   "source": [
    "## 13. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def plot_training_metrics(trainer):\n",
    "    \"\"\"Visualize training và validation metrics\"\"\"\n",
    "    \n",
    "    # Extract metrics from trainer history\n",
    "    history = trainer.state.log_history\n",
    "    \n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    learning_rates = []\n",
    "    steps_train = []\n",
    "    steps_eval = []\n",
    "    \n",
    "    for entry in history:\n",
    "        if 'loss' in entry:\n",
    "            train_loss.append(entry['loss'])\n",
    "            steps_train.append(entry['step'])\n",
    "            if 'learning_rate' in entry:\n",
    "                learning_rates.append(entry['learning_rate'])\n",
    "        if 'eval_loss' in entry:\n",
    "            eval_loss.append(entry['eval_loss'])\n",
    "            steps_eval.append(entry['step'])\n",
    "    \n",
    "    # Check if we have data to plot\n",
    "    if not train_loss:\n",
    "        print(\"\\nNo training data available yet. Train for at least a few steps first.\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Training Progress Overview', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    axes[0, 0].plot(steps_train, train_loss, 'b-', linewidth=2, alpha=0.7, label='Training Loss')\n",
    "    axes[0, 0].set_xlabel('Steps', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Plot 2: Evaluation Loss\n",
    "    if eval_loss:\n",
    "        axes[0, 1].plot(steps_eval, eval_loss, 'r-', linewidth=2, alpha=0.7, label='Validation Loss')\n",
    "        axes[0, 1].set_xlabel('Steps', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "        axes[0, 1].set_title('Validation Loss Over Time', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].legend()\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No validation data yet', \n",
    "                       ha='center', va='center', fontsize=14, transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Learning Rate\n",
    "    if learning_rates:\n",
    "        axes[1, 0].plot(steps_train, learning_rates, 'g-', linewidth=2, alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Steps', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Learning Rate', fontsize=12)\n",
    "        axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No learning rate data', \n",
    "                       ha='center', va='center', fontsize=14, transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Combined Loss Comparison\n",
    "    if eval_loss:\n",
    "        axes[1, 1].plot(steps_train, train_loss, 'b-', linewidth=2, alpha=0.7, label='Train Loss')\n",
    "        axes[1, 1].plot(steps_eval, eval_loss, 'r-', linewidth=2, alpha=0.7, label='Val Loss')\n",
    "        axes[1, 1].set_xlabel('Steps', fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Loss', fontsize=12)\n",
    "        axes[1, 1].set_title('Train vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].legend()\n",
    "    else:\n",
    "        axes[1, 1].plot(steps_train, train_loss, 'b-', linewidth=2, alpha=0.7, label='Train Loss')\n",
    "        axes[1, 1].set_xlabel('Steps', fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Loss', fontsize=12)\n",
    "        axes[1, 1].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = OUTPUT_DIR / \"training_metrics.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nPlot saved: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTraining Loss:\")\n",
    "    print(f\"  Initial: {train_loss[0]:.4f}\")\n",
    "    print(f\"  Final: {train_loss[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(train_loss):.4f}\")\n",
    "    \n",
    "    # Safe improvement calculation\n",
    "    if train_loss[0] > 0:\n",
    "        improvement = (train_loss[0] - train_loss[-1]) / train_loss[0] * 100\n",
    "        print(f\"  Improvement: {improvement:.2f}%\")\n",
    "    else:\n",
    "        print(f\"  Improvement: N/A (initial loss is 0)\")\n",
    "    \n",
    "    if eval_loss:\n",
    "        print(f\"\\nValidation Loss:\")\n",
    "        print(f\"  Initial: {eval_loss[0]:.4f}\")\n",
    "        print(f\"  Final: {eval_loss[-1]:.4f}\")\n",
    "        print(f\"  Best: {min(eval_loss):.4f}\")\n",
    "        print(f\"  Best at step: {steps_eval[eval_loss.index(min(eval_loss))]}\")\n",
    "    else:\n",
    "        print(f\"\\nValidation Loss:\")\n",
    "        print(f\"  No evaluation data yet (will be available after first eval_steps)\")\n",
    "    \n",
    "    print(f\"\\nTotal training steps: {steps_train[-1] if steps_train else 0}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Generate visualization\n",
    "print(\"Generating training visualizations...\")\n",
    "try:\n",
    "    plot_training_metrics(trainer)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError generating plots: {e}\")\n",
    "    print(\"This may happen if training hasn't completed enough steps yet.\")\n",
    "    print(\"Try running this cell again after training progresses further.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fb18f",
   "metadata": {},
   "source": [
    "## 14. Detailed Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9269f8",
   "metadata": {},
   "source": [
    "## 14a. Diagnostic - Model & Tokenizer Validation\n",
    "\n",
    "**Important**: Run this diagnostic before evaluation to check for issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL & TOKENIZER DIAGNOSTIC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Check vocabulary sizes\n",
    "print(\"\\n1. VOCABULARY SIZE CHECK:\")\n",
    "print(f\"   Tokenizer vocab_size: {tokenizer.vocab_size}\")\n",
    "print(f\"   Model config vocab_size: {model.config.vocab_size}\")\n",
    "\n",
    "if tokenizer.vocab_size != model.config.vocab_size:\n",
    "    print(\"     MISMATCH DETECTED! This will cause CUDA errors!\")\n",
    "    print(f\"   Difference: {abs(tokenizer.vocab_size - model.config.vocab_size)}\")\n",
    "else:\n",
    "        print(\"   Vocabulary sizes match\")\n",
    "\n",
    "# 2. Check special tokens\n",
    "print(\"\\n2. SPECIAL TOKENS CHECK:\")\n",
    "print(f\"   pad_token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})\")\n",
    "print(f\"   eos_token: {tokenizer.eos_token} (id={tokenizer.eos_token_id})\")\n",
    "print(f\"   bos_token: {tokenizer.bos_token} (id={tokenizer.bos_token_id})\")\n",
    "\n",
    "# Validate special token IDs are within range\n",
    "invalid_tokens = []\n",
    "if tokenizer.pad_token_id >= tokenizer.vocab_size:\n",
    "    invalid_tokens.append(f\"pad_token_id={tokenizer.pad_token_id}\")\n",
    "if tokenizer.eos_token_id >= tokenizer.vocab_size:\n",
    "    invalid_tokens.append(f\"eos_token_id={tokenizer.eos_token_id}\")\n",
    "if tokenizer.bos_token_id and tokenizer.bos_token_id >= tokenizer.vocab_size:\n",
    "    invalid_tokens.append(f\"bos_token_id={tokenizer.bos_token_id}\")\n",
    "\n",
    "if invalid_tokens:\n",
    "    print(f\"   INVALID TOKEN IDs: {', '.join(invalid_tokens)}\")\n",
    "    print(f\"   These exceed vocab_size={tokenizer.vocab_size}\")\n",
    "else:\n",
    "    print(\"   All special tokens within valid range\")\n",
    "# 3. Check model state\n",
    "print(\"\\n3. MODEL STATE CHECK:\")\n",
    "print(f\"   Model device: {next(model.parameters()).device}\")\n",
    "print(f\"   Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"   Training mode: {model.training}\")\n",
    "print(f\"   Gradient checkpointing: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "# 4. Test simple tokenization\n",
    "print(\"\\n4. TOKENIZATION TEST:\")\n",
    "test_text = \"Hello, this is a test.\"\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"   Input text: '{test_text}'\")\n",
    "print(f\"   Token IDs shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"   Token IDs: {tokens['input_ids'][0].tolist()}\")\n",
    "print(f\"   Max token ID: {tokens['input_ids'].max().item()}\")\n",
    "print(f\"   Min token ID: {tokens['input_ids'].min().item()}\")\n",
    "\n",
    "if tokens['input_ids'].max().item() >= tokenizer.vocab_size:\n",
    "    print(f\"     ERROR: Max token ID ({tokens['input_ids'].max().item()}) >= vocab_size ({tokenizer.vocab_size})\")\n",
    "    print(f\"   ERROR: Max token ID ({tokens['input_ids'].max().item()}) >= vocab_size ({tokenizer.vocab_size})\")\n",
    "    print(f\"    All token IDs within valid range [0, {tokenizer.vocab_size-1}]\")\n",
    "    print(f\"   All token IDs within valid range [0, {tokenizer.vocab_size-1}]\")\n",
    "# 5. Test simple generation (CRITICAL TEST)\n",
    "print(\"\\n5. SIMPLE GENERATION TEST:\")\n",
    "print(\"   Testing with 'Hello' input...\")\n",
    "\n",
    "try:\n",
    "    # Clear any previous CUDA errors\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Simple input\n",
    "    simple_input = tokenizer(\"Hello\", return_tensors=\"pt\", padding=True, return_attention_mask=True)\n",
    "    device = next(model.parameters()).device\n",
    "    simple_input = {k: v.to(device) for k, v in simple_input.items()}\n",
    "    \n",
    "    print(f\"   Input token IDs: {simple_input['input_ids'][0].tolist()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Try minimal generation\n",
    "        output = model.generate(\n",
    "            input_ids=simple_input['input_ids'],\n",
    "            attention_mask=simple_input['attention_mask'],\n",
    "            max_new_tokens=5,  # Just 5 tokens\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    print(f\"   Output token IDs: {output[0].tolist()}\")\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"   Decoded output: '{decoded}'\")\n",
    "    print(\"    Generation successful!\")\n",
    "    print(\"   Generation successful\")\n",
    "except RuntimeError as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"    GENERATION FAILED!\")\n",
    "    print(f\"   GENERATION FAILED\")\n",
    "    \n",
    "    if 'device-side assert' in error_msg or 'CUDA' in error_msg:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ROOT CAUSE IDENTIFIED: Model is generating invalid token IDs\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"1. Model training corrupted the vocabulary embeddings\")\n",
    "        print(\"2. LoRA adapter is incompatible with base model\")\n",
    "        print(\"3. Quantization issue with 4-bit model\")\n",
    "        print(\"4. Model config doesn't match tokenizer\")\n",
    "        print(\"\\nSuggested fixes:\")\n",
    "        print(\"1. Reload the base model (without LoRA)\")\n",
    "        print(\"2. Check if checkpoint is corrupted\")\n",
    "        print(\"3. Try without quantization\")\n",
    "        print(\"4. Verify model and tokenizer are from same checkpoint\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "# 6. Check dataset sample\n",
    "print(\"\\n6. DATASET SAMPLE CHECK:\")\n",
    "if val_dataset:\n",
    "    sample = val_dataset[0]\n",
    "    print(f\"   Sample keys: {sample.keys()}\")\n",
    "    print(f\"   Input preview: {sample['input'][:100]}...\")\n",
    "    \n",
    "    # Tokenize dataset sample\n",
    "    tokens = tokenizer(sample['input'], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    print(f\"   Tokenized shape: {tokens['input_ids'].shape}\")\n",
    "    print(f\"   Max token in sample: {tokens['input_ids'].max().item()}\")\n",
    "    \n",
    "    if tokens['input_ids'].max().item() >= tokenizer.vocab_size:\n",
    "        print(f\"     Dataset contains invalid token IDs!\")\n",
    "        print(f\"   Dataset contains invalid token IDs\")\n",
    "        print(f\"    Dataset tokens are valid\")\n",
    "        print(f\"   Dataset tokens are valid\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"   If all checks pass -> Proceed to evaluation\")\n",
    "print(\"  If all checks pass -> Proceed to evaluation\")\n",
    "print(\"  If generation test fails -> See suggested fixes above\")\n",
    "print(\"  If vocab mismatch -> Reload model and tokenizer together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641477a",
   "metadata": {},
   "source": [
    "## 14b. Model Recovery (Run only if diagnostic fails)\n",
    "\n",
    "**Only run this if the diagnostic test failed!** This will reload the model from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037da9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOVERY: Reload model from checkpoint\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL RECOVERY - Loading from checkpoint\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Find best checkpoint\n",
    "best_checkpoint = checkpoint_mgr.find_latest_checkpoint()\n",
    "if not best_checkpoint:\n",
    "    final_model_path = OUTPUT_DIR / \"unified_lora_adapter\"\n",
    "    if final_model_path.exists():\n",
    "        best_checkpoint = str(final_model_path)\n",
    "\n",
    "if best_checkpoint:\n",
    "    print(f\"\\nLoading from: {best_checkpoint}\")\n",
    "    \n",
    "    # Reload tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        best_checkpoint,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Reload base model\n",
    "    device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else {\"\": \"cpu\"}\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=QUANTIZATION_CONFIG,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=str(CACHE_DIR)\n",
    "    )\n",
    "    \n",
    "    # FIX: Calculate required vocab size including special tokens\n",
    "    special_token_ids = [\n",
    "        tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0,\n",
    "        tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 0,\n",
    "        tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0,\n",
    "    ]\n",
    "    max_special_token_id = max(special_token_ids)\n",
    "    required_vocab_size = max(tokenizer.vocab_size, base_model.config.vocab_size, max_special_token_id + 1)\n",
    "    \n",
    "    # Resize embeddings to accommodate all tokens\n",
    "    if base_model.config.vocab_size != required_vocab_size:\n",
    "        print(f\"  -> Resizing embeddings: {base_model.config.vocab_size} -> {required_vocab_size}\")\n",
    "        base_model.resize_token_embeddings(required_vocab_size)\n",
    "        base_model.config.vocab_size = required_vocab_size\n",
    "    \n",
    "    # Set pad token AFTER resizing\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    base_model.enable_input_require_grads()\n",
    "    \n",
    "    # Load LoRA adapter from checkpoint\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(base_model, best_checkpoint)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\nModel reloaded successfully from checkpoint\")\n",
    "    print(f\"  Device: {next(model.parameters()).device}\")\n",
    "    print(f\"  Vocab size: {model.config.vocab_size}\")\n",
    "    print(f\"  Tokenizer vocab: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Test generation again\n",
    "    print(\"\\nTesting generation after reload...\")\n",
    "    try:\n",
    "        test_input = tokenizer(\"Hello\", return_tensors=\"pt\")\n",
    "        test_input = {k: v.to(next(model.parameters()).device) for k, v in test_input.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_output = model.generate(\n",
    "                **test_input,\n",
    "                max_new_tokens=5,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        print(f\"Generation test passed\")\n",
    "        print(f\"  Output: {tokenizer.decode(test_output[0], skip_special_tokens=True)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Generation still failing: {str(e)[:100]}\")\n",
    "        print(\"\\nThe model checkpoint may be corrupted.\")\n",
    "        print(\"You may need to restart training from an earlier checkpoint.\")\n",
    "else:\n",
    "    print(\"\\nNo checkpoint found to reload from\")\n",
    "    print(\"Training may not have saved any checkpoints yet\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# For debugging CUDA errors, uncomment this:\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "def detailed_evaluation(trainer, dataset, sample_size=100):\n",
    "    \"\"\"Chạy detailed evaluation với sample predictions\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DETAILED EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get evaluation metrics\n",
    "    try:\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(\"\\nOverall Metrics:\")\n",
    "        for key, value in eval_results.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nEvaluation failed: {e}\")\n",
    "        print(\"Continuing with sample predictions...\")\n",
    "        eval_results = {}\n",
    "    \n",
    "    # Sample predictions\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"SAMPLE PREDICTIONS (n={min(sample_size, len(dataset))})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    sample_indices = np.random.choice(len(dataset), min(sample_size, len(dataset)), replace=False)\n",
    "    \n",
    "    predictions_data = []\n",
    "    errors_count = 0\n",
    "    \n",
    "    # Get device explicitly\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"\\nModel device: {device}\")\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices[:10], 1):  # Show first 10\n",
    "        try:\n",
    "            sample = dataset[int(idx)]\n",
    "            \n",
    "            # Generate prediction\n",
    "            input_text = sample['input']\n",
    "            expected_output = sample['output']\n",
    "            \n",
    "            # Validate input text\n",
    "            if not input_text or len(input_text.strip()) == 0:\n",
    "                print(f\"  Sample {i}: Skipping empty input\")\n",
    "                continue\n",
    "            \n",
    "            # Tokenize with explicit parameters\n",
    "            inputs = tokenizer(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=512,\n",
    "                padding=True,\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Move to device explicitly\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Validate token IDs are within vocabulary range\n",
    "            if inputs['input_ids'].max() >= tokenizer.vocab_size:\n",
    "                print(f\"  Sample {i}: Invalid token IDs (max={inputs['input_ids'].max()}, vocab_size={tokenizer.vocab_size})\")\n",
    "                continue\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Use greedy decoding for evaluation (deterministic, reproducible)\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_new_tokens=128,  # Reduced from 256 for safety\n",
    "                    do_sample=False,  # Greedy decoding - deterministic\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Remove input from prediction\n",
    "            if input_text in predicted:\n",
    "                predicted = predicted.replace(input_text, \"\").strip()\n",
    "            \n",
    "            predictions_data.append({\n",
    "                'input': input_text[:100] + \"...\" if len(input_text) > 100 else input_text,\n",
    "                'expected': expected_output[:100] + \"...\" if len(expected_output) > 100 else expected_output,\n",
    "                'predicted': predicted[:100] + \"...\" if len(predicted) > 100 else predicted,\n",
    "            })\n",
    "            \n",
    "            print(f\" Sample {i} processed successfully\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            errors_count += 1\n",
    "            error_msg = str(e)\n",
    "            if 'CUDA' in error_msg or 'device-side assert' in error_msg:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"  CUDA ERROR on sample {i}\")\n",
    "                print(f\"{'='*70}\")\n",
    "                print(f\"Error: {error_msg[:200]}\")\n",
    "                print(f\"\\nThis usually means:\")\n",
    "                print(f\"  1. Invalid token IDs in the dataset\")\n",
    "                print(f\"  2. Corrupted input text\")\n",
    "                print(f\"  3. Memory issues\")\n",
    "                print(f\"\\nTo debug:\")\n",
    "                print(f\"  1. Uncomment CUDA_LAUNCH_BLOCKING=1 at the top of this cell\")\n",
    "                print(f\"  2. Re-run to get exact error location\")\n",
    "                print(f\"  3. Check the input text: {input_text[:50]}...\")\n",
    "                print(f\"{'='*70}\\n\")\n",
    "                \n",
    "                if errors_count > 2:\n",
    "                    print(f\"\\n  Too many CUDA errors ({errors_count}), stopping evaluation\")\n",
    "                    print(f\"Please check your dataset for corrupted samples\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"  Sample {i}: {error_msg[:100]}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            errors_count += 1\n",
    "            print(f\"  Sample {i}: Unexpected error - {str(e)[:100]}\")\n",
    "            if errors_count > 3:\n",
    "                print(f\"\\n  Too many errors ({errors_count}), stopping evaluation\")\n",
    "                break\n",
    "            continue\n",
    "    \n",
    "    # Display predictions\n",
    "    if predictions_data:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PREDICTION RESULTS ({len(predictions_data)} successful)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for i, pred in enumerate(predictions_data, 1):\n",
    "            print(f\"\\n--- Sample {i} ---\")\n",
    "            print(f\"Input: {pred['input']}\")\n",
    "            print(f\"Expected: {pred['expected']}\")\n",
    "            print(f\"Predicted: {pred['predicted']}\")\n",
    "            print(\"-\" * 70)\n",
    "    else:\n",
    "        print(f\"\\n  No predictions generated - all samples failed\")\n",
    "        print(f\"This indicates a serious issue with the model or dataset\")\n",
    "        return None\n",
    "    \n",
    "    # Save predictions to file\n",
    "    predictions_file = OUTPUT_DIR / \"sample_predictions.json\"\n",
    "    with open(predictions_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(predictions_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n Predictions saved: {predictions_file}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Successful predictions: {len(predictions_data)}\")\n",
    "    print(f\"Failed samples: {errors_count}\")\n",
    "    print(f\"Success rate: {len(predictions_data)/(len(predictions_data)+errors_count)*100:.1f}%\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "# Run detailed evaluation with better error handling\n",
    "print(\"Running detailed evaluation...\")\n",
    "print(\"\\nNote: If you encounter CUDA errors:\")\n",
    "print(\"  1. Uncomment CUDA_LAUNCH_BLOCKING=1 at top of this cell\")\n",
    "print(\"  2. Re-run for detailed error location\")\n",
    "print(\"  3. Check dataset for corrupted samples\\n\")\n",
    "\n",
    "try:\n",
    "    eval_metrics = detailed_evaluation(trainer, val_dataset, sample_size=100)\n",
    "    if eval_metrics is not None:\n",
    "        print(\"\\n Evaluation completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n  Evaluation completed with errors - check output above\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION FAILED\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"\\nPossible causes:\")\n",
    "    print(f\"  1. Training hasn't completed yet\")\n",
    "    print(f\"  2. Model or trainer in invalid state\")\n",
    "    print(f\"  3. Dataset contains corrupted samples\")\n",
    "    print(f\"  4. GPU memory issues\")\n",
    "    print(f\"\\nTry:\")\n",
    "    print(f\"  1. Wait for training to complete fully\")\n",
    "    print(f\"  2. Restart kernel and reload checkpoint\")\n",
    "    print(f\"  3. Check dataset integrity\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    eval_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa1684",
   "metadata": {},
   "source": [
    "## 15. Test Inference on Custom Prompts\n",
    "\n",
    "**Note:** If you see warnings like `\"generation flags are not valid and may be ignored\"`, they are harmless and can be ignored. These occur when transformers validates generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42947a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt: str, max_length: int = 256):\n",
    "    \"\"\"Test model với prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove input from response\n",
    "    if prompt in response:\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "    return response\n",
    "\n",
    "# Test với các tasks khác nhau\n",
    "test_prompts = {\n",
    "    \"Fluency Assessment\": \"Task: Assess the fluency of the following text.\\n\\nText: The cat sat on the mat and looked out the window.\\n\\nFluency score:\",\n",
    "    \"Vocabulary Level\": \"Task: Classify the vocabulary level of this text.\\n\\nText: The ubiquitous smartphone has revolutionized communication paradigms.\\n\\nVocabulary level:\",\n",
    "    \"Grammar Correction\": \"Task: Correct any grammar errors in the following text.\\n\\nText: She don't like apples and he have three dogs.\\n\\nCorrected text:\",\n",
    "    \"Dialogue Generation\": \"Task: Continue the conversation naturally.\\n\\nUser: Hello, how are you today?\\nAssistant:\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERACTIVE INFERENCE TESTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for task_name, prompt in test_prompts.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK: {task_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nPrompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    response = test_inference(prompt, max_length=200)\n",
    "    print(f\"Response:\\n{response}\\n\")\n",
    "    \n",
    "    results_table.append({\n",
    "        'task': task_name,\n",
    "        'prompt': prompt,\n",
    "        'response': response\n",
    "    })\n",
    "\n",
    "# Save test results\n",
    "test_results_file = OUTPUT_DIR / \"inference_tests.json\"\n",
    "with open(test_results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_table, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Test results saved: {test_results_file}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d1007",
   "metadata": {},
   "source": [
    "## 16. Task-Specific Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_task_performance(dataset, num_samples=50):\n",
    "    \"\"\"Phân tích performance theo từng task type\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TASK-SPECIFIC PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Group samples by task type (detect from input)\n",
    "    task_groups = {\n",
    "        'fluency': [],\n",
    "        'vocabulary': [],\n",
    "        'grammar': [],\n",
    "        'dialogue': []\n",
    "    }\n",
    "    \n",
    "    # Sample and classify\n",
    "    sample_indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        sample = dataset[int(idx)]\n",
    "        input_lower = sample['input'].lower()\n",
    "        \n",
    "        if 'fluency' in input_lower or 'score' in input_lower:\n",
    "            task_groups['fluency'].append(sample)\n",
    "        elif 'vocabulary' in input_lower or 'level' in input_lower or 'classify' in input_lower:\n",
    "            task_groups['vocabulary'].append(sample)\n",
    "        elif 'grammar' in input_lower or 'correct' in input_lower or 'error' in input_lower:\n",
    "            task_groups['grammar'].append(sample)\n",
    "        elif 'conversation' in input_lower or 'dialogue' in input_lower or 'respond' in input_lower:\n",
    "            task_groups['dialogue'].append(sample)\n",
    "    \n",
    "    # Visualize distribution\n",
    "    task_counts = {k: len(v) for k, v in task_groups.items()}\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "    bars = plt.bar(task_counts.keys(), task_counts.values(), color=colors, alpha=0.7)\n",
    "    \n",
    "    plt.title('Sample Distribution by Task Type', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Task Type', fontsize=12)\n",
    "    plt.ylabel('Number of Samples', fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    dist_plot_path = OUTPUT_DIR / \"task_distribution.png\"\n",
    "    plt.savefig(dist_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTask distribution plot saved: {dist_plot_path}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nTask Distribution:\")\n",
    "    for task, count in task_counts.items():\n",
    "        percentage = (count / sum(task_counts.values())) * 100\n",
    "        print(f\"  {task.capitalize()}: {count} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return task_groups\n",
    "\n",
    "# Run analysis\n",
    "task_analysis = analyze_task_performance(val_dataset, num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a1be0",
   "metadata": {},
   "source": [
    "## 17. Save Final Artifacts & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ecb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training summary\n",
    "# Handle eval_metrics if evaluation was skipped\n",
    "if 'eval_metrics' not in globals() or eval_metrics is None:\n",
    "    eval_metrics = {\"note\": \"Evaluation was not run or returned None\"}\n",
    "\n",
    "summary = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"lora_config\": UNIFIED_LORA_CONFIG,\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"dataset_size\": {\n",
    "        \"train\": len(train_dataset),\n",
    "        \"val\": len(val_dataset),\n",
    "    },\n",
    "    \"training_completed\": datetime.now().isoformat(),\n",
    "    \"platform\": \"kaggle\",\n",
    "    \"output_dir\": str(OUTPUT_DIR),\n",
    "    \"final_metrics\": eval_metrics,\n",
    "    \"total_steps\": trainer.state.global_step if 'trainer' in globals() else \"N/A\",\n",
    "    \"best_checkpoint\": trainer.state.best_model_checkpoint if 'trainer' in globals() else \"N/A\",\n",
    "}\n",
    "\n",
    "summary_file = OUTPUT_DIR / \"training_summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING ARTIFACTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  - unified_lora_adapter/ (final model)\")\n",
    "print(f\"  - checkpoint-*/ (training checkpoints)\")\n",
    "print(f\"  - training_metrics.png (visualization)\")\n",
    "print(f\"  - sample_predictions.json (evaluation samples)\")\n",
    "print(f\"  - inference_tests.json (test results)\")\n",
    "print(f\"  - task_distribution.png (task analysis)\")\n",
    "print(f\"  - training_summary.json (complete summary)\")\n",
    "print(f\"  - training_state.json (resume info)\")\n",
    "\n",
    "print(f\"\\nAll files in /kaggle/working/ will be saved as Kaggle output\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# List all output files with sizes\n",
    "print(\"Output files details:\")\n",
    "total_size = 0\n",
    "for item in sorted(OUTPUT_DIR.rglob(\"*\")):\n",
    "    if item.is_file():\n",
    "        size_mb = item.stat().st_size / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        rel_path = item.relative_to(OUTPUT_DIR)\n",
    "        print(f\"  {rel_path} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal output size: {total_size:.2f} MB\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Define OUTPUT_DIR in case this cell is run standalone\n",
    "if 'OUTPUT_DIR' not in globals():\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working/unified_model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OUTPUT SUMMARY FOR DOWNLOAD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nOutput location: {OUTPUT_DIR}\")\n",
    "print(f\"   (Automatically saved as Kaggle output)\")\n",
    "\n",
    "# List all files with sizes\n",
    "total_size = 0\n",
    "file_list = []\n",
    "\n",
    "if OUTPUT_DIR.exists():\n",
    "    print(f\"\\nFiles to be downloaded:\\n\")\n",
    "    \n",
    "    for item in sorted(OUTPUT_DIR.rglob(\"*\")):\n",
    "        if item.is_file():\n",
    "            size_mb = item.stat().st_size / (1024 * 1024)\n",
    "            total_size += size_mb\n",
    "            rel_path = item.relative_to(OUTPUT_DIR)\n",
    "            file_list.append((str(rel_path), size_mb))\n",
    "            \n",
    "            # Print with proper formatting\n",
    "            if size_mb < 1:\n",
    "                print(f\"   {rel_path} ({size_mb*1024:.1f} KB)\")\n",
    "            else:\n",
    "                print(f\"   {rel_path} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"   Total output size: {total_size:.2f} MB ({total_size/1024:.2f} GB)\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Check for important files\n",
    "    important_files = [\n",
    "        OUTPUT_DIR / \"unified_lora_adapter\",\n",
    "        OUTPUT_DIR / \"training_summary.json\",\n",
    "        OUTPUT_DIR / \"training_metrics.png\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nImportant files check:\")\n",
    "    for file_path in important_files:\n",
    "        if file_path.exists():\n",
    "            if file_path.is_dir():\n",
    "                count = len(list(file_path.iterdir()))\n",
    "                print(f\"   [OK] {file_path.name}/ ({count} files)\")\n",
    "            else:\n",
    "                print(f\"   [OK] {file_path.name}\")\n",
    "        else:\n",
    "            print(f\"   [MISSING] {file_path.name}\")\n",
    "else:\n",
    "    print(f\"\\nOutput directory not found: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOAD INSTRUCTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "After this notebook session ends:\n",
    "\n",
    "1⃣  **Download from Kaggle UI (Easiest):**\n",
    "   - Right side panel -> \"Output\" section\n",
    "   - Click \"Download\" button\n",
    "   - Extract .zip file locally\n",
    "\n",
    "2⃣  **Copy to your local project:**\n",
    "   ```bash\n",
    "   # On your local machine:\n",
    "   cd ~/Documents/RepoGitHub/LexiLingo/DL-Model-Support\n",
    "   unzip ~/Downloads/archive.zip -d ./temp/\n",
    "   cp -r temp/unified_model/* model/outputs/unified/\n",
    "   ```\n",
    "\n",
    "3⃣  **Verify locally:**\n",
    "   ```bash\n",
    "   ls -lh model/outputs/unified/unified_lora_adapter/\n",
    "   ```\n",
    "\n",
    "4⃣  **Alternative - Create Dataset (for sharing/backup):**\n",
    "   - Output -> \"New Dataset\" button\n",
    "   - Set title: \"lexilingo-unified-model\"\n",
    "   - Make public or private\n",
    "   - Download anytime via Kaggle API or web\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING COMPLETE - Ready to download\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Save file list to JSON for reference\n",
    "import json\n",
    "file_manifest = {\n",
    "    \"total_size_mb\": round(total_size, 2),\n",
    "    \"total_files\": len(file_list),\n",
    "    \"files\": [{\"path\": p, \"size_mb\": round(s, 2)} for p, s in file_list],\n",
    "    \"created\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "manifest_file = OUTPUT_DIR / \"file_manifest.json\"\n",
    "with open(manifest_file, 'w') as f:\n",
    "    json.dump(file_manifest, f, indent=2)\n",
    "\n",
    "print(f\"File manifest saved: {manifest_file}\")\n",
    "print(f\"   (Includes list of all {len(file_list)} files with sizes)\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}