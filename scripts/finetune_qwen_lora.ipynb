{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec1f728",
   "metadata": {},
   "source": [
    "# Fine-tune Qwen2.5 vá»›i LoRA cho LexiLingo\n",
    "## Multi-Task Learning Pipeline trÃªn Mac Intel\n",
    "\n",
    "Pipeline nÃ y fine-tune Qwen2.5-1.5B-Instruct vá»›i 4 LoRA adapters:\n",
    "1. **Fluency Scoring** - ÄÃ¡nh giÃ¡ Ä‘á»™ trÃ´i cháº£y (0.0-1.0)\n",
    "2. **Vocabulary Classification** - PhÃ¢n loáº¡i trÃ¬nh Ä‘á»™ tá»« vá»±ng (A2/B1/B2)\n",
    "3. **Grammar Correction** - Sá»­a lá»—i ngá»¯ phÃ¡p + giáº£i thÃ­ch\n",
    "4. **Dialogue Generation** - Táº¡o pháº£n há»“i tutor\n",
    "\n",
    "**YÃªu cáº§u há»‡ thá»‘ng:**\n",
    "- **Laptop** vá»›i 16GB+ RAM (tá»‘t nháº¥t 32GB)\n",
    "- Python 3.10+\n",
    "- ~5GB disk space\n",
    "- **LÆ°u Ã½:** Training sáº½ cháº­m hÆ¡n náº¿u cháº¡y trÃªn local thay vÃ¬ cháº¡y trÃªn CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb29cc2",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79860e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.36.0 \\\n",
    "    peft>=0.7.0 \\\n",
    "    datasets>=2.16.0 \\\n",
    "    accelerate>=0.25.0 \\\n",
    "    bitsandbytes>=0.41.0 \\\n",
    "    trl>=0.7.0 \\\n",
    "    scipy \\\n",
    "    sentencepiece \\\n",
    "    protobuf \\\n",
    "    wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c59366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running on CPU (Mac Intel)\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n",
      "MPS available: False\n",
      "\n",
      "ðŸ’¡ Tip: Training on CPU will be slower. Consider using 4-bit quantization to save memory.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "\n",
    "# Check device - Mac Intel uses CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\" Running on CPU (Mac Intel)\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(\"\\nðŸ’¡ Tip: Training on CPU will be slower. Consider using 4-bit quantization to save memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642331af",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e582289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration set for Mac Intel i9 (CPU)\n",
      "  Batch size: 2 (effective: 32)\n",
      "  Precision: FP32 (full precision)\n",
      "  CPU workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "# LoRA configuration for each task\n",
    "LORA_CONFIGS = {\n",
    "    \"fluency\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    },\n",
    "    \"vocabulary\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    },\n",
    "    \"grammar\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    },\n",
    "    \"dialogue\": {\n",
    "        \"task_type\": TaskType.CAUSAL_LM,\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"inference_mode\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training configuration (optimized for Mac Intel i9 CPU)\n",
    "TRAINING_CONFIG = {\n",
    "    \"output_dir\": \"./outputs\",\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 2,  # Reduced for CPU\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 16,  # Increased to compensate for smaller batch\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 5,  # More frequent logging\n",
    "    \"save_steps\": 100,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"fp16\": False,  # CPU doesn't support fp16 well\n",
    "    \"bf16\": False,  # CPU doesn't support bf16\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"report_to\": \"none\",  # Change to \"wandb\" if needed\n",
    "    \"dataloader_num_workers\": 4,  # Use multiple CPU cores\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "Path(TRAINING_CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./data\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./adapters\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration set for Mac Intel i9 (CPU)\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['per_device_train_batch_size']} (effective: {TRAINING_CONFIG['per_device_train_batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']})\")\n",
    "print(f\"  Precision: FP32 (full precision)\")\n",
    "print(f\"  CPU workers: {TRAINING_CONFIG['dataloader_num_workers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7de5e",
   "metadata": {},
   "source": [
    "## 3. Load Base Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259abf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with 4-bit quantization for CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a5b03351434c0891ab4c573f749b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ab7a3e86954caca215d453011625f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973ec09e3deb46feb0e87ef864653308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fba9aa22cb4b03a4ff210188bae3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff6999d9e4a442bbcfda4cd42296eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963a819ade5640aca77d08c8021747c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3455a40bf940ee8eaf900ce8e4d963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded: Qwen/Qwen2.5-1.5B-Instruct\n",
      "  Parameters: 1.54B\n",
      "  Quantization: 4-bit (saves ~75% memory)\n",
      "  Device: CPU\n",
      "\n",
      "â±ï¸  Expected training time per adapter: 2-3 hours (with small dataset)\n",
      "ðŸ’¡ Consider using smaller sample for testing, then scale up with full data\n"
     ]
    }
   ],
   "source": [
    "# Quantization config for memory efficiency on CPU\n",
    "# 4-bit quantization helps reduce RAM usage significantly\n",
    "print(\"Loading model with 4-bit quantization for CPU...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float32,  # Use float32 for CPU\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"  # Important for causal LM\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model with quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Auto handles CPU placement\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,  # Enable for better memory efficiency\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "print(f\"âœ“ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Parameters: {base_model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"  Quantization: 4-bit (saves ~75% memory)\")\n",
    "print(f\"  Device: CPU\")\n",
    "print(\"\\nâ±ï¸  Expected training time per adapter: 2-3 hours (with small dataset)\")\n",
    "print(\"ðŸ’¡ Consider using smaller sample for testing, then scale up with full data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32816355",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data\n",
    "\n",
    "### 4.1 Fluency Scoring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f02482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c817079ff8b7458baa0def6091bec006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluency dataset size: 5\n",
      "\n",
      "Example:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Rate the fluency of this English sentence on a scale of 0.0 to 1.0:\n",
      "Sentence: I like learning English\n",
      "\n",
      "Provide:\n",
      "1. Fluency score (0.0-1.0)\n",
      "2. Brief reasoning\n",
      "\n",
      "Format: Score: X.XX | Reason: ...<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Score: 0.90 | Reason: Clear subject-verb agreement, natural word order, appropriate vocabulary<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample fluency scoring data\n",
    "fluency_data = [\n",
    "    {\n",
    "        \"text\": \"I like learning English\",\n",
    "        \"score\": 0.90,\n",
    "        \"reasoning\": \"Clear subject-verb agreement, natural word order, appropriate vocabulary\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Yesterday I go to school\",\n",
    "        \"score\": 0.65,\n",
    "        \"reasoning\": \"Incorrect past tense usage, should be 'went'\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"She don't like coffee\",\n",
    "        \"score\": 0.55,\n",
    "        \"reasoning\": \"Subject-verb disagreement, should be 'doesn't'\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The weather is beautiful today\",\n",
    "        \"score\": 0.95,\n",
    "        \"reasoning\": \"Perfect grammar, natural expression, clear meaning\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Me and my friend goes to park\",\n",
    "        \"score\": 0.45,\n",
    "        \"reasoning\": \"Multiple errors: pronoun case, subject-verb agreement, missing article\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_fluency_prompt(example):\n",
    "    \"\"\"Format data for fluency scoring task\"\"\"\n",
    "    prompt = f\"\"\"Rate the fluency of this English sentence on a scale of 0.0 to 1.0:\n",
    "Sentence: {example['text']}\n",
    "\n",
    "Provide:\n",
    "1. Fluency score (0.0-1.0)\n",
    "2. Brief reasoning\n",
    "\n",
    "Format: Score: X.XX | Reason: ...\"\"\"\n",
    "    \n",
    "    response = f\"Score: {example['score']:.2f} | Reason: {example['reasoning']}\"\n",
    "    \n",
    "    # Qwen chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create dataset\n",
    "fluency_dataset = Dataset.from_list(fluency_data)\n",
    "fluency_dataset = fluency_dataset.map(format_fluency_prompt)\n",
    "\n",
    "print(f\"Fluency dataset size: {len(fluency_dataset)}\")\n",
    "print(\"\\nExample:\")\n",
    "print(fluency_dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdd70e",
   "metadata": {},
   "source": [
    "### 4.2 Vocabulary Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb05df60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1720d45fa8f646c6bb118aafe6750702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dataset size: 5\n"
     ]
    }
   ],
   "source": [
    "# Sample vocabulary classification data\n",
    "vocabulary_data = [\n",
    "    {\n",
    "        \"text\": \"I like to eat apples\",\n",
    "        \"level\": \"A2\",\n",
    "        \"key_words\": \"like (A2), eat (A2), apples (A2)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"We should discuss the opportunity\",\n",
    "        \"level\": \"B1\",\n",
    "        \"key_words\": \"discuss (B1), opportunity (B1)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"His argument was quite eloquent\",\n",
    "        \"level\": \"B2\",\n",
    "        \"key_words\": \"argument (B2), eloquent (B2)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The weather is nice today\",\n",
    "        \"level\": \"A2\",\n",
    "        \"key_words\": \"weather (A2), nice (A2)\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"She demonstrated remarkable perseverance\",\n",
    "        \"level\": \"B2\",\n",
    "        \"key_words\": \"demonstrated (B2), remarkable (B2), perseverance (B2)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_vocabulary_prompt(example):\n",
    "    \"\"\"Format data for vocabulary classification task\"\"\"\n",
    "    prompt = f\"\"\"Classify the vocabulary level of this English sentence according to CEFR:\n",
    "Sentence: {example['text']}\n",
    "\n",
    "Provide:\n",
    "1. CEFR Level (A2, B1, or B2)\n",
    "2. Key vocabulary words with their levels\n",
    "\n",
    "Format: Level: XX | Key words: ...\"\"\"\n",
    "    \n",
    "    response = f\"Level: {example['level']} | Key words: {example['key_words']}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "vocabulary_dataset = Dataset.from_list(vocabulary_data)\n",
    "vocabulary_dataset = vocabulary_dataset.map(format_vocabulary_prompt)\n",
    "\n",
    "print(f\"Vocabulary dataset size: {len(vocabulary_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c32d76",
   "metadata": {},
   "source": [
    "### 4.3 Grammar Correction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c7e9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b2af3b53404b5d97f28db7903104b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar dataset size: 5\n"
     ]
    }
   ],
   "source": [
    "# Sample grammar correction data\n",
    "grammar_data = [\n",
    "    {\n",
    "        \"incorrect\": \"She don't like coffee\",\n",
    "        \"correct\": \"She doesn't like coffee\",\n",
    "        \"explanation\": \"Subject-verb agreement: 'she' (3rd person singular) requires 'doesn't' not 'don't'\"\n",
    "    },\n",
    "    {\n",
    "        \"incorrect\": \"I goes to school yesterday\",\n",
    "        \"correct\": \"I went to school yesterday\",\n",
    "        \"explanation\": \"Incorrect tense: 'yesterday' requires past tense 'went', not present 'goes'\"\n",
    "    },\n",
    "    {\n",
    "        \"incorrect\": \"He have a car\",\n",
    "        \"correct\": \"He has a car\",\n",
    "        \"explanation\": \"Subject-verb agreement: 'he' (3rd person singular) requires 'has' not 'have'\"\n",
    "    },\n",
    "    {\n",
    "        \"incorrect\": \"They was playing\",\n",
    "        \"correct\": \"They were playing\",\n",
    "        \"explanation\": \"Subject-verb agreement: 'they' (plural) requires 'were' not 'was'\"\n",
    "    },\n",
    "    {\n",
    "        \"incorrect\": \"I am go to school\",\n",
    "        \"correct\": \"I am going to school\",\n",
    "        \"explanation\": \"Continuous tense requires present participle 'going', not base form 'go'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_grammar_prompt(example):\n",
    "    \"\"\"Format data for grammar correction task\"\"\"\n",
    "    prompt = f\"\"\"Correct the grammar errors in this English sentence:\n",
    "Incorrect: {example['incorrect']}\n",
    "\n",
    "Provide:\n",
    "1. Corrected sentence\n",
    "2. Brief explanation of the error\n",
    "\n",
    "Format: Correct: ... | Explanation: ...\"\"\"\n",
    "    \n",
    "    response = f\"Correct: {example['correct']} | Explanation: {example['explanation']}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "grammar_dataset = Dataset.from_list(grammar_data)\n",
    "grammar_dataset = grammar_dataset.map(format_grammar_prompt)\n",
    "\n",
    "print(f\"Grammar dataset size: {len(grammar_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be100aa",
   "metadata": {},
   "source": [
    "### 4.4 Dialogue Generation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cd53365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1d3ccaa7f64dfcb174b0eaaa7e0cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue dataset size: 5\n"
     ]
    }
   ],
   "source": [
    "# Sample dialogue generation data\n",
    "dialogue_data = [\n",
    "    {\n",
    "        \"user_input\": \"I like learning English\",\n",
    "        \"fluency_score\": 0.90,\n",
    "        \"level\": \"A2\",\n",
    "        \"errors\": \"None\",\n",
    "        \"response\": \"That's wonderful! Your sentence is very clear and natural. You're using the present tense correctly. Would you like to tell me more about why you enjoy learning English?\"\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"Yesterday I go to school\",\n",
    "        \"fluency_score\": 0.65,\n",
    "        \"level\": \"A2\",\n",
    "        \"errors\": \"Incorrect past tense: 'go' should be 'went'\",\n",
    "        \"response\": \"Good try! You're using 'yesterday' correctly to indicate past time. However, with past time expressions, we need the past tense. Try saying: 'Yesterday I went to school.' Can you repeat that?\"\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"She don't like coffee\",\n",
    "        \"fluency_score\": 0.55,\n",
    "        \"level\": \"A2\",\n",
    "        \"errors\": \"Subject-verb agreement: 'don't' should be 'doesn't'\",\n",
    "        \"response\": \"Almost there! When we use 'she', 'he', or 'it', we need 'doesn't' instead of 'don't'. The correct sentence is: 'She doesn't like coffee.' This is because 'she' is third person singular. Let's practice this pattern.\"\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"We should discuss the opportunity\",\n",
    "        \"fluency_score\": 0.92,\n",
    "        \"level\": \"B1\",\n",
    "        \"errors\": \"None\",\n",
    "        \"response\": \"Excellent! Your sentence structure is perfect, and you're using B1 level vocabulary very naturally. The modal verb 'should' is used correctly to make a suggestion. What opportunity are you thinking about?\"\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"The weather is beautiful today\",\n",
    "        \"fluency_score\": 0.95,\n",
    "        \"level\": \"A2\",\n",
    "        \"errors\": \"None\",\n",
    "        \"response\": \"Perfect! Your sentence is grammatically correct and very natural. You're using the present tense 'is' correctly with 'weather', and your word choice is appropriate. Great job!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_dialogue_prompt(example):\n",
    "    \"\"\"Format data for dialogue generation task\"\"\"\n",
    "    prompt = f\"\"\"You are an encouraging English tutor. Generate a supportive response to the student's input.\n",
    "\n",
    "Student Input: {example['user_input']}\n",
    "Fluency Score: {example['fluency_score']}\n",
    "Vocabulary Level: {example['level']}\n",
    "Errors Detected: {example['errors']}\n",
    "\n",
    "Generate an encouraging tutor response that:\n",
    "1. Acknowledges what the student did well\n",
    "2. Gently corrects errors if any\n",
    "3. Provides a learning point\n",
    "4. Encourages further practice\"\"\"\n",
    "    \n",
    "    response = example['response']\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "dialogue_dataset = Dataset.from_list(dialogue_data)\n",
    "dialogue_dataset = dialogue_dataset.map(format_dialogue_prompt)\n",
    "\n",
    "print(f\"Dialogue dataset size: {len(dialogue_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277d5ef",
   "metadata": {},
   "source": [
    "## 5. Fine-tune Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef6af4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_with_lora(task_name, dataset, lora_config):\n",
    "    \"\"\"\n",
    "    Fine-tune base model with LoRA adapter for specific task\n",
    "    \n",
    "    Args:\n",
    "        task_name: Name of the task (fluency, vocabulary, grammar, dialogue)\n",
    "        dataset: Training dataset\n",
    "        lora_config: LoRA configuration dict\n",
    "    \n",
    "    Returns:\n",
    "        Trained model with LoRA adapter\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {task_name.upper()} adapter\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create LoRA config\n",
    "    peft_config = LoraConfig(**lora_config)\n",
    "    \n",
    "    # Prepare model for training\n",
    "    model = base_model\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize the text\n",
    "        result = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{TRAINING_CONFIG['output_dir']}/{task_name}\",\n",
    "        num_train_epochs=TRAINING_CONFIG['num_train_epochs'],\n",
    "        per_device_train_batch_size=TRAINING_CONFIG['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
    "        learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "        weight_decay=TRAINING_CONFIG['weight_decay'],\n",
    "        warmup_ratio=TRAINING_CONFIG['warmup_ratio'],\n",
    "        lr_scheduler_type=TRAINING_CONFIG['lr_scheduler_type'],\n",
    "        logging_steps=TRAINING_CONFIG['logging_steps'],\n",
    "        save_steps=TRAINING_CONFIG['save_steps'],\n",
    "        save_total_limit=TRAINING_CONFIG['save_total_limit'],\n",
    "        fp16=TRAINING_CONFIG['fp16'],\n",
    "        bf16=TRAINING_CONFIG['bf16'],\n",
    "        gradient_checkpointing=TRAINING_CONFIG['gradient_checkpointing'],\n",
    "        optim=TRAINING_CONFIG['optim'],\n",
    "        report_to=TRAINING_CONFIG['report_to'],\n",
    "        logging_first_step=True,\n",
    "        push_to_hub=False,\n",
    "        dataloader_num_workers=TRAINING_CONFIG['dataloader_num_workers'],\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    # Create standard Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nStarting training for {task_name}...\")\n",
    "    print(\" Training on CPU - this will take longer than GPU/MPS\")\n",
    "    print(\"ðŸ’¡ Monitor CPU usage with Activity Monitor to ensure efficient utilization\\n\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save adapter\n",
    "    adapter_path = f\"./adapters/{task_name}_lora_adapter\"\n",
    "    model.save_pretrained(adapter_path)\n",
    "    tokenizer.save_pretrained(adapter_path)\n",
    "    print(f\"\\nâœ“ LoRA adapter saved to: {adapter_path}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d712b7",
   "metadata": {},
   "source": [
    "## 6. Train All Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b783bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training FLUENCY adapter\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 36,929,536 (3.99%)\n",
      "Total params: 925,545,984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eade270e91844d648e3ab6137db7c9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for fluency...\n",
      " Training on CPU - this will take longer than GPU/MPS\n",
      "ðŸ’¡ Monitor CPU usage with Activity Monitor to ensure efficient utilization\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1369807432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 6.1 Train Fluency Scoring Adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m fluency_model = finetune_with_lora(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtask_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fluency\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfluency_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2363814704.py\u001b[0m in \u001b[0;36mfinetune_with_lora\u001b[0;34m(task_name, dataset, lora_config)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ’¡ Monitor CPU usage with Activity Monitor to ensure efficient utilization\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Save adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    313\u001b[0m             )\n\u001b[1;32m    314\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             ) if torch.amp.is_autocast_available(ctx.device_type) else contextlib.nullcontext()\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_autocast_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_autocast_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdetached_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m                 \u001b[0;31m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0;31m# The reason is that in some cases, an error can occur that backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"packing_format_for_cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train each task adapter\n",
    "# Note: In production, use larger datasets (1500-3000 samples per task)\n",
    "\n",
    "# 6.1 Train Fluency Scoring Adapter\n",
    "fluency_model = finetune_with_lora(\n",
    "    task_name=\"fluency\",\n",
    "    dataset=fluency_dataset,\n",
    "    lora_config=LORA_CONFIGS[\"fluency\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Train Vocabulary Classification Adapter\n",
    "vocabulary_model = finetune_with_lora(\n",
    "    task_name=\"vocabulary\",\n",
    "    dataset=vocabulary_dataset,\n",
    "    lora_config=LORA_CONFIGS[\"vocabulary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Train Grammar Correction Adapter\n",
    "grammar_model = finetune_with_lora(\n",
    "    task_name=\"grammar\",\n",
    "    dataset=grammar_dataset,\n",
    "    lora_config=LORA_CONFIGS[\"grammar\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefa086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Train Dialogue Generation Adapter\n",
    "dialogue_model = finetune_with_lora(\n",
    "    task_name=\"dialogue\",\n",
    "    dataset=dialogue_dataset,\n",
    "    lora_config=LORA_CONFIGS[\"dialogue\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601de346",
   "metadata": {},
   "source": [
    "## 7. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cce3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def test_adapter(task_name, test_prompt):\n",
    "    \"\"\"\n",
    "    Test a trained LoRA adapter\n",
    "    \n",
    "    Args:\n",
    "        task_name: Name of the task adapter to test\n",
    "        test_prompt: Test prompt text\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {task_name.upper()} adapter\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Load base model + adapter (with quantization for CPU)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float32,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        f\"./adapters/{task_name}_lora_adapter\"\n",
    "    )\n",
    "    \n",
    "    # Prepare input\n",
    "    messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate (will be slower on CPU)\n",
    "    print(\"â³ Generating response on CPU...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Prompt: {test_prompt}\")\n",
    "    print(f\"\\nResponse: {response}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Fluency Scoring\n",
    "test_adapter(\n",
    "    \"fluency\",\n",
    "    \"Rate the fluency of this sentence: She plays piano every day\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cbcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Vocabulary Classification\n",
    "test_adapter(\n",
    "    \"vocabulary\",\n",
    "    \"Classify the vocabulary level: The presentation was incredibly sophisticated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Grammar Correction\n",
    "test_adapter(\n",
    "    \"grammar\",\n",
    "    \"Correct the grammar: He don't want to go there\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a29814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dialogue Generation\n",
    "test_adapter(\n",
    "    \"dialogue\",\n",
    "    \"\"\"Generate a tutor response:\n",
    "Student Input: I likes playing basketball\n",
    "Fluency Score: 0.60\n",
    "Level: A2\n",
    "Errors: Subject-verb agreement\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2134b27",
   "metadata": {},
   "source": [
    "## 8. Export for Production\n",
    "\n",
    "### 8.1 Merge Adapters (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e25692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights into base model (optional, for deployment)\n",
    "def merge_and_save_adapter(task_name):\n",
    "    \"\"\"\n",
    "    Merge LoRA adapter weights into base model and save\n",
    "    This creates a standalone model without needing PEFT library\n",
    "    \"\"\"\n",
    "    print(f\"Merging {task_name} adapter...\")\n",
    "    \n",
    "    # Load base + adapter\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        f\"./adapters/{task_name}_lora_adapter\"\n",
    "    )\n",
    "    \n",
    "    # Merge and unload\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    output_path = f\"./merged_models/{task_name}_merged\"\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    merged_model.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    \n",
    "    print(f\"âœ“ Merged model saved to: {output_path}\")\n",
    "\n",
    "# Example: Merge fluency adapter\n",
    "# merge_and_save_adapter(\"fluency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48413fe2",
   "metadata": {},
   "source": [
    "### 8.2 Quantize for Mobile (Production Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad766fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to 4-bit for mobile deployment\n",
    "# Note: This requires optimum library\n",
    "# !pip install optimum\n",
    "\n",
    "# from optimum.quanto import quantize, qint4\n",
    "\n",
    "# def quantize_for_mobile(task_name):\n",
    "#     \"\"\"Quantize merged model to 4-bit for mobile\"\"\"\n",
    "#     model_path = f\"./merged_models/{task_name}_merged\"\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "#     \n",
    "#     # Quantize\n",
    "#     quantize(model, weights=qint4)\n",
    "#     \n",
    "#     # Save\n",
    "#     output_path = f\"./quantized_models/{task_name}_q4\"\n",
    "#     Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "#     model.save_pretrained(output_path)\n",
    "#     \n",
    "#     print(f\"âœ“ Quantized model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638609b",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘          FINE-TUNING PIPELINE COMPLETED                      â•‘\n",
    "â•‘              (Mac Intel i9 - CPU Mode)                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ“ Base Model: Qwen2.5-1.5B-Instruct (4-bit quantized)\n",
    "âœ“ 4 LoRA Adapters Trained:\n",
    "  1. Fluency Scoring (r=32, alpha=64)\n",
    "  2. Vocabulary Classification (r=32, alpha=64)\n",
    "  3. Grammar Correction (r=32, alpha=64)\n",
    "  4. Dialogue Generation (r=32, alpha=64)\n",
    "\n",
    "Saved Adapters:\n",
    "  ./adapters/fluency_lora_adapter/\n",
    "  ./adapters/vocabulary_lora_adapter/\n",
    "  ./adapters/grammar_lora_adapter/\n",
    "  ./adapters/dialogue_lora_adapter/\n",
    "\n",
    " MAC INTEL i9 CONSIDERATIONS:\n",
    "\n",
    "CPU Training Performance:\n",
    "  â€¢ 2-3x slower than M1/M2 with MPS\n",
    "  â€¢ Batch size: 2 (vs 8 on GPU)\n",
    "  â€¢ Gradient accumulation: 16 steps\n",
    "  â€¢ Expected time: 2-3 hours per adapter (small dataset)\n",
    "\n",
    "Optimizations Applied:\n",
    "  âœ“ 4-bit quantization (75% RAM saving)\n",
    "  âœ“ Gradient checkpointing enabled\n",
    "  âœ“ Multi-core dataloader (4 workers)\n",
    "  âœ“ Low CPU memory usage mode\n",
    "  âœ“ FP32 precision (CPU compatible)\n",
    "\n",
    "NEXT STEPS:\n",
    "\n",
    "1. Collect Real Data:\n",
    "   - Fluency: 1,500-3,000 annotated sentences\n",
    "   - Vocabulary: 2,500 CEFR-labeled sentences\n",
    "   - Grammar: 2,000 error-correction pairs\n",
    "   - Dialogue: 1,500 tutor-student conversations\n",
    "\n",
    "2. Re-train with Full Dataset:\n",
    "   - Expected training time: 8-12 hours per adapter on CPU\n",
    "   - Consider using cloud GPU for faster training\n",
    "   - Or train overnight with smaller learning rate\n",
    "\n",
    "3. Evaluate Performance:\n",
    "   - Fluency: MAE < 0.12, Pearson > 0.90\n",
    "   - Vocabulary: Accuracy > 90%\n",
    "   - Grammar: F0.5 > 68\n",
    "   - Dialogue: Quality > 96%\n",
    "\n",
    "4. Create Production Models:\n",
    "   - Knowledge distillation to Qwen2.5-0.5B\n",
    "   - LoRA rank: 16 (instead of 32)\n",
    "   - Expected: 88-91% accuracy, 2x faster\n",
    "\n",
    "5. Deploy:\n",
    "   - Mobile: Quantized 0.5B model (~350MB)\n",
    "   - Server: Full 1.5B model (~1GB)\n",
    "   - Switch adapters in < 1ms\n",
    "\n",
    "Memory Usage (Mac Intel):\n",
    "  Development: 1GB storage, 4-6GB RAM (with 4-bit)\n",
    "  Production: 350MB storage, 2GB RAM\n",
    "\n",
    "ðŸ’¡ Tips for Mac Intel:\n",
    "  - Close unnecessary apps to free RAM\n",
    "  - Use Activity Monitor to check CPU utilization\n",
    "  - Consider training with smaller epochs first (2-3)\n",
    "  - Use wandb: set report_to=\"wandb\" for monitoring\n",
    "  - Train one adapter at a time to avoid memory issues\n",
    "  - Consider using Google Colab GPU for faster iteration\n",
    "\n",
    "ðŸ”§ Alternative: Use Cloud GPU\n",
    "  - Google Colab: Free T4 GPU (~15-20 min per adapter)\n",
    "  - Kaggle: Free GPU/TPU (~20-30 min per adapter)\n",
    "  - AWS/Azure: Paid GPU instances (~10-15 min per adapter)\n",
    "\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
